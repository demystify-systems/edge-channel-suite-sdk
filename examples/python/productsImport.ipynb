{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<!-- <a href=\"https://colab.research.google.com/github/demystify-systems/edge-channel-suite/blob/main/demo/HelloSlackDocker2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gr9gc7arDWqF",
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# saas_edge_id = \"60ff8220-4d6f-4433-afd4-67952070e964\"\n",
    "# slack_webhook_url = \"os.getenv(\"SLACK_WEBHOOK_URL\", \"\")\"\n",
    "# job_id = \"a8ddf474-8fbe-4f0d-80a9-fc1fd80d103e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIb8HRHV6l_4"
   },
   "outputs": [],
   "source": [
    "!pip install pg8000 sqlalchemy pandas requests openpyxl zipfile36 psycopg2-binary --quiet\n",
    "!pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib google-cloud-storage --quiet\n",
    "\n",
    "import pg8000\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "from datetime import datetime,date\n",
    "import requests\n",
    "from uuid import UUID\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import mimetypes\n",
    "\n",
    "import pytz\n",
    "from jsonschema import validate, ValidationError, SchemaError\n",
    "import sys\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "import json\n",
    "from psycopg2 import sql as psql \n",
    "from typing import List, Dict,Tuple, Any, Optional,Set,Any\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from traceback import print_exc\n",
    "from openpyxl import load_workbook\n",
    "from io import BytesIO\n",
    "from slack_sdk import WebhookClient\n",
    "# global_message= f\"{customer_id} -- is executing the job from google cloud runner with DB access\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "\n",
    "# # Step 1: Build sdk path from current location\n",
    "# root_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "# sdk_path = os.path.join(root_path, 'sdk')\n",
    "\n",
    "# # Step 2: Clean sys.path and insert sdk_path\n",
    "# sys.path = [p for p in sys.path if 'sdk' not in p]\n",
    "# if sdk_path not in sys.path:\n",
    "#     sys.path.insert(0, sdk_path)\n",
    "\n",
    "# print(f\"SDK path set to: {sdk_path}\")\n",
    "\n",
    "# # Step 3: Import and force reload the module\n",
    "# import export_and_import_transformation\n",
    "# importlib.reload(export_and_import_transformation)\n",
    "\n",
    "# # Step 4: Now access your function\n",
    "# bulk_apply_pipe_rules = export_and_import_transformation.bulk_apply_pipe_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_user = os.getenv(\"DB_USER\", \"postgres\")                # Default to 'postgres' if env variable not set\n",
    "db_pass = os.getenv(\"DB_PASSWORD\", \"saasdbforwindmill2023\")\n",
    "db_name = os.getenv(\"DB_NAME\", \"catalog-edge-db\")\n",
    "db_host = \"/cloudsql/saastify-base-wm:us-central1:saastify-pgdb-us\"\n",
    "# db_host = \"127.0.0.1\"\n",
    "db_port = 5432"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDK Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this after the database configuration section (in[3])\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"\n",
    "    Creates and returns a database connection using global configuration.\n",
    "    Handles both Unix socket and TCP connections.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine if we're using Unix socket or TCP\n",
    "        if db_host.startswith('/cloudsql/'):\n",
    "            connection = pg8000.connect(\n",
    "                user=db_user,\n",
    "                password=db_pass,\n",
    "                database=db_name,\n",
    "                unix_sock=db_host + \"/.s.PGSQL.5432\"\n",
    "            )\n",
    "        else:\n",
    "            connection = pg8000.connect(\n",
    "                user=db_user,\n",
    "                password=db_pass,\n",
    "                database=db_name,\n",
    "                host=db_host,\n",
    "                port=db_port\n",
    "            )\n",
    "        return connection\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: Failed to establish database connection\")\n",
    "        print(f\"ERROR: {str(e)}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_url(url, file_name=\"\"):\n",
    "    get_response = requests.get(url, stream=True)\n",
    "    if file_name == \"\":\n",
    "        file_name = url.split(\"/\")[-1]\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        for chunk in get_response.iter_content(chunk_size=1024):\n",
    "            if chunk:  # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "    return file_name\n",
    "\n",
    "class FileParser(object):\n",
    "    def load(self, url):\n",
    "        self.url = url\n",
    "        self.file_name = download_url(url)\n",
    "        self.file_type = self.file_name.split(\".\")[-1]\n",
    "        print(\"The URL file type is : \", self.file_type)\n",
    "        method_name = \"parse_\" + self.file_type\n",
    "        method = getattr(self, method_name, lambda: \"Invalid\")\n",
    "        return method()\n",
    "\n",
    "    def infer_schema(self):\n",
    "        self.df.info()\n",
    "        self.columns = list(self.df.columns.values.tolist())\n",
    "        print(\"List of all columns are : \", self.columns)\n",
    "        print(\"##### Pandas inferred Schema\")\n",
    "        pandas_schema = self.df.columns.to_series().groupby(self.df.dtypes).groups\n",
    "        print(pandas_schema)\n",
    "\n",
    "    def parse_xlsx(self):\n",
    "        return self.parse_excel()\n",
    "\n",
    "    def parse_xlsm(self):\n",
    "        return self.parse_excel()\n",
    "\n",
    "    def parse_xls(self):\n",
    "        return self.parse_excel()\n",
    "\n",
    "    def parse_csv(self):\n",
    "        df = pd.read_csv(self.file_name, sep=\",\", header=0)\n",
    "        return df\n",
    "\n",
    "    def parse_zip(self):\n",
    "        with zipfile.ZipFile(self.file_name, \"r\") as zip_ref:\n",
    "            zip_ref.extractall()\n",
    "            extracted_files = zip_ref.namelist()\n",
    "            print(extracted_files)\n",
    "            return extracted_files\n",
    "\n",
    "    def parse_tsv(self):\n",
    "        df = pd.read_csv(self.file_name, sep=\"\\t\", header=0)\n",
    "        return df\n",
    "\n",
    "    def parse_json(self):\n",
    "        df = pd.read_json(self.file_name)\n",
    "        return df\n",
    "\n",
    "    def parse_txt(self):\n",
    "        df = pd.read_csv(self.file_name, sep=\" \")\n",
    "        return df\n",
    "\n",
    "    def parse_excel(self):\n",
    "        try:\n",
    "            xls = pd.ExcelFile(self.file_name)\n",
    "            # Read the first sheet by default\n",
    "            sheet_name = xls.sheet_names[0]\n",
    "            df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to parse Excel file: {str(e)}\")\n",
    "            print(f\"ERROR: {traceback.format_exc()}\")\n",
    "            return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_slack_message(message, webhook_url):\n",
    "    try:\n",
    "        print(\"INFO: Attempting to send a message to Slack...\")\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        payload = {\"text\": message}\n",
    "        response = requests.post(webhook_url, data=json.dumps(payload), headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"INFO: Slack message sent successfully.\")\n",
    "        else:\n",
    "            print(\"WARNING: Failed to send Slack message.\")\n",
    "            print(f\"WARNING: Status code: {response.status_code}\")\n",
    "            print(f\"WARNING: Response: {response.text}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: An error occurred while sending a Slack message.\")\n",
    "        print(f\"ERROR: {e}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database configuration - credentials should ideally be stored in environment variables for security\n",
    "\n",
    "\n",
    "def fetch_saas_edge_jobs_details(job_id, saas_edge_id):\n",
    "    try:\n",
    "        print(\"INFO: Attempting to connect to Cloud SQL instance...\")\n",
    "        \n",
    "        # Connect to the Cloud SQL instance\n",
    "        conn = get_db_connection()\n",
    "        print(\"INFO: Connected to Cloud SQL successfully.\")\n",
    "\n",
    "        # Define the query\n",
    "        query = \"\"\"\n",
    "        SELECT * FROM public.saas_edge_jobs\n",
    "        WHERE request_id = %s AND saas_edge_id = %s;\n",
    "        \"\"\"\n",
    "        print(f\"INFO: Executing query: {query} with job_id: {job_id} and saas_edge_id: {saas_edge_id}\")\n",
    "\n",
    "        # Execute the query\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query, (job_id, saas_edge_id))\n",
    "        result = cursor.fetchall()\n",
    "\n",
    "        if not result:\n",
    "            print(f\"WARNING: No results found for job_id: {job_id} and saas_edge_id: {saas_edge_id}\")\n",
    "        else:\n",
    "            print(f\"INFO: Query executed successfully. Number of records fetched: {len(result)}\")\n",
    "\n",
    "        # Process and convert datetime and UUID objects to strings for JSON serialization\n",
    "        row_data = [\n",
    "            {k: (v.isoformat() if isinstance(v, datetime) else str(v) if isinstance(v, UUID) else v) \n",
    "             for k, v in zip([col[0] for col in cursor.description], row)}\n",
    "            for row in result\n",
    "        ]\n",
    "        \n",
    "        print(\"INFO: Data processed for JSON serialization.\")\n",
    "\n",
    "        # Close the connection\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"INFO: Database connection closed.\")\n",
    "        \n",
    "        return row_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: An error occurred while connecting to Cloud SQL or executing the query.\")\n",
    "        print(f\"ERROR: {e}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")  # Full stack trace for debugging\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_channel_attr_template(saas_edge_id, template_id):\n",
    "    try:\n",
    "        print(\"INFO: Attempting to connect to Cloud SQL instance...\")\n",
    "        \n",
    "        conn = get_db_connection()\n",
    "        print(\"INFO: Connected to Cloud SQL successfully.\")\n",
    "\n",
    "        query = \"\"\"\n",
    "        SELECT * FROM public.saas_channel_attr_template\n",
    "        WHERE saas_edge_id = %s AND template_id = %s;\n",
    "        \"\"\"\n",
    "        print(f\"INFO: Executing query for saas_edge_id: {saas_edge_id} and template_id: {template_id}\")\n",
    "\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query, (saas_edge_id, template_id))\n",
    "        result = cursor.fetchall()\n",
    "\n",
    "        if not result:\n",
    "            print(f\"WARNING: No results found for saas_edge_id: {saas_edge_id} and template_id: {template_id}\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"INFO: Query executed successfully. Number of records fetched: {len(result)}\")\n",
    "\n",
    "        # Process results\n",
    "        row_data = [\n",
    "            {k: (v.isoformat() if isinstance(v, datetime) else str(v) if isinstance(v, UUID) else v) \n",
    "             for k, v in zip([col[0] for col in cursor.description], row)}\n",
    "            for row in result\n",
    "        ]\n",
    "        \n",
    "        print(\"INFO: Data processed for JSON serialization.\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"INFO: Database connection closed.\")\n",
    "        \n",
    "        return row_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: An error occurred while fetching channel attribute template data.\")\n",
    "        print(f\"ERROR: {e}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_saas_channel_templates(saas_edge_id, template_id):\n",
    "    try:\n",
    "        print(\"INFO: Attempting to connect to Cloud SQL instance...\")\n",
    "        \n",
    "        conn = get_db_connection()\n",
    "        print(\"INFO: Connected to Cloud SQL successfully.\")\n",
    "\n",
    "        query = \"\"\"\n",
    "        SELECT * FROM public.saas_channel_templates\n",
    "        WHERE saas_edge_id = %s AND template_id = %s;\n",
    "        \"\"\"\n",
    "        print(f\"INFO: Executing query for saas_edge_id: {saas_edge_id} and template_id: {template_id}\")\n",
    "\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query, (saas_edge_id, template_id))\n",
    "        result = cursor.fetchall()\n",
    "\n",
    "        if not result:\n",
    "            print(f\"WARNING: No results found for saas_edge_id: {saas_edge_id} and template_id: {template_id}\")\n",
    "            return None\n",
    "\n",
    "        print(f\"INFO: Query executed successfully. Number of records fetched: {len(result)}\")\n",
    "\n",
    "        # Process results and handle datetime/UUID serialization\n",
    "        row_data = [\n",
    "            {k: (v.isoformat() if isinstance(v, datetime) else str(v) if isinstance(v, UUID) else v) \n",
    "             for k, v in zip([col[0] for col in cursor.description], row)}\n",
    "            for row in result\n",
    "        ]\n",
    "        \n",
    "        print(\"INFO: Data processed for JSON serialization.\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"INFO: Database connection closed.\")\n",
    "        \n",
    "        return row_data[0] if row_data else None  # Return first row since template_id should be unique\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: An error occurred while fetching channel template data.\")\n",
    "        print(f\"ERROR: {e}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main execution code\n",
    "try:\n",
    "    job_id = job_id  # Replace with the actual job_id you want to query\n",
    "    saas_edge_id = saas_edge_id  # Replace with the actual saas_edge_id you want to query\n",
    "    slack_url = os.getenv(\"SLACK_WEBHOOK_URL\", slack_webhook_url)\n",
    "    \n",
    "    if not slack_url:\n",
    "        raise ValueError(\"Slack webhook URL not set in environment variables\")\n",
    "\n",
    "    # Fetch data from the database\n",
    "    print(f\"INFO: Fetching details for job_id: {job_id} and saas_edge_id: {saas_edge_id}\")\n",
    "    row_data = fetch_saas_edge_jobs_details(job_id, saas_edge_id)\n",
    "\n",
    "    if not row_data:\n",
    "        print(\"WARNING: No data retrieved or an error occurred during the fetch operation.\")\n",
    "\n",
    "except Exception as main_e:\n",
    "    print(\"ERROR: An unexpected error occurred in the main execution block.\")\n",
    "    print(f\"ERROR: {main_e}\")\n",
    "    print(f\"ERROR: {traceback.format_exc()}\")  # Full stack trace for debugging\n",
    "\n",
    "\n",
    "request_args = row_data[0].get(\"request_args\",{})\n",
    "\n",
    "print(request_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_path = request_args.get(\"job_path\", \"\")\n",
    "print(\"job_pathh\",job_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to get the database configuration\n",
    "db_config = {\n",
    "    # \"host\":\"localhost\",\n",
    "    \"host\": \"/cloudsql/saastify-base-wm:us-central1:saastify-pgdb-us\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"catalog-edge-db\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"saasdbforwindmill2023\"\n",
    "}\n",
    "\n",
    "\n",
    "# Function to construct the PostgreSQL connection string\n",
    "def create_connection_string() -> str:\n",
    "    try:\n",
    "        connection_string = (\n",
    "            f\"dbname={db_config['database']} \"\n",
    "            f\"user={db_config['user']} \"\n",
    "            f\"password={db_config['password']} \"\n",
    "            f\"host={db_config['host']} \"\n",
    "            f\"port={db_config['port']} \"\n",
    "        )\n",
    "        return connection_string\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing key in DB configuration: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating connection string: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Helper function to convert datetime and decimal objects to serializable format\n",
    "def custom_serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    elif isinstance(obj, Decimal):\n",
    "        return float(obj)\n",
    "    raise TypeError(f\"Type {obj.__class__.__name__} not serializable\")\n",
    "\n",
    "\n",
    "# Function to fetch template attributes\n",
    "def fetch_template_attributes(\n",
    "    saas_edge_id: str, template_id: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    connection_string = create_connection_string()\n",
    "    connection = None\n",
    "    try:\n",
    "        # Establish the connection\n",
    "        connection = psycopg2.connect(connection_string, cursor_factory=RealDictCursor)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Prepare the query to fetch attributes for the given saas_edge_id and template_id\n",
    "        query = \"\"\"\n",
    "            SELECT * FROM saas_channel_attr_template \n",
    "            WHERE saas_edge_id = %s AND template_id = %s \n",
    "            ORDER BY column_pos ASC NULLS LAST, column_name ASC\n",
    "        \"\"\"\n",
    "        print(\"Executing query:\", query)\n",
    "\n",
    "        # Execute the query with saas_edge_id and template_id as parameters\n",
    "        cursor.execute(query, (saas_edge_id, template_id))\n",
    "        response_data = cursor.fetchall()\n",
    "\n",
    "        # If no data is fetched, handle gracefully\n",
    "        if not response_data:\n",
    "            print(\n",
    "                f\"No template attributes found for saas_edge_id: {saas_edge_id} and template_id: {template_id}\"\n",
    "            )\n",
    "            return []\n",
    "\n",
    "        print(\"Number of rows fetched:\", len(response_data))\n",
    "        return response_data\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching template attributes: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "\n",
    "# Function to generate JSON Schema dynamically, including hierarchy support\n",
    "# Also generates the order array based on column_pos or alphabetic order\n",
    "\n",
    "\n",
    "def generate_json_schema(template_attrs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    try:\n",
    "        # Base schema\n",
    "        schema = {\n",
    "            \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": [],\n",
    "            \"order\": [],\n",
    "        }\n",
    "\n",
    "        # Helper to handle hierarchical properties\n",
    "        def add_property(\n",
    "            properties: Dict[str, Any],\n",
    "            attr: Dict[str, Any],\n",
    "            parent_property: Optional[str] = None,\n",
    "        ):\n",
    "            column_name = attr[\"column_name\"]\n",
    "            data_type = (attr.get(\"data_type\") or \"text\").lower()\n",
    "            data_type_format = attr.get(\"data_type_format\")\n",
    "\n",
    "            # Map data_type to JSON schema types\n",
    "            json_type_mapping = {\n",
    "                \"text\": \"string\",\n",
    "                \"number\": \"number\",\n",
    "                \"integer\": \"integer\",\n",
    "                \"boolean\": \"boolean\",\n",
    "                \"array\": \"array\",\n",
    "                \"object\": \"object\",\n",
    "                \"decimal\": \"number\",\n",
    "                \"timestamp\": \"string\",\n",
    "                \"url\": \"string\",\n",
    "                \"date\": \"string\",\n",
    "                \"datetime\": \"string\",\n",
    "                \"time\": \"string\",\n",
    "            }\n",
    "            field_type = json_type_mapping.get(data_type, \"string\")\n",
    "\n",
    "            # Define the property schema\n",
    "            property_schema = {\n",
    "                \"title\": attr.get(\"custom_title\", column_name),\n",
    "                \"type\": field_type,\n",
    "                \"description\": attr.get(\"description\", f\"Field for {column_name}\"),\n",
    "                \"default\": attr.get(\"default_value\"),\n",
    "                \"nullable\": attr.get(\"is_nullable\", True),\n",
    "            }\n",
    "\n",
    "            # Handle data_type_format if available\n",
    "            if data_type_format:\n",
    "                property_schema[\"format\"] = data_type_format\n",
    "\n",
    "            # Handle multi-valued fields (array)\n",
    "            if attr.get(\"accepts_multiple_values\", False):\n",
    "                property_schema = {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": field_type},\n",
    "                    \"description\": f\"Array of {data_type} values for {column_name}\",\n",
    "                }\n",
    "\n",
    "            # Add validation rules\n",
    "            validation_rules = attr.get(\"validation_rules\", {})\n",
    "            if validation_rules:\n",
    "                if \"minLength\" in validation_rules:\n",
    "                    property_schema[\"minLength\"] = validation_rules[\"minLength\"]\n",
    "                if \"maxLength\" in validation_rules:\n",
    "                    property_schema[\"maxLength\"] = validation_rules[\"maxLength\"]\n",
    "                if \"pattern\" in validation_rules:\n",
    "                    property_schema[\"pattern\"] = validation_rules[\"pattern\"]\n",
    "                if \"enum\" in validation_rules:\n",
    "                    property_schema[\"enum\"] = validation_rules[\"enum\"]\n",
    "\n",
    "            # Add to the appropriate place in the properties structure\n",
    "            if parent_property:\n",
    "                if \"properties\" not in properties[parent_property]:\n",
    "                    properties[parent_property][\"properties\"] = {}\n",
    "                properties[parent_property][\"properties\"][column_name] = property_schema\n",
    "            else:\n",
    "                properties[column_name] = property_schema\n",
    "\n",
    "            # Add to required list if mandatory\n",
    "            if attr.get(\"mandatory\", False):\n",
    "                if parent_property:\n",
    "                    if \"required\" not in properties[parent_property]:\n",
    "                        properties[parent_property][\"required\"] = []\n",
    "                    properties[parent_property][\"required\"].append(column_name)\n",
    "                else:\n",
    "                    schema[\"required\"].append(column_name)\n",
    "\n",
    "            # Add to order list\n",
    "            if not parent_property:\n",
    "                schema[\"order\"].append(column_name)\n",
    "\n",
    "        # Process attributes considering hierarchy\n",
    "        for attr in template_attrs:\n",
    "            parent_id = attr.get(\"parent_id\")\n",
    "            if parent_id:\n",
    "                # Find the parent property in existing attributes\n",
    "                parent_attr = next(\n",
    "                    (a for a in template_attrs if a[\"template_attr_id\"] == parent_id),\n",
    "                    None,\n",
    "                )\n",
    "                if parent_attr:\n",
    "                    add_property(schema[\"properties\"], attr, parent_attr[\"column_name\"])\n",
    "            else:\n",
    "                add_property(schema[\"properties\"], attr)\n",
    "\n",
    "        return schema\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating JSON schema: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Function to insert or update the generated schema in the database\n",
    "def insert_or_update_schema(\n",
    "    saas_edge_id: str, template_id: str, schema: Dict[str, Any]\n",
    ") -> None:\n",
    "    connection_string = create_connection_string()\n",
    "    connection = None\n",
    "    try:\n",
    "        # Establish the connection\n",
    "        connection = psycopg2.connect(connection_string)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Convert schema dictionary to JSON string\n",
    "        schema_json = json.dumps(schema)\n",
    "\n",
    "        # Insert or update the schema using ON CONFLICT clause\n",
    "        query = \"\"\"\n",
    "            INSERT INTO saas_template_schema (saas_edge_id, template_id, schema, created_at, updated_at)\n",
    "            VALUES (%s, %s, %s, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)\n",
    "            ON CONFLICT (saas_edge_id, template_id)\n",
    "            DO UPDATE SET\n",
    "                schema = EXCLUDED.schema,\n",
    "                updated_at = CURRENT_TIMESTAMP;\n",
    "        \"\"\"\n",
    "        print(\"Executing query:\", query)\n",
    "\n",
    "        # Execute the query with provided parameters\n",
    "        cursor.execute(query, (saas_edge_id, template_id, schema_json))\n",
    "        connection.commit()\n",
    "        print(\"Schema inserted or updated successfully.\")\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database error while inserting/updating schema: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error in insert_or_update_schema function: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "\n",
    "def get_existing_schema(saas_edge_id: str, template_id: str) -> Any:\n",
    "    \"\"\"\n",
    "    Fetch the existing schema for the given saas_edge_id and template_id from the saas_template_schema table.\n",
    "    \"\"\"\n",
    "    connection_string = create_connection_string()\n",
    "    connection = None\n",
    "    try:\n",
    "        # Establish database connection\n",
    "        connection = psycopg2.connect(connection_string, cursor_factory=RealDictCursor)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Query to check for existing schema\n",
    "        query = \"\"\"\n",
    "            SELECT schema FROM saas_template_schema\n",
    "            WHERE saas_edge_id = %s AND template_id = %s\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (saas_edge_id, template_id))\n",
    "        result = cursor.fetchone()\n",
    "\n",
    "        if result:\n",
    "            return result[\"schema\"]\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching schema: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "\n",
    "def genarate_template_schema(saas_edge_id: str, template_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main function to fetch or generate schema for saas_template_schema.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Check if schema already exists\n",
    "        existing_schema = get_existing_schema(saas_edge_id, template_id)\n",
    "        if existing_schema:\n",
    "            print(\"Schema found in the database.\")\n",
    "            return existing_schema\n",
    "\n",
    "        print(\"No schema found. Generating and saving schema...\")\n",
    "        # Step 2: Fetch template attributes and generate schema\n",
    "        template_attributes = fetch_template_attributes(saas_edge_id, template_id)\n",
    "        if not template_attributes:\n",
    "            raise ValueError(\n",
    "                \"No attributes found for the given saas_edge_id and template_id.\"\n",
    "            )\n",
    "\n",
    "        generated_schema = generate_json_schema(template_attributes)\n",
    "\n",
    "        # Step 3: Save the schema to the database\n",
    "        insert_or_update_schema(saas_edge_id, template_id, generated_schema)\n",
    "        return generated_schema\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n",
    "\n",
    "\n",
    "genarate_template_schema(saas_edge_id, request_args.get(\"template_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "try:\n",
    "    template_id = request_args.get(\"template_id\")\n",
    "    if not template_id:\n",
    "        raise ValueError(\"template_id not found in request_args\")\n",
    "        \n",
    "    template_data = fetch_channel_attr_template(saas_edge_id, template_id)\n",
    "    template_details = fetch_saas_channel_templates(saas_edge_id, template_id)  \n",
    "    if not template_data:\n",
    "        print(\"WARNING: No template data retrieved or an error occurred during the fetch operation.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"ERROR: An unexpected error occurred while fetching template data.\")\n",
    "    print(f\"ERROR: {e}\")\n",
    "    print(f\"ERROR: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "  # Return original value if conversion fails\n",
    "\n",
    "\n",
    "\n",
    "filtered_template_data = {}\n",
    "\n",
    "try:\n",
    "    if template_data:\n",
    "        # filtered_template_data = [\n",
    "        #     {\n",
    "        #         'column_name': item.get('column_name', ''),  # Default to empty string if not found\n",
    "        #         'field_mapping': item.get('field_mapping', {})  # Default to empty dict if not found\n",
    "        #     }\n",
    "        #     for item in template_data\n",
    "        #     if isinstance(item, dict)  # Ensure item is a dictionary\n",
    "        # ]\n",
    "        filtered_template_data = {\n",
    "            item.get('column_name', ''): {\n",
    "                \"field_mapping\":item.get('field_mapping', \"\"),\n",
    "                \"accepts_multiple_values\":item.get('accepts_multiple_values', False),\n",
    "                \"transformation_function\":item.get(\"transformation_rules\", \"\"),\n",
    "                \"data_type\":item.get(\"data_type\", \"text\")\n",
    "            } \n",
    "            for item in template_data\n",
    "            \n",
    "        }\n",
    "        if not filtered_template_data:\n",
    "            print(\"WARNING: No valid template data entries found after filtering\")\n",
    "    else:\n",
    "        print(\"WARNING: template_data is None or empty\")\n",
    "\n",
    "    print(\"Filtered template data:\", filtered_template_data)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to process template data: {str(e)}\")\n",
    "    print(f\"ERROR: {traceback.format_exc()}\")\n",
    "    filtered_template_data = {}  # Ensure we always return a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = request_args.get(\"file_url\", {}).get(\"url\")\n",
    "create_new_product = request_args.get(\"feed_settings\", {}).get(\"create_new_product\", False)\n",
    "print(file_url)\n",
    "\n",
    "if template_details.get(\"meta\",{}):\n",
    "    saastify_table = template_details.get(\"meta\",{}).get(\"saastify_table\", \"\")\n",
    "else:\n",
    "    saastify_table = \"products\"\n",
    "    # create_new_product = template_details.get(\"meta\", {}).get(\"create_new_product\", False)\n",
    "print(saastify_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # saastify_table = \"product_parent_variants\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    file_parse = FileParser()\n",
    "    print(file_url)\n",
    "    data_df = file_parse.load(file_url)\n",
    "    properties_schema = list(data_df)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to parse the file: {str(e)}\")\n",
    "    print(f\"ERROR: {traceback.format_exc()}\")\n",
    "    properties_schema = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(properties_schema)\n",
    "# print(filtered_template_data)\n",
    "# print(template_data)\n",
    "\n",
    "# replace|||value|||C:\\OrderWise Data\\||https://storage.googleapis.com/edge-assets/8dddba20-2928-45b2-a26f-381515f9ea68/catalog-edge/dam/public/|;|replace|||value||| ||%20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO Support for multiple rules\n",
    "\n",
    "# def apply_transformation(value, transformation_rules: str):\n",
    "#     \"\"\"\n",
    "#     Apply multiple transformation rules to a value with comprehensive error handling.\n",
    "#     Rules are separated by |;| character sequence.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Handle None/empty values\n",
    "#         if value is None or transformation_rules is None:\n",
    "#             return value\n",
    "            \n",
    "#         # Ensure transformation_rules is a string\n",
    "#         if not isinstance(transformation_rules, str):\n",
    "#             print(f\"WARNING: Invalid transformation rule type: {type(transformation_rules)}\")\n",
    "#             return value\n",
    "        \n",
    "#         # Normalize backslashes in the input value if it's a string\n",
    "#         if isinstance(value, str):\n",
    "#             transformed_value = value.replace('\\\\', '/')\n",
    "#         else:\n",
    "#             transformed_value = value\n",
    "            \n",
    "#         # Split rules by |;| character sequence and process each rule in sequence\n",
    "#         rules = [rule.strip() for rule in transformation_rules.split('|;|')]\n",
    "#         for rule in rules:\n",
    "#             # Basic string transformations\n",
    "#             if rule == \"uppercase\":\n",
    "#                 transformed_value = transformed_value.upper() if isinstance(transformed_value, str) else transformed_value\n",
    "#             elif rule == \"lowercase\":\n",
    "#                 transformed_value = transformed_value.lower() if isinstance(transformed_value, str) else transformed_value\n",
    "#             elif rule == \"strip\":\n",
    "#                 transformed_value = transformed_value.strip() if isinstance(transformed_value, str) else transformed_value\n",
    "#             elif rule == \"split_comma\":\n",
    "#                 if not isinstance(transformed_value, str):\n",
    "#                     continue\n",
    "#                 # Handle potential backslashes in comma-separated values\n",
    "#                 transformed_value = [item.strip().replace('\\\\', '/') \n",
    "#                                    for item in transformed_value.split(\",\") \n",
    "#                                    if item.strip()]\n",
    "                \n",
    "#             elif rule.startswith(\"set|||\"):\n",
    "#                 try:\n",
    "#                     parts = rule.split(\"|||\")\n",
    "#                     if len(parts) != 2:\n",
    "#                         print(f\"WARNING: Invalid set rule format: {rule}. Expected 2 parts.\")\n",
    "#                         continue\n",
    "#                     _, value = parts\n",
    "#                     transformed_value = value\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"WARNING: Error processing set rule: {rule}. Error: {str(e)}\")\n",
    "#                     continue\n",
    "#             elif rule == \"date_only\":\n",
    "#                 if isinstance(transformed_value, (pd.Timestamp, datetime, date)):\n",
    "#                     transformed_value = transformed_value.strftime('%Y-%m-%d')  # Convert datetime to string with only date\n",
    "#                 elif isinstance(transformed_value, str):\n",
    "#                     try:\n",
    "#                         # Handle cases where value is a string with datetime format\n",
    "#                         parsed_date = pd.to_datetime(transformed_value, errors='coerce')\n",
    "#                         if not pd.isnull(parsed_date):  \n",
    "#                             transformed_value = parsed_date.strftime('%Y-%m-%d')\n",
    "#                             print(\"new\",transformed_value)\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"WARNING: Could not parse date for value '{transformed_value}'. Error: {str(e)}\")\n",
    "#             # Complex transformations\n",
    "#             elif rule.startswith(\"replace|||\"):\n",
    "#                 if not isinstance(transformed_value, str):\n",
    "#                     continue\n",
    "#                 try:\n",
    "#                     # Split using ||| delimiter\n",
    "#                     parts = rule.split(\"|||\")\n",
    "#                     if len(parts) != 3:\n",
    "#                         print(f\"WARNING: Invalid replace rule format: {rule}. Expected 3 parts.\")\n",
    "#                         continue\n",
    "                    \n",
    "#                     _, _, replace_rule = parts\n",
    "#                     original, new = replace_rule.split(\"||\", 1)\n",
    "                    \n",
    "#                     original = original.strip().replace('\\\\', '/')\n",
    "#                     new = new.strip().replace('\\\\', '/')\n",
    "#                     original = original.strip().replace('\\\\', '/')\n",
    "#                     new = new.strip().replace('\\\\', '/')\n",
    "#                     transformed_value = transformed_value.replace(original, new)\n",
    "                    \n",
    "#                 except (IndexError, ValueError) as e:\n",
    "#                     print(f\"WARNING: Invalid replace rule format: {rule}. Error: {str(e)}\")\n",
    "#                     continue\n",
    "                    \n",
    "#             elif rule == \"vlookup:map\":\n",
    "#                 if not isinstance(transformed_value, str):\n",
    "#                     continue\n",
    "#                 mapping = {\n",
    "#                     \"yes\": True,\n",
    "#                     \"no\": False,\n",
    "#                     \"true\": True,\n",
    "#                     \"false\": False,\n",
    "#                     \"1\": True,\n",
    "#                     \"0\": False,\n",
    "#                     \"y\": True,\n",
    "#                     \"n\": False\n",
    "#                 }\n",
    "#                 # Convert to lowercase and strip before mapping\n",
    "#                 lookup_value = transformed_value.lower().strip()\n",
    "#                 transformed_value = mapping.get(lookup_value, transformed_value)\n",
    "            \n",
    "#             # New transformation rule for adjusting negative numbers to zero\n",
    "#             elif rule == \"adjust_negative_to_zero\":\n",
    "#                 print(\"transformed_value\",transformed_value)\n",
    "#                 try:\n",
    "#                     if isinstance(transformed_value, (int, float)):\n",
    "#                         transformed_value = max(0, transformed_value)\n",
    "#                     elif isinstance(transformed_value, list):\n",
    "#                         transformed_value = [max(0, float(val)) if isinstance(val, (int, float, str)) else val \n",
    "#                                           for val in transformed_value]\n",
    "#                     elif isinstance(transformed_value, str):\n",
    "#                         try:\n",
    "#                             num_value = float(transformed_value)\n",
    "#                             transformed_value = str(max(0, num_value))\n",
    "#                         except ValueError:\n",
    "#                             print(f\"WARNING: Could not convert string '{transformed_value}' to number for negative adjustment\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"WARNING: Error applying negative adjustment: {str(e)}\")\n",
    "#                     continue\n",
    "            \n",
    "#             else:\n",
    "#                 print(f\"WARNING: Unknown transformation rule: {rule}\")\n",
    "#                 continue\n",
    "        \n",
    "#         return transformed_value\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"ERROR: Transformation failed for value '{value}' with rules '{transformation_rules}': {str(e)}\")\n",
    "#         return value  # Return original value on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime, date\n",
    "\n",
    "\n",
    "# \u2500\u2500\u2500 Sentinel \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "class RejectRow(Exception):\n",
    "    pass\n",
    "\n",
    "# \u2500\u2500\u2500 Transform helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def uppercase(v, **kw): return v.upper() if isinstance(v, str) else v\n",
    "def lowercase(v, **kw): return v.lower() if isinstance(v, str) else v\n",
    "def strip(v, chars=None, **kw): return v.strip(chars) if isinstance(v, str) else v\n",
    "def title_case(v, **kw): return v.title() if isinstance(v, str) else v\n",
    "def capitalize(v, **kw): return v.capitalize() if isinstance(v, str) else v\n",
    "def split_comma(v, **kw): return [x.strip() for x in v.split(',')] if isinstance(v, str) else v\n",
    "def split(v, delimiter, **kw): return v.split(delimiter) if isinstance(v, str) else v\n",
    "def join(v, delimiter, **kw):\n",
    "    if isinstance(v, str): v = v.split()\n",
    "    return delimiter.join(str(x) for x in v) if isinstance(v, (list, tuple)) else v\n",
    "def replace(v, old, new, **kw): return v.replace(old, new) if isinstance(v, str) else v\n",
    "def replace_regex(v, pattern, repl, **kw): return re.sub(pattern, repl, v) if isinstance(v, str) else v\n",
    "def clean_numeric_value(v, **kw): return float(re.sub(r'[^\\d.]', '', v)) if isinstance(v, str) else v\n",
    "def clean_upc(v, **kw): return re.sub(r'[^\\d]', '', v) if isinstance(v, str) else v\n",
    "def clean_html(v, **kw): return re.sub(r'<.*?>', '', v) if isinstance(v, str) else v\n",
    "def date_only(v, **kw):\n",
    "    if isinstance(v, str):\n",
    "        try: return datetime.strptime(v.strip(), \"%Y-%m-%d %H:%M:%S\").strftime(\"%Y-%m-%d\")\n",
    "        except ValueError: return v.split()[0] if \" \" in v else v\n",
    "    if isinstance(v, (datetime, date)): return v.strftime(\"%Y-%m-%d\")\n",
    "    return v\n",
    "def set_value(v, value, **kw): return value\n",
    "def set_number(v, value, **kw): return int(value)\n",
    "def zero_padding(v, value, **kw): return str(v).zfill(int(value))\n",
    "def addition(v, amount, **kw): return float(v) + float(amount)\n",
    "def subtraction(v, amount, **kw): return float(v) - float(amount)\n",
    "def multiplication(v, factor, **kw): return float(v) * float(factor)\n",
    "def division(v, divisor, **kw): return float(v) / float(divisor) if float(divisor) != 0 else None\n",
    "def percentage(v, factor=100, **kw): return float(v) * float(factor)\n",
    "def adjust_negative_to_zero(v, **kw):\n",
    "    try: return max(0, float(v))\n",
    "    except: return v\n",
    "def vlookup_map(v, mapping=None, **kw):\n",
    "    if isinstance(v, str) and mapping: return mapping.get(v.lower(), v)\n",
    "    return v\n",
    "def prefix(v, prefix_str=\"-\", **kw):\n",
    "    if isinstance(v, (list, tuple)): return [prefix_str + str(x) for x in v]\n",
    "    if isinstance(v, str): return prefix_str + v\n",
    "    return v\n",
    "def suffix(v, suffix_str=\"_\", **kw):\n",
    "    if isinstance(v, (list, tuple)): return [str(x) + suffix_str for x in v]\n",
    "    if isinstance(v, str): return v + suffix_str\n",
    "    return v\n",
    "def copy(v, **kw): return v\n",
    "def rejects(v, **kw): raise RejectRow()\n",
    "\n",
    "# \u2500\u2500\u2500 Registry \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "TRANSFORMS = {\n",
    "    \"uppercase\": uppercase, \"lowercase\": lowercase, \"strip\": strip, \"title_case\": title_case,\n",
    "    \"capitalize\": capitalize, \"split_comma\": split_comma, \"split\": split, \"join\": join,\n",
    "    \"replace\": replace, \"replace_regex\": replace_regex, \"clean_numeric_value\": clean_numeric_value,\n",
    "    \"clean_upc\": clean_upc, \"clean_html\": clean_html, \"date_only\": date_only,\n",
    "    \"set\": set_value, \"set_number\": set_number, \"zero_padding\": zero_padding,\n",
    "    \"addition\": addition, \"subtraction\": subtraction, \"multiplication\": multiplication,\n",
    "    \"division\": division, \"percentage\": percentage, \"adjust_negative_to_zero\": adjust_negative_to_zero,\n",
    "    \"vlookup_map\": vlookup_map, \"prefix\": prefix, \"suffix\": suffix,\n",
    "    \"copy\": copy, \"rejects\": rejects\n",
    "}\n",
    "\n",
    "# \u2500\u2500\u2500 Run one pipeline \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def apply_transformations(val, steps):\n",
    "    v = val\n",
    "    for st in steps:\n",
    "        func = TRANSFORMS[st[\"name\"]]\n",
    "        try: v = func(v, **st.get(\"params\", {}))\n",
    "        except RejectRow: return None, True\n",
    "    return v, False\n",
    "\n",
    "# \u2500\u2500\u2500 DSL engine with broadcasting \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def bulk_apply_pipe_rules(values_list, rule_strings):\n",
    "    if not isinstance(values_list, list):\n",
    "        raise ValueError(\"values_list must be a list\")\n",
    "\n",
    "    if isinstance(rule_strings, str):\n",
    "        rule_strings = [rule_strings] * len(values_list)\n",
    "    elif isinstance(rule_strings, list):\n",
    "        if len(rule_strings) == 1 and len(values_list) > 1:\n",
    "            rule_strings *= len(values_list)\n",
    "        elif len(values_list) == 1 and len(rule_strings) > 1:\n",
    "            values_list *= len(rule_strings)\n",
    "        elif len(rule_strings) != len(values_list):\n",
    "            raise ValueError(\"Length mismatch between values_list and rule_strings\")\n",
    "    else:\n",
    "        raise ValueError(\"rule_strings must be str or list[str]\")\n",
    "\n",
    "    results = []\n",
    "    for val, rule in zip(values_list, rule_strings):\n",
    "        if not rule:\n",
    "            results.append(val)\n",
    "            continue\n",
    "\n",
    "        steps = []\n",
    "        for token_raw in rule.replace('+', '|;|').split('|;|'):\n",
    "            token = token_raw.lstrip()\n",
    "            if not token:\n",
    "                continue\n",
    "\n",
    "            # ---------- split ----------\n",
    "            if token.startswith(\"split|\"):\n",
    "                raw = token[6:]\n",
    "                if raw in (\"|||\", r\"\\|\"):\n",
    "                    delim = \"|\"\n",
    "                elif raw.startswith(\"|||\"):\n",
    "                    delim = raw[3:] or \"|\"\n",
    "                elif raw.startswith(\"||\"):\n",
    "                    tail = raw[2:]\n",
    "                    delim = \" \" if tail in (\"\", \" \") else tail\n",
    "                elif raw.startswith(r\"\\|\"):\n",
    "                    delim = \"|\" + raw[2:]\n",
    "                else:\n",
    "                    delim = raw\n",
    "                if delim != \" \": delim = delim.strip()\n",
    "                if delim == \"\": raise ValueError(\"split requires non-empty delimiter\")\n",
    "                steps.append({\"name\": \"split\", \"params\": {\"delimiter\": delim}})\n",
    "                continue\n",
    "\n",
    "            # ---------- join ----------\n",
    "            if token.startswith(\"join|\"):\n",
    "                steps.append({\"name\": \"join\", \"params\": {\"delimiter\": token[5:]}})\n",
    "                continue\n",
    "\n",
    "            # ---------- prefix / suffix ----------\n",
    "            if token.startswith(\"prefix|\"):\n",
    "                steps.append({\"name\": \"prefix\", \"params\": {\"prefix_str\": token[7:]}})\n",
    "                continue\n",
    "            if token.startswith(\"suffix|\"):\n",
    "                steps.append({\"name\": \"suffix\", \"params\": {\"suffix_str\": token[7:]}})\n",
    "                continue\n",
    "\n",
    "            # ---------- replace_regex ----------\n",
    "            if token.startswith(\"replace_regex|\"):\n",
    "                pat, *rep = token[14:].split(\"||\", 1)\n",
    "                steps.append({\"name\": \"replace_regex\",\n",
    "                              \"params\": {\"pattern\": pat, \"repl\": rep[0] if rep else \"\"}})\n",
    "                continue\n",
    "\n",
    "            # ---------- replace ----------\n",
    "            if token.startswith(\"replace|\"):\n",
    "                try:\n",
    "                    old, new = token[8:].split('|', 1)\n",
    "                except ValueError:\n",
    "                    raise ValueError(f\"replace requires two parameters: {token}\")\n",
    "                steps.append({\"name\": \"replace\",\n",
    "                              \"params\": {\"old\": old, \"new\": new}})\n",
    "                continue\n",
    "\n",
    "            # ---------- vlookup ----------\n",
    "            if token.startswith(\"vlookup|\"):\n",
    "                mapping = {}\n",
    "                for pair in token[8:].rstrip('|').split(','):\n",
    "                    k, v = pair.split(':', 1)\n",
    "                    v = v.strip()\n",
    "                    mapping[k.strip().lower()] = \\\n",
    "                        True if v.lower() == \"true\" else \\\n",
    "                        False if v.lower() == \"false\" else \\\n",
    "                        float(v) if re.match(r'^-?\\d+\\.\\d+$', v) else \\\n",
    "                        int(v) if v.isdigit() else v\n",
    "                steps.append({\"name\": \"vlookup_map\", \"params\": {\"mapping\": mapping}})\n",
    "                continue\n",
    "\n",
    "            # ---------- arithmetic ----------\n",
    "            for op in (\"addition\", \"subtraction\", \"multiplication\", \"division\", \"percentage\"):\n",
    "                if token.startswith(f\"{op}|\"):\n",
    "                    param = token.split('|', 1)[1]\n",
    "                    key = (\"amount\" if op in (\"addition\", \"subtraction\") else\n",
    "                           \"factor\" if op in (\"multiplication\", \"percentage\") else\n",
    "                           \"divisor\")\n",
    "                    steps.append({\"name\": op, \"params\": {key: param}})\n",
    "                    break\n",
    "            else:\n",
    "                # ---------- strip with chars / set / zero_padding ----------\n",
    "                if token.startswith(\"strip|\"):\n",
    "                    steps.append({\"name\": \"strip\",\n",
    "                                  \"params\": {\"chars\": token.split('|',1)[1]}})\n",
    "                elif token.startswith(\"set|\"):\n",
    "                    steps.append({\"name\": \"set\",\n",
    "                                  \"params\": {\"value\": token.split('|',1)[1]}})\n",
    "                elif token.startswith(\"set_number|\"):\n",
    "                    steps.append({\"name\": \"set_number\",\n",
    "                                  \"params\": {\"value\": token.split('|',1)[1]}})\n",
    "                elif token.startswith(\"zero_padding|\"):\n",
    "                    steps.append({\"name\": \"zero_padding\",\n",
    "                                  \"params\": {\"value\": token.split('|',1)[1]}})\n",
    "                elif token == \"rejects\":\n",
    "                    steps.append({\"name\": \"rejects\"})\n",
    "                else:\n",
    "                    steps.append({\"name\": token})\n",
    "\n",
    "        out, _ = apply_transformations(val, steps)\n",
    "        results.append(out)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def apply_transformation(value, transformation_rules: str):\n",
    "    \"\"\"\n",
    "    Adapter that preserves your old call site but normalizes legacy tokens:\n",
    "    - 'set|||VALUE'        -> 'set|VALUE'\n",
    "    - 'replace|||OLD||NEW' -> 'replace|OLD|NEW'   (optional but handy)\n",
    "    Then forwards to bulk_apply_pipe_rules unchanged.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if transformation_rules is None:\n",
    "            return value\n",
    "        if not isinstance(transformation_rules, str):\n",
    "            print(f\"WARNING: Invalid transformation rule type: {type(transformation_rules)}\")\n",
    "            return value\n",
    "\n",
    "        # Optional: keep your backslash \u2192 slash normalization\n",
    "        if isinstance(value, str):\n",
    "            value = value.replace(\"\\\\\", \"/\")\n",
    "\n",
    "        # --- Normalize legacy rule formats (minimal, safe) ---\n",
    "        fixed_rules = transformation_rules\n",
    "\n",
    "        # set|||VALUE  -> set|VALUE\n",
    "        fixed_rules = re.sub(r'(?<!\\w)set\\|\\|\\|', 'set|', fixed_rules)\n",
    "\n",
    "        # replace|||OLD||NEW -> replace|OLD|NEW\n",
    "        # (only converts the first '||' after 'replace|||' to keep the rest intact)\n",
    "        fixed_rules = re.sub(r'(?<!\\w)replace\\|\\|\\|([^|]+)\\|\\|', r'replace|\\1|', fixed_rules)\n",
    "\n",
    "        # Hand off directly to your DSL (no extra splitting here)\n",
    "        return bulk_apply_pipe_rules([value], fixed_rules)[0]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Transformation failed for value '{value}' with rules '{transformation_rules}': {str(e)}\")\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_value(value, data_type):\n",
    "    import datetime\n",
    "\n",
    "    \"\"\"\n",
    "    Converts the given value to the specified data type.\n",
    "    If the value is already correct, it remains unchanged.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if data_type == \"text\":\n",
    "            return str(value) if value is not None else \"\"\n",
    "\n",
    "        elif data_type == \"integer\":\n",
    "            return int(value) if isinstance(value, (int, float, str)) and str(value).isdigit() else 0\n",
    "\n",
    "        elif data_type == \"float\":\n",
    "            return float(value) if isinstance(value, (int, float, str)) and str(value).replace('.', '', 1).isdigit() else 0.0\n",
    "\n",
    "        elif data_type == \"boolean\":\n",
    "            return str(value).lower() in [\"true\", \"1\", \"yes\"]\n",
    "\n",
    "        elif data_type == \"datetime\":\n",
    "            if isinstance(value, (datetime.datetime, datetime.date, pd.Timestamp)):\n",
    "                return value.strftime('%Y-%m-%d %H:%M:%S')  # Convert datetime to string\n",
    "            try: \n",
    "                return str(pd.to_datetime(value))  # Convert if possible\n",
    "            except:\n",
    "                return str(value)\n",
    "        elif data_type == \"timestamp\":\n",
    "            if isinstance(value, (datetime.datetime, datetime.date, pd.Timestamp)):\n",
    "                return value.strftime('%Y-%m-%d %H:%M:%S')  # Convert datetime to string\n",
    "            try: \n",
    "                return str(pd.to_datetime(value))  # Convert if possible\n",
    "            except:\n",
    "                return str(value)\n",
    "        \n",
    "                \n",
    "        elif data_type == \"date\":\n",
    "            if isinstance(value, (datetime.datetime, datetime.date, pd.Timestamp)):\n",
    "                return value.strftime('%Y-%m-%d ')  # Convert timestamp to YYYY-MM-DD (No time)\n",
    "            try:\n",
    "                return pd.to_datetime(value).strftime('%Y-%m-%d')  # Convert if possible\n",
    "            except:\n",
    "                return str(value)\n",
    "\n",
    "        elif data_type == \"jsonb\":\n",
    "            if isinstance(value, str):\n",
    "                try:\n",
    "                    return json.loads(value)  # Convert JSON string to dict\n",
    "                except json.JSONDecodeError:\n",
    "                    return {}  # Return empty dict if invalid JSON\n",
    "            return value  # If already dict, return as is\n",
    "\n",
    "        elif data_type == \"array\":\n",
    "            return value if isinstance(value, list) else [value]  # Ensure it's always a list\n",
    "\n",
    "        else:\n",
    "            return value  # Keep as is for unknown data types\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Failed to convert value '{value}' to {data_type}: {e}\")\n",
    "        return value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_request_objects(data_df, filtered_template_data):\n",
    "    request_objects = []\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        for index, row in data_df.iterrows():\n",
    "            \n",
    "            \n",
    "            request_obj = {}\n",
    "            # Convert row to dictionary and handle NaN values\n",
    "            row_dict = {\n",
    "                k: (None if pd.isna(v) else v) \n",
    "                for k, v in row.to_dict().items()\n",
    "            }\n",
    "            \n",
    "            for col_name in data_df.columns:\n",
    "                \n",
    "                if col_name not in filtered_template_data:\n",
    "                    continue  # column not in template, skip\n",
    "\n",
    "                value = row_dict.get(col_name)\n",
    "                # if value is None or (isinstance(value, str) and value.strip() == \"\"):\n",
    "                #     continue  # value is empty, skip\n",
    "\n",
    "                field_config = filtered_template_data[col_name]\n",
    "                if not field_config or not field_config.get(\"field_mapping\"):\n",
    "                    continue\n",
    "                try:\n",
    "                    if pd.notnull(value):\n",
    "                        # Only clean if it's numeric (int/float) and ends in .0\n",
    "                        if isinstance(value, (int, float)) and float(value).is_integer():\n",
    "                            value = str(int(value))  # 123456.0 \u2192 \"123456\"\n",
    "                        else:\n",
    "                            value = str(value).strip()  # Keep as-is for strings like \"123456.0lkw\"\n",
    "                except Exception as e:\n",
    "                    print(f\"WARNING: Error cleaning value '{value}': {e}\")\n",
    "\n",
    "                if value is None:\n",
    "                    value = None\n",
    "                \n",
    "                field_mapping = field_config['field_mapping']\n",
    "                accepts_multiple = field_config.get('accepts_multiple_values', False)\n",
    "                transformation_function = field_config.get(\"transformation_function\", \"\")\n",
    "                data_type = field_config.get(\"data_type\", \"\")\n",
    "                \n",
    "\n",
    "                if data_type:\n",
    "                    value = convert_value(value, data_type)\n",
    "                   \n",
    "                # trasnformation needs to be applied to value\n",
    "                if transformation_function:\n",
    "                    value = apply_transformation(value, transformation_function)\n",
    "                    # while isinstance(temp_value, list) and temp_value:\n",
    "                    #     value = temp_value[0]\n",
    "                \n",
    "                # Handle nested mappings (e.g., \"raw_product_data.field_name\")\n",
    "                if \".\" in field_mapping:\n",
    "                    parent_key, child_key = field_mapping.split(\".\", 1)\n",
    "                    \n",
    "                    # Initialize parent dictionary if it doesn't exist\n",
    "                    if parent_key not in request_obj:\n",
    "                        request_obj[parent_key] = {}\n",
    "                        \n",
    "                    request_obj[parent_key][child_key] = value.strip() if isinstance(value, str) else value\n",
    "                else:\n",
    "                    # Handle non-nested fields\n",
    "                    if accepts_multiple and isinstance(value, str):\n",
    "                        request_obj[field_mapping] = [v.strip() for v in value.split(',') if v.strip()]\n",
    "                    else:\n",
    "                        request_obj[field_mapping] = value.strip() if isinstance(value, str) else value\n",
    "            \n",
    "            request_objects.append(request_obj)\n",
    "            \n",
    "        print(f\"INFO: Successfully created {len(request_objects)} request objects\")\n",
    "        return request_objects\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to build request objects: {str(e)}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "request_objects = build_request_objects(data_df, filtered_template_data)\n",
    "# print(request_objects)\n",
    "# print(data_df)\n",
    "# print(filtered_template_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update job details\n",
    "def update_job_details(job_id, total_count,success_count, failure_count, failed_url=None,failed_report=None):\n",
    "    \"\"\"\n",
    "    Update job details in the database with success/failure counts and failed job summary link\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        job_response = {\n",
    "            \"total\": total_count,\n",
    "            \"success\": success_count,\n",
    "            \"failed\": failure_count,\n",
    "        }\n",
    "\n",
    "        update_fields = [\"job_response = %s\"]\n",
    "        update_values = [json.dumps(job_response)]\n",
    "        \n",
    "\n",
    "        # Add failed_job_summary_link if present\n",
    "        if failed_url:\n",
    "            update_fields.append(\"failed_job_summary_link = %s\")\n",
    "            update_values.append(failed_url)\n",
    "\n",
    "        # Add job_id for WHERE clause\n",
    "        update_values.append(job_id)\n",
    "        print(\"update_values\",update_values)\n",
    "\n",
    "        query = f\"\"\"\n",
    "        UPDATE saas_edge_jobs\n",
    "        SET {\", \".join(update_fields)}\n",
    "        WHERE request_id = %s\n",
    "        RETURNING *;\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(query, update_values)\n",
    "        updated_row = cursor.fetchone()\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"INFO: Successfully updated job details for job_id: {job_id}\")\n",
    "        return updated_row\n",
    "\n",
    "    except Exception as e:\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "        print(f\"ERROR: Failed to update job details: {str(e)}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import os, re, json, mimetypes, hashlib, urllib.parse, traceback\n",
    "from io import BytesIO\n",
    "GCP_BUCKET_NAME = \"edge-assets\"\n",
    "credentials = {\n",
    "  \"type\": \"service_account\",\n",
    "  \"project_id\": \"saastify-base-wm\",\n",
    "  \"private_key_id\": \"d3ed03651cb920c119a78e2434459a52d3b9f541\",\n",
    "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEuwIBADANBgkqhkiG9w0BAQEFAASCBKUwggShAgEAAoIBAQC+M2925iyNZ34w\\n5UeOyN3LDo+OncMeHEjyBGnx7xtcBBv9/4rmrYn3cLu6iD31jrnhIkcjju6K2Fdz\\nOgNaXqQkoIT7OGqREh5ouFZAQQJWoy153BiyL8eUcd5CzBTMDTW2MpaOo4Y2jOii\\nzM6w94+Ajw7KoWHOi4o8KuItM9IDw394j2kCLeUFe0k2PQ/JNAJa9uKQgOd7bmAj\\nPhlDJuLRQPvpFOw93MLgPRi8JkSF88HeB/AiT/7Awnh7xAtaj36Kmycub2vIEufh\\nnKZ/U+0DEqhfLHSo9EixY+PCJSZsxjgwchDQQgyGB75zcuZVX/ArwzopFyGYO/uK\\nJCXxTNE7AgMBAAECgf8WqQoeoE2uiyX9rVNZL5U9G/7+fs1ASR5ntx7oNBSOYe7z\\n0/44fXRyhnvXPWQkXVzH9c2D7wN8h0nj8IV1vtDPjFBLne0UW5RD5bJg9V3R9J72\\nZcKLeCXPCcHxM19G8Ev16REG7XSQCzmsK7p0Wwo9xs18Vr3QXc+aW4GW4RWkXPG6\\nhoWOD6If0NtHVRZZqXEtSqShepptSz5ezcNGIgW5ksWeciMqziQssCFVbCM6E88+\\nADPHiB1VgtQvd3jqbh9KoUHEPSSZ9Lcp54k8bRx/9hpB9l9AtlQP8lh0oVYdVbuX\\n5+Eko6jH1tF/YELobR4TXnUqZg15z5g1y8vHkfUCgYEA4iRRYBRUEMczWlvOW/PC\\nvyO6JmoQVBHhU8J/6PfG2v+9c+sCf+e9dn4HTuPnjdI8QLy9VC1NR3/wXqCCf/Ea\\nIXPitUq3Gsezl01Zp2m7i3y8d4HZNoan9tcHMdIQ7bm17ZBbN/RE3Hw2dx/O+kFY\\nGvDhTTrw+GLfGPGnfiBLBK0CgYEA11BMuIQ8+HVD5aqux0jb7HAHsDYz24v87Z9L\\nwocBNy0J2lAtpiQ+SFs/0g5cVOyqm9MlThBshgUDGDUaczfqjZaz13rWc+qPBAjD\\nawUf9QfTO53UQIZP1dgNuybXBEi1PqMfd+yeUkjIDfF7+ntfwDr/BdH6XJ9tFMeR\\nlmUEAocCgYEApxW6Ykjiy/rCgJKwZ9Q1IdCd62AWbGdBmwdsRo88B/dI3WrYT/TD\\nUddQQwO0xF5/Uj2hjZ5jKN7olKH3idx0OB9NdDGeFFVU5geqpD1E6ozhG1N/UAAx\\n/flmQXM6OssqFjrAixkZ/+Zuv5lq7hB1roInlU5lWMCEogN6g4AMrYkCgYAI6ckT\\nRl4jxu71nfg4Rbrc8dJPqB7DcusYhySiu+X/+7xRrkoFe7CcXDKrJm8KEPYLF1WP\\nAr0LWz/Ci8g5htIN5HQzcmFYURh0iUxVrNOi2B0VdbYoqaa6aoQ/AB+cjMn7+tK9\\nqyzuqRanBR0lxF+1XHvcKNIdbXgdiRlsyWe+FwKBgHDn/KAFQ5yHEPUC5IUU8ktB\\no9W5eS33NGnsVZ6PKqncyGD2WUdpqdQeMSYQb/OSxULr49HVM71lrhiyqGHMsgb0\\nY/K62jVmlhnhDQ27F36rBojyidQPA+NxDr/8jgS78hktnydzh2j6cnMWGm3tJpmY\\nivU2NdIrfBP4gzRJXlzm\\n-----END PRIVATE KEY-----\\n\",\n",
    "  \"client_email\": \"wm-bulk-job-mgr@saastify-base-wm.iam.gserviceaccount.com\",\n",
    "  \"client_id\": \"104148745789000738235\",\n",
    "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/wm-bulk-job-mgr%40saastify-base-wm.iam.gserviceaccount.com\",\n",
    "  \"universe_domain\": \"googleapis.com\"\n",
    "}\n",
    "\n",
    "\n",
    "from difflib import SequenceMatcher, get_close_matches\n",
    "import re\n",
    "\n",
    "\n",
    "from decimal import Decimal, InvalidOperation\n",
    "\n",
    "\n",
    "def clean_value(val):\n",
    "    if val in (\"\", None, \"null\", \"NULL\"):\n",
    "        return None\n",
    "    return val\n",
    "\n",
    "\n",
    "\n",
    "class ProductDataManager:\n",
    "    def __init__(self, saas_edge_id, create_new_product=False, conn=None):\n",
    "        self.saas_edge_id = saas_edge_id\n",
    "        self.job_id=job_id\n",
    "        self.create_new_product = create_new_product\n",
    "        self.conn = conn or get_db_connection()\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.table_schema = \"public\"\n",
    "\n",
    "        # category caches\n",
    "        self.all_categories_with_paths = []\n",
    "        self.leaf_categories = []\n",
    "\n",
    "        print(f\"INFO: Initialized ProductDataManager for saas_edge_id: {saas_edge_id}\")\n",
    "        self._preload_category_tree()\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.cursor:\n",
    "            self.cursor.close()\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            print(\"INFO: Database connection closed\")\n",
    "\n",
    "    def _is_blank(self, v):\n",
    "        return v is None or (isinstance(v, str) and v.strip() == \"\") or v in (\"null\", \"NULL\")\n",
    "    \n",
    "    def _coalesce_nonblank(self, *vals):\n",
    "        for v in vals:\n",
    "            if not self._is_blank(v):\n",
    "                return v\n",
    "        return None\n",
    "    \n",
    "    def _to_decimal(self, v, default=None):\n",
    "        try:\n",
    "            if self._is_blank(v):\n",
    "                return default\n",
    "            return float(str(v))\n",
    "        except Exception:\n",
    "            return default\n",
    "\n",
    "    def get_or_create_product(self, sku):\n",
    "        \"\"\"Get existing product_id or create new product and return its id\"\"\"\n",
    "        try:\n",
    "            # Check if product exists\n",
    "            query = \"\"\"\n",
    "            SELECT product_id FROM products \n",
    "            WHERE sku = %s AND saas_edge_id = %s;\n",
    "            \"\"\"\n",
    "            self.cursor.execute(query, (sku, self.saas_edge_id))\n",
    "            result = self.cursor.fetchone()\n",
    "            \n",
    "            if result:\n",
    "                print(f\"INFO: Found existing product with SKU: {sku}\")\n",
    "                return result[0]\n",
    "            \n",
    "            if not self.create_new_product:\n",
    "                print(f\"WARNING: Product with SKU {sku} not found and create_new_product is False\")\n",
    "                return None\n",
    "                \n",
    "            # Create new product if not exists\n",
    "            query = \"\"\"\n",
    "            INSERT INTO products (sku, saas_edge_id, last_update_ts)\n",
    "            VALUES (%s, %s, CURRENT_TIMESTAMP)\n",
    "            RETURNING product_id;\n",
    "            \"\"\"\n",
    "            self.cursor.execute(query, (sku, self.saas_edge_id))\n",
    "            product_id = self.cursor.fetchone()[0]\n",
    "            self.conn.commit()\n",
    "            print(f\"INFO: Created new product with SKU: {sku}, product_id: {product_id}\")\n",
    "            return product_id\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            print(f\"ERROR: Failed to get/create product: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def upload_asset_to_gcp_bucket(self,file_data, file_name, gcp_bucket_name, creds, subdirectory):\n",
    "        try:\n",
    "            credentials = service_account.Credentials.from_service_account_info(creds)\n",
    "            client = storage.Client(credentials=credentials)\n",
    "            bucket = client.bucket(gcp_bucket_name)\n",
    "            blob = bucket.blob(f\"{subdirectory}/{file_name}\")\n",
    "            ct, _ = mimetypes.guess_type(file_name)\n",
    "            content_type = ct or \"application/octet-stream\"\n",
    "            if blob.exists():\n",
    "                blob.delete()\n",
    "            if isinstance(file_data, (list, dict)):\n",
    "                    # Convert to JSON string for lists and dictionaries\n",
    "                upload_data = json.dumps(file_data, indent=2)\n",
    "                blob.upload_from_string(upload_data, content_type='application/json')\n",
    "            elif isinstance(file_data, str):\n",
    "                blob.upload_from_string(file_data, content_type='text/plain')\n",
    "            elif isinstance(file_data, (bytes, bytearray)):\n",
    "                blob.upload_from_string(file_data, content_type=content_type)\n",
    "            else:\n",
    "            # file-like: rewind & upload with explicit MIME\n",
    "                if hasattr(file_data, \"seek\"):\n",
    "                    file_data.seek(0)\n",
    "                blob.upload_from_file(file_data,\n",
    "                                      content_type=content_type,\n",
    "                                      rewind=True)\n",
    "            # blob.upload_from_file(file_data, rewind=True, content_type='application/octet-stream')\n",
    "            a=f\"https://storage.googleapis.com/{gcp_bucket_name}/{subdirectory}/{file_name}\"\n",
    "            \n",
    "            return a\n",
    "        except Exception as e:\n",
    "            # logger.info(f\"Error uploading file to GCP: {e}\")\n",
    "            print(f\"Error uploading file to GCP: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # def _normalize_option_input_type(self, s: str = None ) -> str:\n",
    "    #     \"\"\"\n",
    "    #     Canonicalize UI type names \u2192 one of: 'Swatch', 'Rectangle', 'Dropdown', 'Radio'.\n",
    "    #     Accepts common synonyms.\n",
    "    #     \"\"\"\n",
    "    #     if not s:\n",
    "    #         return \"Dropdown\"\n",
    "    #     t = str(s).strip().lower()\n",
    "    #     if t in {\"swatch\", \"colour-swatch\", \"color-swatch\"}:\n",
    "    #         return \"Swatch\"\n",
    "    #     if t in {\"rectangle\", \"rect\", \"pill\", \"tag\", \"tile\", \"chip\"}:\n",
    "    #         return \"Rectangle\"\n",
    "    #     if t in {\"dropdown\", \"select\", \"menu\"}:\n",
    "    #         return \"Dropdown\"\n",
    "    #     if t in {\"radio\", \"boolean\", \"yes/no\", \"yesno\"}:\n",
    "    #         return \"Radio\"\n",
    "    #     # default\n",
    "    #     return \"Dropdown\"\n",
    "\n",
    "    # import re\n",
    "\n",
    "    # def _infer_option_input_type(self, option_name: str, option_value: str = None) -> str:\n",
    "    #     \"\"\"\n",
    "    #     Heuristics to pick an input_type when none is provided.\n",
    "    #     Returns one of: 'Swatch', 'Rectangle', 'Dropdown', 'Radio'.\n",
    "    #     \"\"\"\n",
    "    #     n = (option_name or \"\").strip().lower()\n",
    "    #     v = (option_value or \"\").strip().lower() if option_value else \"\"\n",
    "    \n",
    "    #     common_colors = {\n",
    "    #         \"black\",\"white\",\"red\",\"blue\",\"navy\",\"green\",\"yellow\",\"orange\",\"purple\",\"pink\",\"grey\",\"gray\",\n",
    "    #         \"brown\",\"beige\",\"silver\",\"gold\",\"maroon\",\"teal\",\"turquoise\",\"cyan\",\"magenta\",\"olive\",\"indigo\",\n",
    "    #         \"charcoal\",\"ivory\",\"cream\",\"khaki\",\"tan\"\n",
    "    #     }\n",
    "    \n",
    "    #     color_keywords    = (\"color\",\"colour\",\"shade\",\"tone\",\"hue\")\n",
    "    #     pattern_keywords  = (\"pattern\",\"finish\",\"texture\",\"print\",\"graphic\")\n",
    "    #     size_keywords     = (\n",
    "    #         \"size\",\"casual size\",\"apparel size\",\"shoe size\",\"footwear size\",\"waist\",\"chest\",\"length\",\n",
    "    #         \"width\",\"height\",\"inseam\",\"neck\",\"sleeve\",\"hip\",\"bust\"\n",
    "    #     )\n",
    "    #     material_keywords = (\"material\",\"fabric\",\"composition\",\"fibre\",\"fiber\",\"shell\",\"lining\")\n",
    "    #     rectangle_keywords= (\"fit\",\"style\",\"cut\",\"theme\",\"collection\",\"grade\",\"rating\",\"flavor\",\"flavour\",\"scent\",\"pack\",\"pack size\",\"model\",\"type\")\n",
    "    \n",
    "    #     # 1) explicit color \u2192 Swatch\n",
    "    #     if any(k in n for k in color_keywords):\n",
    "    #         return \"Swatch\"\n",
    "    \n",
    "    #     # 2) pattern/finish/texture:\n",
    "    #     #    - if value looks like a color/hex \u2192 Swatch\n",
    "    #     #    - otherwise \u2192 Rectangle (tiles with names like \"Matte\", \"Houndstooth\")\n",
    "    #     if any(k in n for k in pattern_keywords):\n",
    "    #         if v in common_colors or v.startswith(\"#\") and re.fullmatch(r\"#?[0-9a-f]{3,8}\", v):\n",
    "    #             return \"Swatch\"\n",
    "    #         return \"Rectangle\"\n",
    "    \n",
    "    #     # 3) size-like \u2192 Dropdown\n",
    "    #     if any(k in n for k in size_keywords):\n",
    "    #         return \"Dropdown\"\n",
    "    \n",
    "    #     # 4) material-like \u2192 Dropdown\n",
    "    #     if any(k in n for k in material_keywords):\n",
    "    #         return \"Dropdown\"\n",
    "    \n",
    "    #     # 5) rectangle-ish categories (small discrete labels visualized as tiles)\n",
    "    #     if any(k in n for k in rectangle_keywords):\n",
    "    #         return \"Rectangle\"\n",
    "    \n",
    "    #     # 6) value-based hints\n",
    "    #     if v in {\"yes\",\"no\",\"true\",\"false\"}:\n",
    "    #         return \"Radio\"\n",
    "    #     if v.startswith(\"#\") and re.fullmatch(r\"#?[0-9a-f]{3,8}\", v):\n",
    "    #         return \"Swatch\"\n",
    "    #     if v in common_colors:\n",
    "    #         return \"Swatch\"\n",
    "    \n",
    "    #     # default\n",
    "    #     return \"Dropdown\"\n",
    "\n",
    "\n",
    "    def _preload_category_tree(self):\n",
    "        try:\n",
    "            self.cursor.execute(\"\"\"\n",
    "                SELECT category_id, name, parent_id\n",
    "                FROM product_categories\n",
    "                WHERE saas_edge_id = %s\n",
    "            \"\"\", (str(self.saas_edge_id),))\n",
    "            rows = self.cursor.fetchall()\n",
    "            cat_dict = {r[0]: {\"name\": r[1], \"parent_id\": r[2]} for r in rows}\n",
    "\n",
    "            def build_path(cat_id):\n",
    "                path = []\n",
    "                while cat_id:\n",
    "                    c = cat_dict[cat_id]\n",
    "                    path.insert(0, c[\"name\"])\n",
    "                    cat_id = c[\"parent_id\"]\n",
    "                return path\n",
    "\n",
    "            cats = []\n",
    "            for cid in cat_dict:\n",
    "                try:\n",
    "                    cats.append({\n",
    "                        \"category_id\": cid,\n",
    "                        \"parent_id\": cat_dict[cid][\"parent_id\"],\n",
    "                        \"path\": build_path(cid)\n",
    "                    })\n",
    "                except KeyError as e:\n",
    "                    print(f\" Skipping category {cid} due to missing parent: {e}\")\n",
    "\n",
    "            self.all_categories_with_paths = cats\n",
    "            parent_ids = {c[\"parent_id\"] for c in cats if c[\"parent_id\"] is not None}\n",
    "            self.leaf_categories = [c for c in cats if c[\"category_id\"] not in parent_ids]\n",
    "            print(f\"INFO: Preloaded {len(self.leaf_categories)} leaf categories\")\n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            print(f\"WARN: category preload failed: {e}\")\n",
    "            self.all_categories_with_paths = []\n",
    "            self.leaf_categories = []\n",
    "\n",
    "    def _normalize_for_match(self, text: str) -> str:\n",
    "        \"\"\"lowercase, &\u2192and, strip punctuation & extra spaces\"\"\"\n",
    "        if text is None:\n",
    "            return \"\"\n",
    "        s = str(text).lower().replace(\"&\", \"and\")\n",
    "        s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "        return \" \".join(s.split())\n",
    "\n",
    "    def _find_category_by_label(self, label: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Input is a single leaf label, e.g. 'Janitorial Supplies'.\n",
    "        Ranking:\n",
    "          1) exact match on leaf\n",
    "          2) exact match on any node in path\n",
    "          3) 'contains' on leaf\n",
    "          4) 'contains' on any node\n",
    "          5) fuzzy (difflib) on leaf, then any node   (threshold 0.65)\n",
    "        Prefer deeper paths within each tier.\n",
    "        \"\"\"\n",
    "        if not label:\n",
    "            return None\n",
    "        norm = self._normalize_for_match(label)\n",
    "\n",
    "        cats = self.all_categories_with_paths or []\n",
    "        parent_ids = {c[\"parent_id\"] for c in cats if c[\"parent_id\"] is not None}\n",
    "        def is_leaf(cat): return cat[\"category_id\"] not in parent_ids\n",
    "\n",
    "        def depth_key(cat): return len(cat.get(\"path\") or [])\n",
    "\n",
    "        tier1 = []; tier2 = []; tier3 = []; tier4 = []\n",
    "        for c in cats:\n",
    "            path = c.get(\"path\") or []\n",
    "            if not path:\n",
    "                continue\n",
    "            leaf = path[-1]\n",
    "            n_leaf = self._normalize_for_match(leaf)\n",
    "            n_any = {self._normalize_for_match(p) for p in path}\n",
    "\n",
    "            if norm in n_any:\n",
    "                if norm == n_leaf:\n",
    "                    tier1.append(c)  # exact leaf\n",
    "                else:\n",
    "                    tier2.append(c)  # exact non-leaf\n",
    "            else:\n",
    "                if norm and norm in n_leaf:\n",
    "                    tier3.append(c)  # contains leaf\n",
    "                elif any(norm and norm in n for n in n_any):\n",
    "                    tier4.append(c)  # contains non-leaf\n",
    "\n",
    "        for tier in (tier1, tier2, tier3, tier4):\n",
    "            if tier:\n",
    "                return sorted(tier, key=depth_key, reverse=True)[0]\n",
    "\n",
    "        # fuzzy fallback on leaf names\n",
    "        leaf_names = [c[\"path\"][-1] for c in cats if c.get(\"path\")]\n",
    "        best = get_close_matches(label, leaf_names, n=1, cutoff=0.65)\n",
    "        if best:\n",
    "            picked = best[0]\n",
    "            for c in cats:\n",
    "                if c.get(\"path\") and c[\"path\"][-1] == picked:\n",
    "                    score = SequenceMatcher(None, label.lower(), picked.lower()).ratio()\n",
    "                    print(f\"INFO: fuzzy category match '{label}' \u2192 '{picked}' (score={score:.2f})\")\n",
    "                    return c\n",
    "\n",
    "        # fuzzy on any node\n",
    "        all_nodes = list({p for c in cats for p in (c.get(\"path\") or [])})\n",
    "        best = get_close_matches(label, all_nodes, n=1, cutoff=0.65)\n",
    "        if best:\n",
    "            picked = best[0]\n",
    "            for c in cats:\n",
    "                if c.get(\"path\") and picked in c[\"path\"]:\n",
    "                    score = SequenceMatcher(None, label.lower(), picked.lower()).ratio()\n",
    "                    print(f\"INFO: fuzzy category match '{label}' \u2192 '{picked}' (score={score:.2f})\")\n",
    "                    return c\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _map_product_to_category_id(self, product_id: int, category_id: int) -> bool:\n",
    "        \"\"\"\n",
    "        Safe UPSERT without ON CONFLICT (works even if no unique index).\n",
    "        \"\"\"\n",
    "        now_iso = datetime.now().isoformat()\n",
    "        # check existing\n",
    "        self.cursor.execute(\"\"\"\n",
    "            SELECT id FROM saas_category_product_mapping\n",
    "            WHERE product_id=%s AND category_id=%s AND saas_edge_id=%s\n",
    "            LIMIT 1\n",
    "        \"\"\", (product_id, category_id, self.saas_edge_id))\n",
    "        row = self.cursor.fetchone()\n",
    "        if row:\n",
    "            # update the timestamp\n",
    "            self.cursor.execute(\"\"\"\n",
    "                UPDATE saas_category_product_mapping\n",
    "                SET updated_at=%s\n",
    "                WHERE id=%s\n",
    "            \"\"\", (now_iso, row[0]))\n",
    "            print(f\"\u2713 Product-category (existing) id={row[0]} updated for product={product_id}, cat={category_id}\")\n",
    "            return True\n",
    "\n",
    "        # insert\n",
    "        self.cursor.execute(\"\"\"\n",
    "            INSERT INTO saas_category_product_mapping\n",
    "            (product_id, category_id, saas_edge_id, created_at, updated_at)\n",
    "            VALUES (%s, %s, %s, %s, %s)\n",
    "            RETURNING id\n",
    "        \"\"\", (product_id, category_id, self.saas_edge_id, now_iso, now_iso))\n",
    "        new_row = self.cursor.fetchone()\n",
    "        if new_row:\n",
    "            print(f\"\u2713 Product-category (new) id={new_row[0]} for product={product_id}, cat={category_id}\")\n",
    "            return True\n",
    "        print(\"\u2717 Failed to create product-category mapping\")\n",
    "        return False\n",
    "\n",
    "    def _map_product_to_category_label(self, product_id: int, label: str) -> bool:\n",
    "        match = self._find_category_by_label(label)\n",
    "        print(\"matching\",match)\n",
    "        if not match:\n",
    "            print(f\"\u2717 Category label not found: {label}\")\n",
    "            return False\n",
    "        return self._map_product_to_category_id(product_id, match[\"category_id\"])\n",
    "\n",
    "    def _normalize_label(self, s):\n",
    "        if s is None:\n",
    "            return \"\"\n",
    "        s = str(s)\n",
    "        s = s.replace(\"\\u00A0\", \" \")            # NBSP \u2192 space\n",
    "        s = s.replace(\"&\", \"and\")               # unify ampersand\n",
    "        s = re.sub(r\"\\s+\", \" \", s.strip())      # collapse/trim spaces\n",
    "        return s.lower()\n",
    "    \n",
    "    def _find_category_id_ci(self, name, parent_id):\n",
    "        q_norm = self._normalize_label(name)\n",
    "    \n",
    "        # DB-side normalizer: lower \u2192 NBSP->space \u2192 &->and \u2192 collapse spaces \u2192 trim\n",
    "        norm_sql = (\n",
    "            \"btrim(regexp_replace(replace(lower(translate(name, chr(160),' ')),'&','and'), '\\\\s+', ' ', 'g'))\"\n",
    "        )\n",
    "    \n",
    "        if parent_id is not None:\n",
    "            # exact match under the given parent\n",
    "            self.cursor.execute(\n",
    "                r\"\"\"\n",
    "                SELECT category_id\n",
    "                FROM product_categories\n",
    "                WHERE saas_edge_id = %s\n",
    "                  AND parent_id = %s\n",
    "                  AND \"\"\" + norm_sql + r\"\"\" = %s\n",
    "                LIMIT 1\n",
    "                \"\"\",\n",
    "                (self.saas_edge_id, parent_id, q_norm),\n",
    "            )\n",
    "            row = self.cursor.fetchone()\n",
    "            if row:\n",
    "                return row[0]\n",
    "    \n",
    "        # \ud83d\udd11 single-part path (no explicit parent): search everywhere for a unique name\n",
    "        self.cursor.execute(\n",
    "            r\"\"\"\n",
    "            SELECT category_id\n",
    "            FROM product_categories\n",
    "            WHERE saas_edge_id = %s\n",
    "              AND \"\"\" + norm_sql + r\"\"\" = %s\n",
    "            \"\"\",\n",
    "            (self.saas_edge_id, q_norm),\n",
    "        )\n",
    "        rows = self.cursor.fetchall()\n",
    "        if not rows:\n",
    "            return None\n",
    "        if len(rows) == 1:\n",
    "            return rows[0][0]\n",
    "    \n",
    "        # If ambiguous (same name under multiple parents), only then fall back to root\n",
    "        self.cursor.execute(\n",
    "            r\"\"\"\n",
    "            SELECT category_id\n",
    "            FROM product_categories\n",
    "            WHERE saas_edge_id = %s\n",
    "              AND parent_id IS NULL\n",
    "              AND \"\"\" + norm_sql + r\"\"\" = %s\n",
    "            LIMIT 1\n",
    "            \"\"\",\n",
    "            (self.saas_edge_id, q_norm),\n",
    "        )\n",
    "        row = self.cursor.fetchone()\n",
    "        return row[0] if row else None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _next_category_id(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the next category_id:\n",
    "          1) take MAX(category_id) for this saas_edge_id, +1\n",
    "          2) if none exist for this tenant, fall back to global MAX(category_id) +1\n",
    "        Uses a light table lock to avoid race conditions inside the current txn.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # prevents two concurrent inserts from picking the same MAX\n",
    "            self.cursor.execute(\"LOCK TABLE product_categories IN SHARE ROW EXCLUSIVE MODE\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "        # tenant-scoped max first\n",
    "        self.cursor.execute(\"\"\"\n",
    "            SELECT MAX(category_id) \n",
    "            FROM product_categories \n",
    "            WHERE saas_edge_id = %s\n",
    "        \"\"\", (self.saas_edge_id,))\n",
    "        row = self.cursor.fetchone()\n",
    "        if row and row[0] is not None:\n",
    "            return int(row[0]) + 1\n",
    "    \n",
    "        # fallback: global max\n",
    "        self.cursor.execute(\"SELECT MAX(category_id) FROM product_categories\")\n",
    "        row = self.cursor.fetchone()\n",
    "        return (int(row[0]) if row and row[0] is not None else 0) + 1\n",
    "\n",
    "\n",
    "    # def _insert_category_under(self, name, parent_id):\n",
    "    #     \"\"\"Insert (name, parent_id, saas_edge_id). Auto-fill created_at/updated_at if columns exist.\"\"\"\n",
    "    #     cols = self._get_table_columns(\"product_categories\")\n",
    "    #     column_names = [\"name\", \"parent_id\", \"saas_edge_id\"]\n",
    "    #     placeholders = [\"%s\", \"%s\", \"%s\"]\n",
    "    #     values = [name, parent_id, self.saas_edge_id]\n",
    "    \n",
    "    #     if \"created_at\" in cols:\n",
    "    #         column_names.append(\"created_at\")\n",
    "    #         placeholders.append(\"CURRENT_TIMESTAMP\")\n",
    "    #     if \"updated_at\" in cols:\n",
    "    #         column_names.append(\"updated_at\")\n",
    "    #         placeholders.append(\"CURRENT_TIMESTAMP\")\n",
    "    \n",
    "    #     sql = f\"\"\"\n",
    "    #         INSERT INTO product_categories ({\", \".join(column_names)})\n",
    "    #         VALUES ({\", \".join(placeholders)})\n",
    "    #         RETURNING category_id\n",
    "    #     \"\"\"\n",
    "    #     self.cursor.execute(sql, values)\n",
    "    #     return self.cursor.fetchone()[0]\n",
    "\n",
    "    def _insert_category_under(self, name, parent_id):\n",
    "        cols = self._get_table_columns(\"product_categories\")\n",
    "    \n",
    "        # advisory lock on (tenant,parent_id,normalized_name) for this txn\n",
    "        key = f\"{self.saas_edge_id}|{parent_id}|{self._normalize_label(name)}\"\n",
    "        self.cursor.execute(\"SELECT pg_advisory_xact_lock(hashtext(%s))\", (key,))\n",
    "    \n",
    "        # re-check after taking the lock (another worker may have inserted it)\n",
    "        existing = self._find_category_id_ci(name, parent_id)\n",
    "        if existing:\n",
    "            return existing\n",
    "    \n",
    "        new_id = self._next_category_id()\n",
    "        column_names = [\"category_id\", \"name\", \"parent_id\", \"saas_edge_id\"]\n",
    "        placeholders = [\"%s\", \"%s\", \"%s\", \"%s\"]\n",
    "        values = [new_id, self._normalize_label(name).title(), parent_id, self.saas_edge_id]\n",
    "    \n",
    "        if \"created_at\" in cols:\n",
    "            column_names.append(\"created_at\"); placeholders.append(\"CURRENT_TIMESTAMP\")\n",
    "        if \"updated_at\" in cols:\n",
    "            column_names.append(\"updated_at\"); placeholders.append(\"CURRENT_TIMESTAMP\")\n",
    "    \n",
    "        sql = f\"\"\"\n",
    "            INSERT INTO product_categories ({\", \".join(column_names)})\n",
    "            VALUES ({\", \".join(placeholders)})\n",
    "            RETURNING category_id\n",
    "        \"\"\"\n",
    "        self.cursor.execute(sql, values)\n",
    "        return self.cursor.fetchone()[0]\n",
    "\n",
    "\n",
    "    \n",
    "    def _get_or_create_category_under(self, raw_name, parent_id):\n",
    "        \"\"\"Return (category_id, created_flag). Creates a node if it doesn't exist.\"\"\"\n",
    "        name = self._normalize_label(raw_name)\n",
    "        if not name:\n",
    "            raise ValueError(\"Empty category token in path\")\n",
    "    \n",
    "        cat_id = self._find_category_id_ci(name, parent_id)\n",
    "        if cat_id:\n",
    "            return cat_id, False\n",
    "    \n",
    "        cat_id = self._insert_category_under(name, parent_id)\n",
    "        return cat_id, True\n",
    "    \n",
    "    def _ensure_category_path(self, raw_path):\n",
    "        \"\"\"\n",
    "        Ensure a path like 'A > B > C' exists; create any missing nodes.\n",
    "        Returns the leaf category_id (C).\n",
    "        If only a single token is provided, it becomes a top-level category when missing.\n",
    "        \"\"\"\n",
    "        if not raw_path:\n",
    "            return None\n",
    "    \n",
    "        parts = [self._normalize_label(p) for p in str(raw_path).split(\">\")]\n",
    "        parts = [p for p in parts if p]\n",
    "        if not parts:\n",
    "            return None\n",
    "    \n",
    "        parent_id = None\n",
    "        created_any = False\n",
    "        for token in parts:\n",
    "            parent_id, created = self._get_or_create_category_under(token, parent_id)\n",
    "            created_any = created_any or created\n",
    "    \n",
    "        if created_any:\n",
    "            # refresh caches so _find_category_by_label/_preload sees new nodes\n",
    "            try:\n",
    "                self._preload_category_tree()\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "        return parent_id \n",
    "\n",
    "    def _apply_category_create_and_map(self, product_id: int, payload: dict) -> None:\n",
    "        \"\"\"\n",
    "        Map a product to one or more categories.\n",
    "    \n",
    "        Accepts any of these (as a single value, comma-separated string, or list):\n",
    "          \u2022 category_id, category_ids           \u2192 numeric IDs\n",
    "          \u2022 category, categories, category_path \u2192 labels or 'A > B > C' paths\n",
    "    \n",
    "        For names/paths, creates missing nodes and maps to the leaf.\n",
    "        Never raises.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not product_id or not isinstance(payload, dict):\n",
    "                return\n",
    "    \n",
    "            def _iter_items(val):\n",
    "                if val in (None, \"\", \"null\", \"NULL\"):\n",
    "                    return []\n",
    "                if isinstance(val, list):\n",
    "                    return [str(x).strip() for x in val if str(x).strip()]\n",
    "                return [s.strip() for s in str(val).split(\",\") if s.strip()]\n",
    "    \n",
    "            mapped_ids = set()\n",
    "    \n",
    "            # 1) Exact by ID(s)\n",
    "            for key in (\"category_ids\", \"category_id\"):\n",
    "                for item in _iter_items(payload.get(key)):\n",
    "                    try:\n",
    "                        cid = int(item)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    if cid not in mapped_ids and self._map_product_to_category_id(product_id, cid):\n",
    "                        mapped_ids.add(cid)\n",
    "    \n",
    "            # 2) Names/paths (can be multiple)\n",
    "            for key in (\"categories\", \"category\", \"category_path\"):\n",
    "                for token in _iter_items(payload.get(key)):\n",
    "                    # full path like \"A > B > C\"\n",
    "                    if \">\" in token:\n",
    "                        leaf_id = self._ensure_category_path(token)\n",
    "                        if leaf_id and leaf_id not in mapped_ids and self._map_product_to_category_id(product_id, leaf_id):\n",
    "                            mapped_ids.add(leaf_id)\n",
    "                        continue\n",
    "    \n",
    "                    # single label \u2192 ensure as top-level if missing\n",
    "                    leaf_id = self._ensure_category_path(token)\n",
    "                    if leaf_id and leaf_id not in mapped_ids and self._map_product_to_category_id(product_id, leaf_id):\n",
    "                        mapped_ids.add(leaf_id)\n",
    "                        continue\n",
    "    \n",
    "                    # fuzzy fallback if ensure failed for some reason\n",
    "                    match = self._find_category_by_label(token)\n",
    "                    if match:\n",
    "                        cid = match[\"category_id\"]\n",
    "                        if cid not in mapped_ids and self._map_product_to_category_id(product_id, cid):\n",
    "                            mapped_ids.add(cid)\n",
    "        except Exception as e:\n",
    "            print(f\"WARN: category mapping skipped for product {product_id}: {e}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def _to_int(self, v, default=None):\n",
    "        \"\"\"Coerce numbers like '50', '50.0', 50.0 \u2192 50 (int).\"\"\"\n",
    "        if v in (None, \"\", \"null\", \"NULL\"):\n",
    "            return default\n",
    "        try:\n",
    "            if isinstance(v, int):\n",
    "                return v\n",
    "            if isinstance(v, float):\n",
    "                if v != v:   # NaN\n",
    "                    return default\n",
    "                return int(v)\n",
    "            # strings, decimals, etc.\n",
    "            return int(float(str(v).strip()))\n",
    "        except Exception:\n",
    "            return default\n",
    "\n",
    "    def _to_decimal(self, v):\n",
    "        \"\"\"Coerce to Decimal or None (safe for numeric/price columns).\"\"\"\n",
    "        if v in (None, \"\", \"null\", \"NULL\"):\n",
    "            return None\n",
    "        try:\n",
    "            return Decimal(str(v).strip())\n",
    "        except (InvalidOperation, ValueError):\n",
    "            return None\n",
    "\n",
    "    # ========= DB meta =========\n",
    "    def _get_table_columns(self, table_name: str) -> set[str]:\n",
    "        self.cursor.execute(\"\"\"\n",
    "            SELECT column_name\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_schema = %s AND table_name = %s\n",
    "        \"\"\", (self.table_schema, table_name))\n",
    "        return {r[0] for r in self.cursor.fetchall()}\n",
    "\n",
    "    def _get_json_columns(self, table_name: str) -> dict:\n",
    "        self.cursor.execute(\"\"\"\n",
    "            SELECT column_name, data_type\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_schema = %s AND table_name = %s\n",
    "              AND data_type IN ('json','jsonb')\n",
    "        \"\"\", (self.table_schema, table_name))\n",
    "        return {r[0]: r[1] for r in self.cursor.fetchall()}\n",
    "\n",
    "\n",
    "    def _get_column_types(self, table_name: str):\n",
    "        self.cursor.execute(\"\"\"\n",
    "            SELECT column_name, data_type, udt_name\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_schema = %s AND table_name = %s\n",
    "        \"\"\", (self.table_schema, table_name))\n",
    "        out = {}\n",
    "        for col, dt, udt in self.cursor.fetchall():\n",
    "            out[col] = {\"data_type\": dt, \"udt_name\": udt}\n",
    "        json_cols = self._get_json_columns(table_name)\n",
    "        for col in out:\n",
    "            out[col][\"is_json\"] = col in json_cols\n",
    "            out[col][\"json_cast\"] = \"jsonb\" if json_cols.get(col) == \"jsonb\" else (\"json\" if json_cols.get(col) else None)\n",
    "        return out\n",
    "\n",
    "    def _to_list_normalized(self, v):\n",
    "        if v is None: return []\n",
    "        if isinstance(v, list): items = v\n",
    "        elif isinstance(v, (bytes, bytearray)):\n",
    "            try: items = json.loads(v.decode(\"utf-8\"))\n",
    "            except Exception: items = [v.decode(\"utf-8\",\"ignore\")]\n",
    "        elif isinstance(v, str):\n",
    "            s = v.strip()\n",
    "            if not s: items = []\n",
    "            else:\n",
    "                try:\n",
    "                    loaded = json.loads(s)\n",
    "                    items = loaded if isinstance(loaded, list) else [s]\n",
    "                except Exception:\n",
    "                    items = [p.strip() for p in s.split(\",\") if p.strip()]\n",
    "        else:\n",
    "            items = [str(v)]\n",
    "        seen, out = set(), []\n",
    "        for x in items:\n",
    "            sx = str(x).strip()\n",
    "            if sx and sx.lower() not in seen:\n",
    "                seen.add(sx.lower()); out.append(sx)\n",
    "        return out\n",
    "\n",
    "    def _merge_tags(self, existing_value, incoming_value):\n",
    "        base = self._to_list_normalized(existing_value)\n",
    "        inc  = self._to_list_normalized(incoming_value)\n",
    "        seen = {x.lower() for x in base}\n",
    "        for t in inc:\n",
    "            if t.lower() not in seen:\n",
    "                base.append(t); seen.add(t.lower())\n",
    "        return base\n",
    "\n",
    "    def _coerce_for_column(self, col_name: str, value, col_types: dict):\n",
    "        meta = col_types.get(col_name, {\"data_type\": None, \"is_json\": False, \"json_cast\": None})\n",
    "        if meta.get(\"is_json\"):\n",
    "            return json.dumps(value), f\"::{meta.get('json_cast')}\"\n",
    "        if meta.get(\"data_type\") == \"ARRAY\":\n",
    "            return (value if isinstance(value, list) else self._to_list_normalized(value)), None\n",
    "        if isinstance(value, list):  return \",\".join(str(x) for x in value), None\n",
    "        if isinstance(value, dict):  return json.dumps(value), None\n",
    "        return value, None\n",
    "\n",
    "\n",
    "    def _get_next_bigint_pk(self, table: str, pk_column: str) -> int:\n",
    "        \"\"\"\n",
    "        Allocate a bigint PK safely:\n",
    "          1) try the sequence (pg_get_serial_sequence), else\n",
    "          2) fallback to MAX(pk)+1\n",
    "        \"\"\"\n",
    "        seq_name = None\n",
    "        try:\n",
    "            self.cursor.execute(\n",
    "                \"SELECT pg_get_serial_sequence(%s, %s)\",\n",
    "                (f\"{self.table_schema}.{table}\", pk_column)\n",
    "            )\n",
    "            row = self.cursor.fetchone()\n",
    "            seq_name = row[0] if row else None\n",
    "        except Exception:\n",
    "            seq_name = None\n",
    "\n",
    "        if seq_name:\n",
    "            try:\n",
    "                # can't param identifiers; ensure simple safe identifier\n",
    "                if not re.fullmatch(r\"[A-Za-z0-9_.]+\", seq_name):\n",
    "                    raise ValueError(\"Unexpected sequence name\")\n",
    "                self.cursor.execute(f\"SELECT nextval('{seq_name}')\")\n",
    "                row = self.cursor.fetchone()\n",
    "                if row and row[0]:\n",
    "                    return int(row[0])\n",
    "            except Exception as e:\n",
    "                print(f\"WARN: nextval failed for sequence {seq_name}: {e}\")\n",
    "\n",
    "        # fallback: MAX+1\n",
    "        q = psql.SQL(\"SELECT COALESCE(MAX({pk}),0)+1 FROM {tbl}\").format(\n",
    "            pk=psql.Identifier(pk_column),\n",
    "            tbl=psql.Identifier(table)\n",
    "        )\n",
    "        self.cursor.execute(q)\n",
    "        row = self.cursor.fetchone()\n",
    "        return int((row and row[0]) or 1)\n",
    "\n",
    "    def _download_image_with_meta(self, src: str):\n",
    "        \"\"\"\n",
    "        Download image and return (file_data, meta_dict).\n",
    "        meta_dict keys: filename_original, extension, mime_type, filesize_bytes,\n",
    "                        checksum_sha256, width, height\n",
    "        \"\"\"\n",
    "        r = requests.get(src, stream=True, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        content = r.content\n",
    "        file_data = BytesIO(content)\n",
    "    \n",
    "        # filename / ext from URL (fallbacks)\n",
    "        url_path = urllib.parse.urlparse(src).path\n",
    "        filename_original = os.path.basename(url_path) or \"image\"\n",
    "        guessed_mime, _ = mimetypes.guess_type(filename_original)\n",
    "        mime_type = guessed_mime or \"image/jpeg\"\n",
    "        extension = (os.path.splitext(filename_original)[1] or \".jpg\").lstrip(\".\").lower()\n",
    "    \n",
    "        # width/height\n",
    "        width = height = None\n",
    "        try:\n",
    "            im = Image.open(BytesIO(content))\n",
    "            width, height = im.size\n",
    "            if not extension or extension == \".\":\n",
    "                extension = (im.format or \"JPEG\").lower()\n",
    "            # normalize to jpg/png/etc \u2014 keep whatever came from path when present\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "        meta = {\n",
    "            \"filename_original\": filename_original,\n",
    "            \"extension\": extension,\n",
    "            \"mime_type\": mime_type,\n",
    "            \"filesize_bytes\": len(content),\n",
    "            \"checksum_sha256\": hashlib.sha256(content).hexdigest(),\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "        }\n",
    "        return file_data, meta\n",
    "\n",
    "\n",
    "    def _upsert_asset_and_get_id(self, gcp_url: str, source_url: str, meta: dict) -> int:\n",
    "        \"\"\"\n",
    "        Ensure a single row in `assets` per (saas_edge_id, checksum_sha256).\n",
    "        Returns assets.asset_id (to be stored as saas_asset_id in the other tables).\n",
    "        \"\"\"\n",
    "        # 1) try checksum\n",
    "        self.cursor.execute(\"\"\"\n",
    "            SELECT asset_id\n",
    "            FROM assets\n",
    "            WHERE saas_edge_id=%s AND checksum_sha256=%s\n",
    "            LIMIT 1\n",
    "        \"\"\", (self.saas_edge_id, meta[\"checksum_sha256\"]))\n",
    "        row = self.cursor.fetchone()\n",
    "    \n",
    "        if row:\n",
    "            asset_id = row[0]\n",
    "            # keep asset_url fresh; fill in missing meta\n",
    "            self.cursor.execute(\"\"\"\n",
    "                UPDATE assets\n",
    "                   SET asset_url=%s,\n",
    "                       source_url = COALESCE(source_url, %s),\n",
    "                       mime_type  = COALESCE(mime_type, %s),\n",
    "                       extension  = COALESCE(extension, %s),\n",
    "                       filename_original = COALESCE(filename_original, %s),\n",
    "                       filesize_bytes = COALESCE(filesize_bytes, %s),\n",
    "                       width = COALESCE(width, %s),\n",
    "                       height = COALESCE(height, %s),\n",
    "                       asset_type='image',\n",
    "                       updated_at=CURRENT_TIMESTAMP\n",
    "                 WHERE asset_id=%s\n",
    "            \"\"\", (\n",
    "                gcp_url, source_url,\n",
    "                meta[\"mime_type\"], meta[\"extension\"], meta[\"filename_original\"],\n",
    "                meta[\"filesize_bytes\"], meta[\"width\"], meta[\"height\"],\n",
    "                asset_id\n",
    "            ))\n",
    "            return asset_id\n",
    "    \n",
    "        # 2) optional: try exact url match\n",
    "        self.cursor.execute(\"\"\"\n",
    "            SELECT asset_id\n",
    "            FROM assets\n",
    "            WHERE saas_edge_id=%s AND (asset_url=%s OR source_url=%s)\n",
    "            LIMIT 1\n",
    "        \"\"\", (self.saas_edge_id, gcp_url, source_url))\n",
    "        row = self.cursor.fetchone()\n",
    "        if row:\n",
    "            asset_id = row[0]\n",
    "            self.cursor.execute(\"\"\"\n",
    "                UPDATE assets\n",
    "                   SET checksum_sha256=%s,\n",
    "                       mime_type=%s,\n",
    "                       extension=%s,\n",
    "                       filename_original=%s,\n",
    "                       filesize_bytes=%s,\n",
    "                       width=%s,\n",
    "                       height=%s,\n",
    "                       asset_type='image',\n",
    "                       updated_at=CURRENT_TIMESTAMP\n",
    "                 WHERE asset_id=%s\n",
    "            \"\"\", (\n",
    "                meta[\"checksum_sha256\"], meta[\"mime_type\"], meta[\"extension\"],\n",
    "                meta[\"filename_original\"], meta[\"filesize_bytes\"],\n",
    "                meta[\"width\"], meta[\"height\"], asset_id\n",
    "            ))\n",
    "            return asset_id\n",
    "    \n",
    "        # 3) insert new\n",
    "        self.cursor.execute(\"\"\"\n",
    "            INSERT INTO assets\n",
    "            (saas_edge_id, asset_url, alt_text, created_at, updated_at, asset_type,\n",
    "             is_imagine_with_ai, meta, filename_original, extension, mime_type,\n",
    "             filesize_bytes, checksum_sha256, width, height, usage_rights, source,\n",
    "             owner, archived, tags, categories, source_url)\n",
    "            VALUES\n",
    "            (%s, %s, '', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP, 'image',\n",
    "             false, NULL, %s, %s, %s,\n",
    "             %s, %s, %s, %s, NULL, NULL,\n",
    "             NULL, false, NULL, NULL, %s)\n",
    "            RETURNING asset_id\n",
    "        \"\"\", (\n",
    "            self.saas_edge_id, gcp_url,\n",
    "            meta[\"filename_original\"], meta[\"extension\"], meta[\"mime_type\"],\n",
    "            meta[\"filesize_bytes\"], meta[\"checksum_sha256\"], meta[\"width\"], meta[\"height\"],\n",
    "            source_url\n",
    "        ))\n",
    "        return self.cursor.fetchone()[0]\n",
    "\n",
    "\n",
    "    def _append_unique_variant_assets_to_parent(self, parent_product_id: int, new_assets: list[dict]) -> None:\n",
    "        \"\"\"\n",
    "        Append variant assets to parent slots (1..10) only if unique.\n",
    "        Dedupe by saas_asset_id when available, else by asset_url.\n",
    "        After changes, mirror back to products.image_url*.\n",
    "        new_assets: [{gcp_url, saas_asset_id, src, pos}, ...] in desired order.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            pa_cols = self._get_table_columns(\"productassets\")\n",
    "    \n",
    "            # Current parent slots 1..10\n",
    "            self.cursor.execute(\"\"\"\n",
    "                SELECT asset_id, asset_url, saas_asset_id, channel_asset_pos\n",
    "                FROM productassets\n",
    "                WHERE product_id=%s AND saas_edge_id=%s\n",
    "                  AND channel_asset_pos BETWEEN 1 AND 10\n",
    "                ORDER BY channel_asset_pos\n",
    "            \"\"\", (parent_product_id, self.saas_edge_id))\n",
    "            rows = self.cursor.fetchall()\n",
    "    \n",
    "            occupied_slots = {pos for (_aid, _url, _sid, pos) in rows}\n",
    "            free_slots     = [i for i in range(1, 11) if i not in occupied_slots]\n",
    "            seen_ids       = {sid for (_aid, _url, sid, _pos) in rows if sid is not None}\n",
    "            seen_urls      = {url for (_aid, url, _sid, _pos) in rows}\n",
    "            had_any_before = bool(rows)\n",
    "    \n",
    "            for asset in new_assets:\n",
    "                if not free_slots:\n",
    "                    break\n",
    "                url  = asset[\"gcp_url\"]\n",
    "                sid  = asset.get(\"saas_asset_id\")\n",
    "                src  = asset.get(\"src\")\n",
    "                role = \"linked\"\n",
    "    \n",
    "                # de-dupe\n",
    "                if \"saas_asset_id\" in pa_cols and sid:\n",
    "                    if sid in seen_ids:\n",
    "                        continue\n",
    "                else:\n",
    "                    if url in seen_urls:\n",
    "                        continue\n",
    "    \n",
    "                slot = free_slots.pop(0)\n",
    "                is_default = (slot == 1 and not had_any_before)\n",
    "                if slot == 1 and not had_any_before:\n",
    "                    role = \"main\"\n",
    "    \n",
    "                # upsert-at-slot (slot was free \u2192 insert)\n",
    "                cols = [\"product_id\",\"saas_edge_id\",\"asset_url\",\"channel_asset_pos\",\"source_url\",\"asset_type\",\"is_default\",\"created_at\"]\n",
    "                ph   =  [\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"CURRENT_TIMESTAMP\"]\n",
    "                vals = [parent_product_id, self.saas_edge_id, url, slot, src, \"image\", is_default]\n",
    "    \n",
    "                if \"saas_asset_id\" in pa_cols:\n",
    "                    cols.append(\"saas_asset_id\"); ph.insert(6, \"%s\"); vals.insert(6, sid)\n",
    "                if \"role\" in pa_cols:\n",
    "                    cols.append(\"role\"); ph.append(\"%s\"); vals.append(role)\n",
    "                if \"position\" in pa_cols:\n",
    "                    cols.append(\"position\"); ph.append(\"%s\"); vals.append(slot)\n",
    "    \n",
    "                self.cursor.execute(f\"\"\"\n",
    "                    INSERT INTO productassets ({\", \".join(cols)})\n",
    "                    VALUES ({\", \".join(ph)})\n",
    "                \"\"\", tuple(vals))\n",
    "    \n",
    "                # mark as seen so later variant assets don't re-add\n",
    "                if sid:\n",
    "                    seen_ids.add(sid)\n",
    "                seen_urls.add(url)\n",
    "    \n",
    "            # mirror parent 1..10 back to products.image_url*\n",
    "            self.cursor.execute(\"\"\"\n",
    "                SELECT asset_url, channel_asset_pos\n",
    "                FROM productassets\n",
    "                WHERE product_id=%s AND saas_edge_id=%s\n",
    "                  AND channel_asset_pos BETWEEN 1 AND 10\n",
    "                ORDER BY channel_asset_pos\n",
    "            \"\"\", (parent_product_id, self.saas_edge_id))\n",
    "            rows = self.cursor.fetchall()\n",
    "            col_names   = [\"image_url\"] + [f\"image_url_{i}\" for i in range(1, 10)]\n",
    "            col_updates = { col_names[pos-1]: url for (url, pos) in rows if 1 <= pos <= 10 }\n",
    "            if col_updates:\n",
    "                set_clause = \", \".join(f\"{col}=%s\" for col in col_updates)\n",
    "                self.cursor.execute(\n",
    "                    f\"UPDATE products SET {set_clause} WHERE product_id=%s AND saas_edge_id=%s\",\n",
    "                    (*col_updates.values(), parent_product_id, self.saas_edge_id)\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"WARN: failed to append variant images to parent: {e}\")\n",
    "\n",
    "\n",
    "    def _process_parent_images_from_updates(self, parent_id: int, parent_sku: str, updates: dict):\n",
    "        IMAGE_COLUMNS = [\"image_url\"] + [f\"image_url_{i}\" for i in range(1, 10)]\n",
    "        pa_cols = self._get_table_columns(\"productassets\")\n",
    "    \n",
    "        for pos_idx, col in enumerate(IMAGE_COLUMNS, start=1):\n",
    "            src = updates.get(col)\n",
    "            if not src:\n",
    "                continue\n",
    "            try:\n",
    "                file_data, meta = self._download_image_with_meta(src)\n",
    "                file_name = f\"{parent_sku}.jpg\" if pos_idx == 1 else f\"{parent_sku}_{pos_idx-1}.jpg\"\n",
    "                subdir = f\"{self.saas_edge_id}/catalog_edge/dam/public/imported\"\n",
    "                gcp_url = self.upload_asset_to_gcp_bucket(file_data, file_name, GCP_BUCKET_NAME, credentials, subdir) or src\n",
    "                saas_asset_id = self._upsert_asset_and_get_id(gcp_url, src, meta)\n",
    "    \n",
    "                # de-dupe within 1..10 by url OR saas_asset_id\n",
    "                if \"saas_asset_id\" in pa_cols:\n",
    "                    self.cursor.execute(\"\"\"\n",
    "                        DELETE FROM productassets\n",
    "                        WHERE product_id=%s AND saas_edge_id=%s\n",
    "                          AND (asset_url=%s OR saas_asset_id=%s)\n",
    "                          AND channel_asset_pos BETWEEN 1 AND 10\n",
    "                          AND channel_asset_pos <> %s\n",
    "                    \"\"\", (parent_id, self.saas_edge_id, gcp_url, saas_asset_id, pos_idx))\n",
    "                else:\n",
    "                    self.cursor.execute(\"\"\"\n",
    "                        DELETE FROM productassets\n",
    "                        WHERE product_id=%s AND saas_edge_id=%s\n",
    "                          AND asset_url=%s\n",
    "                          AND channel_asset_pos BETWEEN 1 AND 10\n",
    "                          AND channel_asset_pos <> %s\n",
    "                    \"\"\", (parent_id, self.saas_edge_id, gcp_url, pos_idx))\n",
    "    \n",
    "                # upsert slot\n",
    "                self.cursor.execute(\"\"\"\n",
    "                    SELECT asset_id\n",
    "                    FROM productassets\n",
    "                    WHERE product_id=%s AND saas_edge_id=%s AND channel_asset_pos=%s\n",
    "                    LIMIT 1\n",
    "                \"\"\", (parent_id, self.saas_edge_id, pos_idx))\n",
    "                row = self.cursor.fetchone()\n",
    "    \n",
    "                role = \"main\" if pos_idx == 1 else \"linked\"\n",
    "                if row:\n",
    "                    set_bits = [\"asset_url=%s\",\"source_url=%s\",\"asset_type='image'\",\"is_default=%s\"]\n",
    "                    params   = [gcp_url, src, pos_idx == 1]\n",
    "                    if \"saas_asset_id\" in pa_cols:\n",
    "                        set_bits.append(\"saas_asset_id=%s\"); params.append(saas_asset_id)\n",
    "                    if \"role\" in pa_cols:\n",
    "                        set_bits.append(\"role=%s\"); params.append(role)\n",
    "                    if \"position\" in pa_cols:\n",
    "                        set_bits.append(\"position=%s\"); params.append(pos_idx)\n",
    "    \n",
    "                    self.cursor.execute(f\"\"\"\n",
    "                        UPDATE productassets SET {\", \".join(set_bits)}\n",
    "                        WHERE asset_id=%s\n",
    "                    \"\"\", (*params, row[0]))\n",
    "                else:\n",
    "                    cols = [\"product_id\",\"saas_edge_id\",\"asset_url\",\"channel_asset_pos\",\"source_url\",\"asset_type\",\"is_default\",\"created_at\"]\n",
    "                    ph   =  [\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"CURRENT_TIMESTAMP\"]\n",
    "                    vals = [parent_id, self.saas_edge_id, gcp_url, pos_idx, src, \"image\", pos_idx == 1]\n",
    "                    if \"saas_asset_id\" in pa_cols:\n",
    "                        cols.append(\"saas_asset_id\"); ph.insert(6,\"%s\"); vals.insert(6, saas_asset_id)\n",
    "                    if \"role\" in pa_cols:\n",
    "                        cols.append(\"role\"); ph.append(\"%s\"); vals.append(role)\n",
    "                    if \"position\" in pa_cols:\n",
    "                        cols.append(\"position\"); ph.append(\"%s\"); vals.append(pos_idx)\n",
    "    \n",
    "                    self.cursor.execute(f\"\"\"\n",
    "                        INSERT INTO productassets ({\", \".join(cols)})\n",
    "                        VALUES ({\", \".join(ph)})\n",
    "                    \"\"\", tuple(vals))\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\"WARN: parent image upload failed for {parent_sku} slot {pos_idx}: {e}\")\n",
    "    \n",
    "        # mirror back to products.image_url*\n",
    "        self.cursor.execute(\"\"\"\n",
    "            SELECT asset_url, channel_asset_pos\n",
    "            FROM productassets\n",
    "            WHERE product_id=%s AND saas_edge_id=%s\n",
    "              AND channel_asset_pos BETWEEN 1 AND 10\n",
    "            ORDER BY channel_asset_pos\n",
    "        \"\"\", (parent_id, self.saas_edge_id))\n",
    "        rows = self.cursor.fetchall()\n",
    "        col_names = [\"image_url\"] + [f\"image_url_{i}\" for i in range(1, 10)]\n",
    "        col_updates = { col_names[pos-1]: url for (url, pos) in rows if 1 <= pos <= 10 }\n",
    "        if col_updates:\n",
    "            set_clause = \", \".join(f\"{col}=%s\" for col in col_updates)\n",
    "            self.cursor.execute(\n",
    "                f\"UPDATE products SET {set_clause} WHERE product_id=%s AND saas_edge_id=%s\",\n",
    "                (*col_updates.values(), parent_id, self.saas_edge_id)\n",
    "            )\n",
    "\n",
    "    def _upsert_parent(self, parent_sku: str, updates: Optional[dict]) -> int:\n",
    "        product_cols = self._get_table_columns(\"products\")\n",
    "        col_types    = self._get_column_types(\"products\")\n",
    "        immutable    = {\"product_id\",\"sku\",\"saas_edge_id\",\"created_at\",\"updated_at\",\"last_update_ts\"}\n",
    "    \n",
    "        # fetch existing\n",
    "        self.cursor.execute(\"\"\"\n",
    "            SELECT product_id, raw_product_data, tags\n",
    "            FROM products\n",
    "            WHERE sku=%s AND saas_edge_id=%s\n",
    "            LIMIT 1\n",
    "        \"\"\", (parent_sku, self.saas_edge_id))\n",
    "        row = self.cursor.fetchone()\n",
    "        existing_id   = row[0] if row else None\n",
    "        existing_raw  = {}\n",
    "        existing_tags = None\n",
    "        if row:\n",
    "            if row[1]:\n",
    "                try:\n",
    "                    existing_raw = row[1] if isinstance(row[1], dict) else json.loads(row[1])\n",
    "                except Exception:\n",
    "                    existing_raw = {}\n",
    "            existing_tags = row[2]\n",
    "    \n",
    "        set_parts, params = [], []\n",
    "    \n",
    "        def add_set(col, val):\n",
    "            if col not in product_cols or col in immutable:\n",
    "                return\n",
    "            coerced, cast = self._coerce_for_column(col, val, col_types)\n",
    "            set_parts.append(f\"\\\"{col}\\\"=%s{cast or ''}\")\n",
    "            params.append(coerced)\n",
    "    \n",
    "        # collect fields (skip empties)\n",
    "        for k, v in (updates or {}).items():\n",
    "            if k in immutable or k in (\"sku\",\"saas_edge_id\"):\n",
    "                continue\n",
    "            if v in (None, \"\", \"null\", \"NULL\"):\n",
    "                continue\n",
    "    \n",
    "            if k == \"raw_product_data\" and isinstance(v, dict):\n",
    "                merged = dict(existing_raw or {})\n",
    "                merged.update(v)\n",
    "                add_set(\"raw_product_data\", merged)\n",
    "                continue\n",
    "    \n",
    "            if k == \"tags\":\n",
    "                merged_tags = self._merge_tags(existing_tags, v)\n",
    "                add_set(\"tags\", merged_tags)\n",
    "                continue\n",
    "    \n",
    "            if k in product_cols:\n",
    "                add_set(k, v)\n",
    "    \n",
    "        if existing_id:\n",
    "            if set_parts:\n",
    "                set_parts.append(\"last_update_ts=CURRENT_TIMESTAMP\")\n",
    "                self.cursor.execute(f\"\"\"\n",
    "                    UPDATE products\n",
    "                    SET {\", \".join(set_parts)}\n",
    "                    WHERE product_id=%s AND saas_edge_id=%s\n",
    "                \"\"\", (*params, existing_id, self.saas_edge_id))\n",
    "            product_id = existing_id\n",
    "            # print(f\"INFO: Upsert parent (update): {parent_sku} -> product_id={product_id}\")\n",
    "        else:\n",
    "            cols, ph, vals = [\"sku\",\"saas_edge_id\"], [\"%s\",\"%s\"], [parent_sku, self.saas_edge_id]\n",
    "            for k, v in (updates or {}).items():\n",
    "                if k in immutable or k in (\"sku\",\"saas_edge_id\"):\n",
    "                    continue\n",
    "                if v in (None, \"\", \"null\", \"NULL\"):\n",
    "                    continue\n",
    "                if k not in product_cols:\n",
    "                    continue\n",
    "                coerced, cast = self._coerce_for_column(k, v, col_types)\n",
    "                cols.append(k)\n",
    "                ph.append(\"%s\" + (cast or \"\"))\n",
    "                vals.append(coerced)\n",
    "    \n",
    "            self.cursor.execute(f\"\"\"\n",
    "                INSERT INTO products ({\", \".join(f'\"{c}\"' for c in cols)}, last_update_ts)\n",
    "                VALUES ({\", \".join(ph)}, CURRENT_TIMESTAMP)\n",
    "                RETURNING product_id\n",
    "            \"\"\", vals)\n",
    "            product_id = self.cursor.fetchone()[0]\n",
    "            # print(f\"INFO: Upsert parent (create): {parent_sku} -> product_id={product_id}\")\n",
    "    \n",
    "        # ---------- PARENT IMAGES ONLY (slots 1..10) ----------\n",
    "        try:\n",
    "            IMAGE_COLUMNS = [\"image_url\"] + [f\"image_url_{i}\" for i in range(1, 10)]\n",
    "            has_parent_images = bool(updates) and any(updates.get(c) for c in IMAGE_COLUMNS)\n",
    "    \n",
    "            # guard: if payload sku != parent_sku we assume it's a variant payload \u2192 skip touching parent assets\n",
    "            payload_sku = (updates or {}).get(\"sku\", parent_sku)\n",
    "            is_parent_payload = str(payload_sku).strip() == str(parent_sku).strip()\n",
    "    \n",
    "            if has_parent_images and is_parent_payload:\n",
    "                self._process_parent_images_from_updates(product_id, parent_sku, updates)\n",
    "            elif has_parent_images and not is_parent_payload:\n",
    "                print(f\"INFO: Skipping parent image processing for {parent_sku} (payload sku '{payload_sku}' != parent).\")\n",
    "        except Exception as img_err:\n",
    "            print(f\"WARN: parent image processing skipped for {parent_sku}: {img_err}\")\n",
    "        #         for slot_idx in range(10):\n",
    "        #             col_name  = IMAGE_COLUMNS[slot_idx]\n",
    "        #             source_url = updates.get(col_name)\n",
    "        #             if not source_url:\n",
    "        #                 continue\n",
    "    \n",
    "        #             try:\n",
    "        #                 r = requests.get(source_url, stream=True, timeout=20)\n",
    "        #                 r.raise_for_status()\n",
    "        #                 file_data = BytesIO(r.content)\n",
    "    \n",
    "        #                 file_name = f\"{parent_sku}.jpg\" if slot_idx == 0 else f\"{parent_sku}_{slot_idx}.jpg\"\n",
    "        #                 subdir    = f\"{self.saas_edge_id}/catalog_edge/dam/public/imported\"\n",
    "        #                 gcp_url   = self.upload_asset_to_gcp_bucket(\n",
    "        #                     file_data, file_name, GCP_BUCKET_NAME, credentials, subdir\n",
    "        #                 ) or source_url\n",
    "    \n",
    "        #                 # de-dupe strictly within 1..10\n",
    "        #                 self.cursor.execute(\"\"\"\n",
    "        #                     DELETE FROM productassets\n",
    "        #                     WHERE product_id=%s AND saas_edge_id=%s\n",
    "        #                       AND asset_url=%s\n",
    "        #                       AND channel_asset_pos BETWEEN 1 AND 10\n",
    "        #                       AND channel_asset_pos <> %s\n",
    "        #                 \"\"\", (product_id, self.saas_edge_id, gcp_url, slot_idx + 1))\n",
    "    \n",
    "        #                 # upsert slot 1..10\n",
    "        #                 self.cursor.execute(\"\"\"\n",
    "        #                     SELECT asset_id\n",
    "        #                     FROM productassets\n",
    "        #                     WHERE product_id=%s AND saas_edge_id=%s AND channel_asset_pos=%s\n",
    "        #                     LIMIT 1\n",
    "        #                 \"\"\", (product_id, self.saas_edge_id, slot_idx + 1))\n",
    "        #                 existing = self.cursor.fetchone()\n",
    "    \n",
    "        #                 if existing:\n",
    "        #                     self.cursor.execute(\"\"\"\n",
    "        #                         UPDATE productassets\n",
    "        #                         SET asset_url=%s, source_url=%s\n",
    "        #                         WHERE asset_id=%s\n",
    "        #                     \"\"\", (gcp_url, source_url, existing[0]))\n",
    "        #                 else:\n",
    "        #                     self.cursor.execute(\"\"\"\n",
    "        #                         INSERT INTO productassets\n",
    "        #                         (product_id, saas_edge_id, asset_url, channel_asset_pos, source_url)\n",
    "        #                         VALUES (%s,%s,%s,%s,%s)\n",
    "        #                     \"\"\", (product_id, self.saas_edge_id, gcp_url, slot_idx + 1, source_url))\n",
    "    \n",
    "        #             except Exception as e_up:\n",
    "        #                 print(f\"Parent image upload failed for {parent_sku} slot {slot_idx+1}: {e_up}\")\n",
    "    \n",
    "        #         # mirror productassets 1..10 \u2192 products.image_url*\n",
    "        #         self.cursor.execute(\"\"\"\n",
    "        #             SELECT asset_url, channel_asset_pos\n",
    "        #             FROM productassets\n",
    "        #             WHERE product_id=%s AND saas_edge_id=%s\n",
    "        #               AND channel_asset_pos BETWEEN 1 AND 10\n",
    "        #             ORDER BY channel_asset_pos\n",
    "        #         \"\"\", (product_id, self.saas_edge_id))\n",
    "        #         rows = self.cursor.fetchall()\n",
    "    \n",
    "        #         # only update columns that have a value\n",
    "        #         col_updates = {\n",
    "        #             IMAGE_COLUMNS[pos-1]: url\n",
    "        #             for (url, pos) in rows\n",
    "        #             if 1 <= pos <= 10 and url\n",
    "        #         }\n",
    "        #         if col_updates:\n",
    "        #             set_clause = \", \".join(f\"{col}=%s\" for col in col_updates)\n",
    "        #             self.cursor.execute(\n",
    "        #                 f\"UPDATE products SET {set_clause} WHERE product_id=%s AND saas_edge_id=%s\",\n",
    "        #                 (*col_updates.values(), product_id, self.saas_edge_id)\n",
    "        #             )\n",
    "        #             print(f\"INFO: Synced parent image columns for SKU {parent_sku}\")\n",
    "        #     elif has_parent_images and not is_parent_payload:\n",
    "        #         print(f\"INFO: Skipping parent image processing for {parent_sku} (payload sku '{payload_sku}' != parent).\")\n",
    "        # except Exception as img_err:\n",
    "        #     print(f\"WARN: parent image processing skipped for {parent_sku}: {img_err}\")\n",
    "        # ---------- END PARENT IMAGES ONLY ----------\n",
    "    \n",
    "        # create/map category (+ mirror)\n",
    "        try:\n",
    "            self._apply_category_create_and_map(product_id, updates or {})\n",
    "        except Exception as e:\n",
    "            print(f\"WARN: category mapping (parent) skipped for {parent_sku}: {e}\")\n",
    "    \n",
    "        return product_id\n",
    "\n",
    "   \n",
    "\n",
    "    def _upsert_variant(self, parent_product_id: int, variant_obj: dict) -> int:\n",
    "        \"\"\"Upsert variant WITHOUT ON CONFLICT (works even if no unique indexes).\"\"\"\n",
    "        sku = variant_obj.get(\"sku\")\n",
    "        if not sku:\n",
    "            raise ValueError(\"variant sku is required\")\n",
    "    \n",
    "        # Coerce numbers to correct types\n",
    "        price_val = self._to_decimal(variant_obj.get(\"price\"))\n",
    "        inv_qty   = self._to_int(variant_obj.get(\"inventory_quantity\"), default=0)\n",
    "        height    = self._to_decimal(variant_obj.get(\"height\"))\n",
    "        width     = self._to_decimal(variant_obj.get(\"width\"))\n",
    "        length    = self._to_decimal(variant_obj.get(\"length\"))\n",
    "        weight    = self._to_decimal(variant_obj.get(\"weight\"))\n",
    "    \n",
    "        raw_json = json.dumps(variant_obj.get(\"raw_variant_data\", {}))\n",
    "    \n",
    "        # check existing\n",
    "        self.cursor.execute(\"\"\"\n",
    "            SELECT variant_id FROM productvariants\n",
    "            WHERE sku=%s AND saas_edge_id=%s\n",
    "            LIMIT 1\n",
    "        \"\"\", (sku, self.saas_edge_id))\n",
    "        row = self.cursor.fetchone()\n",
    "    \n",
    "        if row:\n",
    "            variant_id = row[0]\n",
    "            self.cursor.execute(\"\"\"\n",
    "                UPDATE productvariants\n",
    "                SET product_id=%s,\n",
    "                    variant_name=%s,\n",
    "                    price=%s,\n",
    "                    inventory_quantity=%s,\n",
    "                    barcode=%s,\n",
    "                    image_url=%s,\n",
    "                    image_url_1=%s,\n",
    "                    image_url_2=%s,\n",
    "                    image_url_3=%s,\n",
    "                    image_url_4=%s,\n",
    "                    upc=%s,\n",
    "                    ean=%s,\n",
    "                    mpn=%s,\n",
    "                    costprice=%s,\n",
    "                    retailprice=%s,\n",
    "                    height=%s,\n",
    "                    width=%s,\n",
    "                    length=%s,\n",
    "                    weight=%s,\n",
    "                    raw_variant_data=%s::jsonb,\n",
    "                    updated_at=CURRENT_TIMESTAMP\n",
    "                WHERE variant_id=%s AND saas_edge_id=%s\n",
    "            \"\"\", (\n",
    "                parent_product_id,\n",
    "                variant_obj.get(\"variant_name\") or sku,\n",
    "                price_val,\n",
    "                inv_qty,\n",
    "                variant_obj.get(\"barcode\"),\n",
    "                variant_obj.get(\"image_url\"),\n",
    "                variant_obj.get(\"image_url_1\"),\n",
    "                variant_obj.get(\"image_url_2\"),\n",
    "                variant_obj.get(\"image_url_3\"),\n",
    "                variant_obj.get(\"image_url_4\"),\n",
    "                variant_obj.get(\"upc\"),\n",
    "                variant_obj.get(\"ean\"),\n",
    "                variant_obj.get(\"mpn\"),\n",
    "                self._to_decimal(variant_obj.get(\"costprice\")),\n",
    "                self._to_decimal(variant_obj.get(\"retailprice\")),\n",
    "                height,\n",
    "                width,\n",
    "                length,\n",
    "                weight,\n",
    "                raw_json,\n",
    "                variant_id, self.saas_edge_id\n",
    "            ))\n",
    "            return variant_id\n",
    "    \n",
    "        # insert new\n",
    "        self.cursor.execute(\"\"\"\n",
    "            INSERT INTO productvariants\n",
    "            (product_id, saas_edge_id, sku, variant_name, price, inventory_quantity,\n",
    "             barcode, image_url, image_url_1, image_url_2, image_url_3, image_url_4,\n",
    "             upc, ean, mpn, costprice, retailprice, height, width, length, weight,\n",
    "             raw_variant_data, created_at, updated_at)\n",
    "            VALUES (%s,%s,%s,%s,%s,%s,\n",
    "                    %s,%s,%s,%s,%s,%s,\n",
    "                    %s,%s,%s,%s,%s,%s,%s,%s,%s,\n",
    "                    %s::jsonb, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)\n",
    "            RETURNING variant_id\n",
    "        \"\"\", (\n",
    "            parent_product_id, self.saas_edge_id,\n",
    "            sku,\n",
    "            variant_obj.get(\"variant_name\") or sku,\n",
    "            price_val,\n",
    "            inv_qty,\n",
    "            variant_obj.get(\"barcode\"),\n",
    "            variant_obj.get(\"image_url\"),\n",
    "            variant_obj.get(\"image_url_1\"),\n",
    "            variant_obj.get(\"image_url_2\"),\n",
    "            variant_obj.get(\"image_url_3\"),\n",
    "            variant_obj.get(\"image_url_4\"),\n",
    "            variant_obj.get(\"upc\"),\n",
    "            variant_obj.get(\"ean\"),\n",
    "            variant_obj.get(\"mpn\"),\n",
    "            self._to_decimal(variant_obj.get(\"costprice\")),\n",
    "            self._to_decimal(variant_obj.get(\"retailprice\")),\n",
    "            height,\n",
    "            width,\n",
    "            length,\n",
    "            weight,\n",
    "            raw_json,\n",
    "        ))\n",
    "        return self.cursor.fetchone()[0]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # def _normalize_option_input_type(self, s: str  = None) -> str:\n",
    "    #     \"\"\"\n",
    "    #     Canonicalize UI type names \u2192 one of: 'Swatch', 'Rectangle', 'Dropdown', 'Radio'.\n",
    "    #     Accepts common synonyms.\n",
    "    #     \"\"\"\n",
    "    #     if not s:\n",
    "    #         return \"Dropdown\"\n",
    "    #     t = str(s).strip().lower()\n",
    "    \n",
    "    #     # swatch\n",
    "    #     if t in {\"swatch\", \"colour-swatch\", \"color-swatch\"}:\n",
    "    #         return \"Swatch\"\n",
    "    \n",
    "    #     # rectangle tiles / pills / tags\n",
    "    #     if t in {\"rectangle\", \"rect\", \"pill\", \"tag\", \"tile\", \"chip\"}:\n",
    "    #         return \"Rectangle\"\n",
    "    \n",
    "    #     # dropdown\n",
    "    #     if t in {\"dropdown\", \"select\", \"menu\"}:\n",
    "    #         return \"Dropdown\"\n",
    "    \n",
    "    #     # radio / boolean\n",
    "    #     if t in {\"radio\", \"boolean\", \"yes/no\", \"yesno\"}:\n",
    "    #         return \"Radio\"\n",
    "    \n",
    "    #     # default\n",
    "    #     return \"Dropdown\"\n",
    "    \n",
    "    \n",
    "    # def _infer_option_input_type(self, option_name: str, option_value: str = None) -> str:\n",
    "    #     \"\"\"\n",
    "    #     Heuristics to pick an input_type when none is provided.\n",
    "    #     Returns one of: 'Swatch', 'Rectangle', 'Dropdown', 'Radio'.\n",
    "    #     \"\"\"\n",
    "    #     n = (option_name or \"\").strip().lower()\n",
    "    #     v = (option_value or \"\").strip().lower() if option_value else \"\"\n",
    "    \n",
    "    #     common_colors = {\n",
    "    #         \"black\",\"white\",\"red\",\"blue\",\"navy\",\"green\",\"yellow\",\"orange\",\"purple\",\"pink\",\"grey\",\"gray\",\n",
    "    #         \"brown\",\"beige\",\"silver\",\"gold\",\"maroon\",\"teal\",\"turquoise\",\"cyan\",\"magenta\",\"olive\",\"indigo\",\n",
    "    #         \"charcoal\",\"ivory\",\"cream\",\"khaki\",\"tan\"\n",
    "    #     }\n",
    "    \n",
    "    #     color_keywords    = (\"color\",\"colour\",\"shade\",\"tone\",\"hue\")\n",
    "    #     pattern_keywords  = (\"pattern\",\"finish\",\"texture\",\"print\",\"graphic\")\n",
    "    #     size_keywords     = (\n",
    "    #         \"size\",\"casual size\",\"casual sizes\",\"apparel size\",\"shoe size\",\"footwear size\",\"waist\",\"chest\",\"length\",\n",
    "    #         \"width\",\"height\",\"inseam\",\"neck\",\"sleeve\",\"hip\",\"bust\"\n",
    "    #     )\n",
    "    #     material_keywords = (\"material\",\"fabric\",\"composition\",\"fibre\",\"fiber\",\"shell\",\"lining\")\n",
    "    #     rectangle_keywords= (\"fit\",\"style\",\"cut\",\"theme\",\"collection\",\"grade\",\"rating\",\"flavor\",\"flavour\",\"scent\",\"pack\",\"pack size\",\"model\",\"type\")\n",
    "    \n",
    "    #     # 1) explicit color \u2192 Swatch\n",
    "    #     if any(k in n for k in color_keywords):\n",
    "    #         return \"Swatch\"\n",
    "    \n",
    "    #     # 2) pattern / finish / texture\n",
    "    #     if any(k in n for k in pattern_keywords):\n",
    "    #         if v in common_colors or (v.startswith(\"#\") and re.fullmatch(r\"#?[0-9a-f]{3,8}\", v)):\n",
    "    #             return \"Swatch\"\n",
    "    #         return \"Rectangle\"\n",
    "    \n",
    "    #     # 3) size-like \u2192 Dropdown\n",
    "    #     if any(k in n for k in size_keywords):\n",
    "    #         return \"Dropdown\"\n",
    "    \n",
    "    #     # 4) material-like \u2192 Dropdown\n",
    "    #     if any(k in n for k in material_keywords):\n",
    "    #         return \"Dropdown\"\n",
    "    \n",
    "    #     # 5) tile-ish (rectangle) categories\n",
    "    #     if any(k in n for k in rectangle_keywords):\n",
    "    #         return \"Rectangle\"\n",
    "    \n",
    "    #     # 6) value-based hints\n",
    "    #     if v in {\"yes\",\"no\",\"true\",\"false\"}:\n",
    "    #         return \"Radio\"\n",
    "    #     if v.startswith(\"#\") and re.fullmatch(r\"#?[0-9a-f]{3,8}\", v):\n",
    "    #         return \"Swatch\"\n",
    "    #     if v in common_colors:\n",
    "    #         return \"Swatch\"\n",
    "    \n",
    "    #     # default\n",
    "    #     return \"Dropdown\"\n",
    "    \n",
    "    \n",
    "    # def _ensure_option_and_value(\n",
    "    #     self,\n",
    "    #     parent_product_id: int,\n",
    "    #     option_name: str,\n",
    "    #     option_value: str,\n",
    "    #     input_type: str  = None,\n",
    "    #     usage_type: str = \"default\",\n",
    "    # ) -> tuple[int = None, int  = None]:\n",
    "    #     \"\"\"\n",
    "    #     Upsert product_option/product_option_value for ANY option name (including Custom like 'Casual Sizes').\n",
    "    #     - Preserves the display name as provided (title-casing is NOT forced).\n",
    "    #     - Uses explicit input_type if provided; otherwise infers; then NORMALIZES to canonical set.\n",
    "    #     - usage_type kept as provided (default 'default').\n",
    "    #     \"\"\"\n",
    "    #     name_disp = (option_name or \"\").strip()\n",
    "    #     value     = (option_value or \"\").strip()\n",
    "    #     if not name_disp or not value:\n",
    "    #         return (None, None)\n",
    "    \n",
    "    #     # infer when missing\n",
    "    #     if not (input_type and input_type.strip()):\n",
    "    #         input_type = self._infer_option_input_type(name_disp, value)\n",
    "    \n",
    "    #     # normalize whatever we have (explicit or inferred)\n",
    "    #     input_type = self._normalize_option_input_type(input_type)\n",
    "    \n",
    "    #     # product_option (product_id, name)\n",
    "    #     self.cursor.execute(\"\"\"\n",
    "    #         SELECT product_option_id, input_type\n",
    "    #         FROM product_option\n",
    "    #         WHERE product_id=%s AND name=%s\n",
    "    #         LIMIT 1\n",
    "    #     \"\"\", (parent_product_id, name_disp))\n",
    "    #     row = self.cursor.fetchone()\n",
    "    \n",
    "    #     if row:\n",
    "    #         product_option_id, existing_input_type = row[0], (row[1] or \"\")\n",
    "    #         existing_input_type = self._normalize_option_input_type(existing_input_type)\n",
    "    #         if input_type != existing_input_type or not existing_input_type:\n",
    "    #             self.cursor.execute(\"\"\"\n",
    "    #                 UPDATE product_option\n",
    "    #                    SET input_type=%s, usage_type=%s, updated_at=CURRENT_TIMESTAMP, is_active=true\n",
    "    #                  WHERE product_option_id=%s\n",
    "    #             \"\"\", (input_type, usage_type, product_option_id))\n",
    "    #         else:\n",
    "    #             self.cursor.execute(\"\"\"\n",
    "    #                 UPDATE product_option\n",
    "    #                    SET usage_type=%s, updated_at=CURRENT_TIMESTAMP, is_active=true\n",
    "    #                  WHERE product_option_id=%s\n",
    "    #             \"\"\", (usage_type, product_option_id))\n",
    "    #     else:\n",
    "    #         self.cursor.execute(\"\"\"\n",
    "    #             INSERT INTO product_option\n",
    "    #                 (product_id, name, input_type, usage_type, is_active, sort_order, created_at, updated_at)\n",
    "    #             VALUES\n",
    "    #                 (%s,%s,%s,%s,true,0,CURRENT_TIMESTAMP,CURRENT_TIMESTAMP)\n",
    "    #             RETURNING product_option_id\n",
    "    #         \"\"\", (parent_product_id, name_disp, input_type, usage_type))\n",
    "    #         product_option_id = self.cursor.fetchone()[0]\n",
    "    \n",
    "    #     # product_option_value (single value per variant per option)\n",
    "    #     self.cursor.execute(\"\"\"\n",
    "    #         SELECT product_option_value_id\n",
    "    #         FROM product_option_value\n",
    "    #         WHERE product_option_id=%s AND value=%s\n",
    "    #         LIMIT 1\n",
    "    #     \"\"\", (product_option_id, value))\n",
    "    #     row = self.cursor.fetchone()\n",
    "    \n",
    "    #     if row:\n",
    "    #         product_value_id = row[0]\n",
    "    #         self.cursor.execute(\"\"\"\n",
    "    #             UPDATE product_option_value\n",
    "    #                SET updated_at=CURRENT_TIMESTAMP, is_active=true\n",
    "    #              WHERE product_option_value_id=%s\n",
    "    #         \"\"\", (product_value_id,))\n",
    "    #     else:\n",
    "    #         self.cursor.execute(\"\"\"\n",
    "    #             INSERT INTO product_option_value\n",
    "    #                 (product_option_id, value, sort_order, is_active, created_at, updated_at)\n",
    "    #             VALUES\n",
    "    #                 (%s,%s,0,true,CURRENT_TIMESTAMP,CURRENT_TIMESTAMP)\n",
    "    #             RETURNING product_option_value_id\n",
    "    #         \"\"\", (product_option_id, value))\n",
    "    #         product_value_id = self.cursor.fetchone()[0]\n",
    "    \n",
    "    #     return product_option_id, product_value_id\n",
    "    \n",
    "    \n",
    "    # def _link_variant_option_value(\n",
    "    #     self,\n",
    "    #     variant_id: int,\n",
    "    #     product_option_id: int = None,\n",
    "    #     product_value_id: int = None,\n",
    "    # ):\n",
    "    #     \"\"\"\n",
    "    #     Ensure single row per (variant_id, product_option_id).\n",
    "    #     If exists but value differs \u2192 update to new product_value_id.\n",
    "    #     \"\"\"\n",
    "    #     if not (product_option_id and product_value_id):\n",
    "    #         return\n",
    "    \n",
    "    #     self.cursor.execute(\"\"\"\n",
    "    #         SELECT id, product_value_id\n",
    "    #         FROM product_variant_option_value\n",
    "    #         WHERE variant_id=%s AND product_option_id=%s AND option_scope='product'\n",
    "    #         LIMIT 1\n",
    "    #     \"\"\", (variant_id, product_option_id))\n",
    "    #     row = self.cursor.fetchone()\n",
    "    \n",
    "    #     if row:\n",
    "    #         row_id, existing_val = row\n",
    "    #         if existing_val != product_value_id:\n",
    "    #             self.cursor.execute(\"\"\"\n",
    "    #                 UPDATE product_variant_option_value\n",
    "    #                    SET product_value_id=%s\n",
    "    #                  WHERE id=%s\n",
    "    #             \"\"\", (product_value_id, row_id))\n",
    "    #     else:\n",
    "    #         self.cursor.execute(\"\"\"\n",
    "    #             INSERT INTO product_variant_option_value\n",
    "    #                 (variant_id, option_scope, product_option_id, product_value_id)\n",
    "    #             VALUES (%s, 'product', %s, %s)\n",
    "    #         \"\"\", (variant_id, product_option_id, product_value_id))\n",
    "    \n",
    "    \n",
    "    # def _kv_from_sources(self, obj: dict, *keys: str):\n",
    "    #     \"\"\"\n",
    "    #     Return first non-empty from obj[k], obj['raw_product_data'][k], obj['raw_variant_data'][k].\n",
    "    #     \"\"\"\n",
    "    #     rpd = obj.get(\"raw_product_data\") or {}\n",
    "    #     rvd = obj.get(\"raw_variant_data\") or {}\n",
    "    #     for k in keys:\n",
    "    #         for src in (obj, rpd, rvd):\n",
    "    #             if isinstance(src, dict):\n",
    "    #                 v = src.get(k)\n",
    "    #                 if v not in (None, \"\", \"null\", \"NULL\"):\n",
    "    #                     return v\n",
    "    #     return None\n",
    "    \n",
    "    \n",
    "    # def _collect_op_fields(self, variant_obj: dict) -> list[dict]:\n",
    "    #     \"\"\"\n",
    "    #     Parse OP_* family from root, raw_product_data, raw_variant_data:\n",
    "    #       OP_<Name> = value\n",
    "    #       OP_<Name>_type = Swatch/Dropdown/Radio/Rectangle (synonyms allowed)\n",
    "    #       OP_<Name>_usage = usage_type (optional, default 'default')\n",
    "    #     Returns list of dicts: {name, value, input_type?, usage_type?}\n",
    "    #     \"\"\"\n",
    "    #     pairs = []\n",
    "    #     seen = set()\n",
    "    \n",
    "    #     def scan(d):\n",
    "    #         if not isinstance(d, dict):\n",
    "    #             return\n",
    "    #         for k, v in d.items():\n",
    "    #             if not isinstance(k, str):\n",
    "    #                 continue\n",
    "    #             if not k.lower().startswith(\"op_\"):\n",
    "    #                 continue\n",
    "    #             if v in (None, \"\", \"null\", \"NULL\"):\n",
    "    #                 continue\n",
    "    \n",
    "    #             # Base name & suffix (_type / _usage)\n",
    "    #             # e.g., OP_Casual Sizes, OP_Casual Sizes_type, OP_Casual Sizes_usage\n",
    "    #             if k.lower().endswith(\"_type\") or k.lower().endswith(\"_usage\"):\n",
    "    #                 continue\n",
    "    \n",
    "    #             base = k[3:]  # remove \"OP_\"\n",
    "    #             base = base.replace(\"_\", \" \").strip()  # tolerate underscores\n",
    "    #             name_disp = \" \".join(base.split())     # collapse spaces (keep user-given words)\n",
    "    #             key_base = name_disp.lower()\n",
    "    \n",
    "    #             # fetch optional type & usage from the same dict scope (or from other sources later)\n",
    "    #             t_key = f\"OP_{base}_type\"\n",
    "    #             u_key = f\"OP_{base}_usage\"\n",
    "    #             input_type = d.get(t_key) or d.get(t_key.lower()) or None\n",
    "    #             usage_type = d.get(u_key) or d.get(u_key.lower()) or \"default\"\n",
    "    \n",
    "    #             # Value (string) \u2013 assume one value for this variant\n",
    "    #             val = str(v).strip()\n",
    "    #             if not val:\n",
    "    #                 continue\n",
    "    \n",
    "    #             if key_base in seen:\n",
    "    #                 # don't duplicate same option name from multiple sources\n",
    "    #                 continue\n",
    "    \n",
    "    #             pairs.append({\"name\": name_disp, \"value\": val, \"input_type\": input_type, \"usage_type\": usage_type})\n",
    "    #             seen.add(key_base)\n",
    "    \n",
    "    #     scan(variant_obj)\n",
    "    #     scan(variant_obj.get(\"raw_product_data\") or {})\n",
    "    #     scan(variant_obj.get(\"raw_variant_data\") or {})\n",
    "    \n",
    "    #     return pairs\n",
    "    \n",
    "    \n",
    "    # def _extract_options_from_variant(self, parent_sku: str, variant_sku: str, variant_obj: dict) -> list[dict]:\n",
    "    #     \"\"\"\n",
    "    #     Return a list of {name, value, input_type} using ONLY explicit fields present in payload.\n",
    "    #     Sources (combined; first definition of a name wins):\n",
    "    #       1) 'options': [{name, value, input_type?}, ...]\n",
    "    #       2) OP_* family: OP_<Name> (=value), OP_<Name>_type, OP_<Name>_usage\n",
    "    #       3) option_name + option_value (comma-separated supported) [in root or raw_*]\n",
    "    #       4) numbered 'variant_option_i_*' / 'option_i_*'\n",
    "    #       5) raw_product_data/raw_variant_data 'option_name' + 'option_value'\n",
    "    #     \u274c No inference from SKU text.\n",
    "    #     \"\"\"\n",
    "    #     out: list[dict] = []\n",
    "    #     seen_names = set()\n",
    "    \n",
    "    #     def add_pair(name: str, value: str, input_type: str  = None):\n",
    "    #         n = (name or \"\").strip()\n",
    "    #         v = (value or \"\").strip()\n",
    "    #         if not n or not v:\n",
    "    #             return\n",
    "    #         key = n.lower()\n",
    "    #         if key in seen_names:\n",
    "    #             return\n",
    "    #         # type: explicit \u2192 normalize; else infer \u2192 normalize\n",
    "    #         if input_type and str(input_type).strip():\n",
    "    #             it = self._normalize_option_input_type(input_type)\n",
    "    #         else:\n",
    "    #             it = self._infer_option_input_type(n, v)\n",
    "    #         out.append({\"name\": n, \"value\": v, \"input_type\": it})\n",
    "    #         seen_names.add(key)\n",
    "    \n",
    "    #     # 1) explicit options list\n",
    "    #     if isinstance(variant_obj.get(\"options\"), list):\n",
    "    #         for p in variant_obj[\"options\"]:\n",
    "    #             n = (p.get(\"name\") or \"\").strip()\n",
    "    #             v = (p.get(\"value\") or \"\").strip()\n",
    "    #             t = p.get(\"input_type\")\n",
    "    #             if n and v:\n",
    "    #                 add_pair(n, v, t)\n",
    "    \n",
    "    #     # 2) OP_* family\n",
    "    #     for p in self._collect_op_fields(variant_obj):\n",
    "    #         add_pair(p[\"name\"], p[\"value\"], p.get(\"input_type\"))\n",
    "    \n",
    "    #     # 3) option_name + option_value on root\n",
    "    #     on = self._kv_from_sources(variant_obj, \"option_name\")\n",
    "    #     ov = self._kv_from_sources(variant_obj, \"option_value\")\n",
    "    #     if on and ov:\n",
    "    #         names  = [s.strip() for s in str(on).split(\",\") if s.strip()]\n",
    "    #         values = [s.strip() for s in str(ov).split(\",\") if s.strip()]\n",
    "    #         if names and len(names) == len(values):\n",
    "    #             for n, v in zip(names, values):\n",
    "    #                 add_pair(n, v, None)\n",
    "    \n",
    "    #     # 4) numbered fields (variant_option_i_* OR option_i_*)\n",
    "    #     numbered = []\n",
    "    #     for prefix in (\"variant_option\", \"option\"):\n",
    "    #         for i in range(1, 6):\n",
    "    #             n = (variant_obj.get(f\"{prefix}_{i}_name\") or \"\").strip()\n",
    "    #             v = (variant_obj.get(f\"{prefix}_{i}_value\") or \"\").strip()\n",
    "    #             t = (variant_obj.get(f\"{prefix}_{i}_type\")  or \"\").strip()\n",
    "    #             if n and v:\n",
    "    #                 numbered.append((n, v, t if t else None))\n",
    "    #     for n, v, t in numbered:\n",
    "    #         add_pair(n, v, t)\n",
    "    \n",
    "    #     # 5) raw_* option_name + option_value\n",
    "    #     for src in (\"raw_product_data\", \"raw_variant_data\"):\n",
    "    #         block = variant_obj.get(src) or {}\n",
    "    #         if isinstance(block, dict):\n",
    "    #             on2 = block.get(\"option_name\")\n",
    "    #             ov2 = block.get(\"option_value\")\n",
    "    #             if on2 and ov2:\n",
    "    #                 names  = [s.strip() for s in str(on2).split(\",\") if s.strip()]\n",
    "    #                 values = [s.strip() for s in str(ov2).split(\",\") if s.strip()]\n",
    "    #                 if names and len(names) == len(values):\n",
    "    #                     for n, v in zip(names, values):\n",
    "    #                         add_pair(n, v, None)\n",
    "    \n",
    "    #     return out\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def _filter_parent_updates(self, row: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Keep only fields that are safe for the parent product, even when they come\n",
    "        on a variant row. Do NOT include variant-only fields, images, or SKU fields.\n",
    "    \n",
    "        Also:\n",
    "          - Merge raw_product_data.\n",
    "          - If 'price' missing at root, try raw_product_data['Base Price'] or ['Price'].\n",
    "        \"\"\"\n",
    "        if not isinstance(row, dict):\n",
    "            return {}\n",
    "    \n",
    "        allowed_root = {\n",
    "            \"name\",\n",
    "            \"brand\",\n",
    "            \"category\", \"categories\", \"category_id\", \"category_path\",\n",
    "            \"tags\",\n",
    "            \"text_attr_1\", \"text_attr_2\",\n",
    "            \"price\",                    # allow parent base price\n",
    "            \"description\",              # if you have this column\n",
    "            \"short_description\",        # if you have this column\n",
    "        }\n",
    "    \n",
    "        safe = {}\n",
    "    \n",
    "        # allow-list root keys (non-empty only)\n",
    "        for k in allowed_root:\n",
    "            v = row.get(k)\n",
    "            if v not in (None, \"\", \"null\", \"NULL\"):\n",
    "                safe[k] = v\n",
    "    \n",
    "        # merge raw_product_data if present\n",
    "        if isinstance(row.get(\"raw_product_data\"), dict):\n",
    "            # let _upsert_parent handle merge-at-write\n",
    "            safe[\"raw_product_data\"] = row[\"raw_product_data\"]\n",
    "    \n",
    "            # parent price fallback from raw data if root price not sent\n",
    "            if \"price\" not in safe:\n",
    "                rpd = row[\"raw_product_data\"]\n",
    "                for key in (\"Base Price\", \"Price\", \"BasePrice\"):\n",
    "                    if rpd.get(key) not in (None, \"\", \"null\", \"NULL\"):\n",
    "                        safe[\"price\"] = rpd[key]\n",
    "                        break\n",
    "    \n",
    "        # NEVER apply images from variant rows to the parent\n",
    "        # (so we intentionally ignore image_url, image_url_1..9)\n",
    "    \n",
    "        return safe\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def _plain(self, v):\n",
    "        \"\"\"Strip psycopg2.sql objects from params \u2192 plain Python types.\"\"\"\n",
    "        try:\n",
    "            import psycopg2.sql as psql\n",
    "            if isinstance(v, (psql.Composed, psql.SQL, psql.Identifier, psql.Literal)):\n",
    "                try:\n",
    "                    return v.as_string(self.conn)\n",
    "                except Exception:\n",
    "                    return str(v)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return v\n",
    "\n",
    "   \n",
    "\n",
    "    def _handle_variant_assets_inline(self, variant_id, variant_sku, variant_obj):\n",
    "        \"\"\"\n",
    "        For each variant image:\n",
    "          1) Download \u2192 upload to GCS\n",
    "          2) Upsert into `assets` (dedupe by checksum)\n",
    "          3) Ensure a `productassets` row at offset 101..110 (product scope)\n",
    "          4) Ensure a `productassets_variants` row at 1..10 (variant scope)\n",
    "          5) Append unique images to parent slots 1..10 and mirror back to products.image_url*\n",
    "        \"\"\"\n",
    "        VARIANT_POS_OFFSET = 100\n",
    "        IMAGE_COLS = [\"image_url\"] + [f\"image_url_{i}\" for i in range(1, 10)]\n",
    "    \n",
    "        # find parent product_id for this variant\n",
    "        self.cursor.execute(\"\"\"\n",
    "            SELECT product_id\n",
    "            FROM productvariants\n",
    "            WHERE variant_id=%s AND saas_edge_id=%s\n",
    "            LIMIT 1\n",
    "        \"\"\", (variant_id, self.saas_edge_id))\n",
    "        row = self.cursor.fetchone()\n",
    "        if not row:\n",
    "            print(f\"Variant asset upload skipped for {variant_sku}: parent product not found\")\n",
    "            return\n",
    "        parent_product_id = row[0]\n",
    "    \n",
    "        pa_cols  = self._get_table_columns(\"productassets\")\n",
    "        pav_cols = self._get_table_columns(\"productassets_variants\")\n",
    "    \n",
    "        # candidates to promote to parent (preserve order)\n",
    "        promote_candidates = []\n",
    "    \n",
    "        for pos in range(10):\n",
    "            col = IMAGE_COLS[pos]\n",
    "            src = variant_obj.get(col)\n",
    "            if not src:\n",
    "                continue\n",
    "    \n",
    "            try:\n",
    "                # 1) download + meta\n",
    "                file_data, meta = self._download_image_with_meta(src)\n",
    "    \n",
    "                # 2) upload to GCS\n",
    "                file_name = f\"{variant_sku}.jpg\" if pos == 0 else f\"{variant_sku}_{pos}.jpg\"\n",
    "                subdir    = f\"{self.saas_edge_id}/catalog_edge/dam/public/imported\"\n",
    "                gcp_url   = self.upload_asset_to_gcp_bucket(file_data, file_name, GCP_BUCKET_NAME, credentials, subdir) or src\n",
    "    \n",
    "                # 3) ensure asset exists in `assets` (global dedupe)\n",
    "                saas_asset_id = self._upsert_asset_and_get_id(gcp_url, src, meta)\n",
    "    \n",
    "                # === A) productassets @ 101..110 (variant offset) ===\n",
    "                pa_slot = VARIANT_POS_OFFSET + (pos + 1)\n",
    "                role    = \"main\" if pos == 0 else \"linked\"\n",
    "    \n",
    "                # de-dupe ONLY inside 101..110\n",
    "                if \"saas_asset_id\" in pa_cols:\n",
    "                    self.cursor.execute(\"\"\"\n",
    "                        DELETE FROM productassets\n",
    "                        WHERE product_id=%s AND saas_edge_id=%s\n",
    "                          AND (asset_url=%s OR saas_asset_id=%s)\n",
    "                          AND channel_asset_pos BETWEEN %s AND %s\n",
    "                          AND channel_asset_pos <> %s\n",
    "                    \"\"\", (parent_product_id, self.saas_edge_id, gcp_url, saas_asset_id,\n",
    "                          VARIANT_POS_OFFSET+1, VARIANT_POS_OFFSET+10, pa_slot))\n",
    "                else:\n",
    "                    self.cursor.execute(\"\"\"\n",
    "                        DELETE FROM productassets\n",
    "                        WHERE product_id=%s AND saas_edge_id=%s\n",
    "                          AND asset_url=%s\n",
    "                          AND channel_asset_pos BETWEEN %s AND %s\n",
    "                          AND channel_asset_pos <> %s\n",
    "                    \"\"\", (parent_product_id, self.saas_edge_id, gcp_url,\n",
    "                          VARIANT_POS_OFFSET+1, VARIANT_POS_OFFSET+10, pa_slot))\n",
    "    \n",
    "                self.cursor.execute(\"\"\"\n",
    "                    SELECT asset_id\n",
    "                    FROM productassets\n",
    "                    WHERE product_id=%s AND saas_edge_id=%s AND channel_asset_pos=%s\n",
    "                    LIMIT 1\n",
    "                \"\"\", (parent_product_id, self.saas_edge_id, pa_slot))\n",
    "                pa = self.cursor.fetchone()\n",
    "    \n",
    "                if pa:\n",
    "                    set_bits = [\"asset_url=%s\",\"source_url=%s\",\"asset_type='image'\",\"is_default=%s\"]\n",
    "                    params   = [gcp_url, src, (pos == 0)]\n",
    "                    if \"saas_asset_id\" in pa_cols:\n",
    "                        set_bits.append(\"saas_asset_id=%s\"); params.append(saas_asset_id)\n",
    "                    if \"role\" in pa_cols:\n",
    "                        set_bits.append(\"role=%s\"); params.append(role)\n",
    "                    if \"position\" in pa_cols:\n",
    "                        set_bits.append(\"position=%s\"); params.append(pa_slot)\n",
    "    \n",
    "                    self.cursor.execute(f\"\"\"\n",
    "                        UPDATE productassets SET {\", \".join(set_bits)}\n",
    "                        WHERE asset_id=%s\n",
    "                    \"\"\", (*params, pa[0]))\n",
    "                    asset_id = pa[0]\n",
    "                else:\n",
    "                    # build INSERT with created_at added LAST\n",
    "                    cols = [\"product_id\",\"saas_edge_id\",\"asset_url\",\"channel_asset_pos\",\"source_url\",\"asset_type\",\"is_default\"]\n",
    "                    ph   =  [\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\"]\n",
    "                    vals = [parent_product_id, self.saas_edge_id, gcp_url, pa_slot, src, \"image\", (pos == 0)]\n",
    "    \n",
    "                    if \"saas_asset_id\" in pa_cols:\n",
    "                        cols.append(\"saas_asset_id\"); ph.append(\"%s\"); vals.append(saas_asset_id)\n",
    "                    if \"role\" in pa_cols:\n",
    "                        cols.append(\"role\"); ph.append(\"%s\"); vals.append(role)\n",
    "                    if \"position\" in pa_cols:\n",
    "                        cols.append(\"position\"); ph.append(\"%s\"); vals.append(pa_slot)\n",
    "    \n",
    "                    cols.append(\"created_at\"); ph.append(\"CURRENT_TIMESTAMP\")\n",
    "    \n",
    "                    self.cursor.execute(f\"\"\"\n",
    "                        INSERT INTO productassets ({\", \".join(cols)})\n",
    "                        VALUES ({\", \".join(ph)})\n",
    "                        RETURNING asset_id\n",
    "                    \"\"\", tuple(vals))\n",
    "                    asset_id = self.cursor.fetchone()[0]\n",
    "    \n",
    "                # === B) productassets_variants @ 1..10 ===\n",
    "                if \"saas_asset_id\" in pav_cols:\n",
    "                    self.cursor.execute(\"\"\"\n",
    "                        DELETE FROM productassets_variants\n",
    "                        WHERE variant_id=%s AND saas_edge_id=%s\n",
    "                          AND (asset_url=%s OR saas_asset_id=%s)\n",
    "                          AND var_asset_pos <> %s\n",
    "                    \"\"\", (variant_id, self.saas_edge_id, gcp_url, saas_asset_id, pos+1))\n",
    "                else:\n",
    "                    self.cursor.execute(\"\"\"\n",
    "                        DELETE FROM productassets_variants\n",
    "                        WHERE variant_id=%s AND saas_edge_id=%s\n",
    "                          AND asset_url=%s\n",
    "                          AND var_asset_pos <> %s\n",
    "                    \"\"\", (variant_id, self.saas_edge_id, gcp_url, pos+1))\n",
    "    \n",
    "                self.cursor.execute(\"\"\"\n",
    "                    SELECT productasset_id\n",
    "                    FROM productassets_variants\n",
    "                    WHERE variant_id=%s AND saas_edge_id=%s AND var_asset_pos=%s\n",
    "                    LIMIT 1\n",
    "                \"\"\", (variant_id, self.saas_edge_id, pos+1))\n",
    "                pv = self.cursor.fetchone()\n",
    "    \n",
    "                if pv:\n",
    "                    set_bits = [\n",
    "                        \"productasset_id=%s\",\"asset_url=%s\",\"source_url=%s\",\n",
    "                        \"channel_asset_url=%s\",\"asset_type='image'\",\"is_default=%s\"\n",
    "                    ]\n",
    "                    params   = [asset_id, gcp_url, src, gcp_url, (pos == 0)]\n",
    "                    if \"saas_asset_id\" in pav_cols:\n",
    "                        set_bits.insert(1, \"saas_asset_id=%s\"); params.insert(1, saas_asset_id)\n",
    "                    if \"role\" in pav_cols:\n",
    "                        set_bits.append(\"role=%s\"); params.append(role)\n",
    "                    if \"position\" in pav_cols:\n",
    "                        set_bits.append(\"position=%s\"); params.append(pos+1)\n",
    "    \n",
    "                    self.cursor.execute(f\"\"\"\n",
    "                        UPDATE productassets_variants\n",
    "                        SET {\", \".join(set_bits)}, updated_at=CURRENT_TIMESTAMP\n",
    "                        WHERE variant_id=%s AND saas_edge_id=%s AND var_asset_pos=%s\n",
    "                    \"\"\", (*params, variant_id, self.saas_edge_id, pos+1))\n",
    "                else:\n",
    "                    # build INSERT with created_at/updated_at added LAST\n",
    "                    cols = [\n",
    "                        \"productasset_id\",\"variant_id\",\"var_asset_pos\",\"saas_edge_id\",\n",
    "                        \"asset_url\",\"alt_text\",\"asset_type\",\"channel_asset_pos\",\n",
    "                        \"channel_asset_url\",\"is_default\",\"source_url\"\n",
    "                    ]\n",
    "                    ph   =  [\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\"]\n",
    "                    vals = [asset_id, variant_id, pos+1, self.saas_edge_id,\n",
    "                            gcp_url, \"\", \"image\", pos+1, gcp_url, (pos == 0), src]\n",
    "    \n",
    "                    if \"saas_asset_id\" in pav_cols:\n",
    "                        cols.insert(4, \"saas_asset_id\"); ph.insert(4, \"%s\"); vals.insert(4, saas_asset_id)\n",
    "                    if \"role\" in pav_cols:\n",
    "                        cols.append(\"role\"); ph.append(\"%s\"); vals.append(role)\n",
    "                    if \"position\" in pav_cols:\n",
    "                        cols.append(\"position\"); ph.append(\"%s\"); vals.append(pos+1)\n",
    "    \n",
    "                    cols += [\"created_at\",\"updated_at\"]\n",
    "                    ph   += [\"CURRENT_TIMESTAMP\",\"CURRENT_TIMESTAMP\"]\n",
    "    \n",
    "                    self.cursor.execute(f\"\"\"\n",
    "                        INSERT INTO productassets_variants ({\", \".join(cols)})\n",
    "                        VALUES ({\", \".join(ph)})\n",
    "                    \"\"\", tuple(vals))\n",
    "    \n",
    "                # queue for possible promotion to parent 1..10\n",
    "                promote_candidates.append({\"gcp_url\": gcp_url, \"saas_asset_id\": saas_asset_id, \"src\": src})\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\"Variant asset upload failed for {variant_sku} pos {pos+1}: {e}\")\n",
    "    \n",
    "        # === Append unique variant images to parent (1..10) & mirror back ===\n",
    "        try:\n",
    "            self.cursor.execute(\"\"\"\n",
    "                SELECT asset_id, asset_url, saas_asset_id, channel_asset_pos\n",
    "                FROM productassets\n",
    "                WHERE product_id=%s AND saas_edge_id=%s\n",
    "                  AND channel_asset_pos BETWEEN 1 AND 10\n",
    "                ORDER BY channel_asset_pos\n",
    "            \"\"\", (parent_product_id, self.saas_edge_id))\n",
    "            rows = self.cursor.fetchall()\n",
    "    \n",
    "            occupied  = {pos for (_aid, _url, _sid, pos) in rows}\n",
    "            free      = [i for i in range(1, 11) if i not in occupied]\n",
    "            seen_ids  = {sid for (_aid, _url, sid, _pos) in rows if sid is not None}\n",
    "            seen_urls = {url for (_aid, url, _sid, _pos) in rows}\n",
    "            had_any_before = bool(rows)\n",
    "    \n",
    "            for cand in promote_candidates:\n",
    "                if not free:\n",
    "                    break\n",
    "                url = cand[\"gcp_url\"]; sid = cand.get(\"saas_asset_id\"); src = cand.get(\"src\")\n",
    "    \n",
    "                # dedupe by saas_asset_id (preferred) or URL\n",
    "                if \"saas_asset_id\" in pa_cols and sid:\n",
    "                    if sid in seen_ids:\n",
    "                        continue\n",
    "                else:\n",
    "                    if url in seen_urls:\n",
    "                        continue\n",
    "    \n",
    "                slot = free.pop(0)\n",
    "                is_default = (slot == 1 and not had_any_before)\n",
    "                role = \"main\" if is_default else \"linked\"\n",
    "    \n",
    "                # build INSERT with created_at added LAST\n",
    "                cols = [\"product_id\",\"saas_edge_id\",\"asset_url\",\"channel_asset_pos\",\"source_url\",\"asset_type\",\"is_default\"]\n",
    "                ph   =  [\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\"]\n",
    "                vals = [parent_product_id, self.saas_edge_id, url, slot, src, \"image\", is_default]\n",
    "    \n",
    "                if \"saas_asset_id\" in pa_cols and sid is not None:\n",
    "                    cols.append(\"saas_asset_id\"); ph.append(\"%s\"); vals.append(sid)\n",
    "                if \"role\" in pa_cols:\n",
    "                    cols.append(\"role\"); ph.append(\"%s\"); vals.append(role)\n",
    "                if \"position\" in pa_cols:\n",
    "                    cols.append(\"position\"); ph.append(\"%s\"); vals.append(slot)\n",
    "    \n",
    "                cols.append(\"created_at\"); ph.append(\"CURRENT_TIMESTAMP\")\n",
    "    \n",
    "                self.cursor.execute(f\"\"\"\n",
    "                    INSERT INTO productassets ({\", \".join(cols)})\n",
    "                    VALUES ({\", \".join(ph)})\n",
    "                \"\"\", tuple(vals))\n",
    "    \n",
    "                if sid is not None:\n",
    "                    seen_ids.add(sid)\n",
    "                seen_urls.add(url)\n",
    "                had_any_before = True\n",
    "    \n",
    "            # mirror back to products.image_url*\n",
    "            self.cursor.execute(\"\"\"\n",
    "                SELECT asset_url, channel_asset_pos\n",
    "                FROM productassets\n",
    "                WHERE product_id=%s AND saas_edge_id=%s\n",
    "                  AND channel_asset_pos BETWEEN 1 AND 10\n",
    "                ORDER BY channel_asset_pos\n",
    "            \"\"\", (parent_product_id, self.saas_edge_id))\n",
    "            rows = self.cursor.fetchall()\n",
    "            col_names = [\"image_url\"] + [f\"image_url_{i}\" for i in range(1, 10)]\n",
    "            col_updates = { col_names[pos-1]: url for (url, pos) in rows if 1 <= pos <= 10 and url }\n",
    "            if col_updates:\n",
    "                set_clause = \", \".join(f\"{col}=%s\" for col in col_updates)\n",
    "                self.cursor.execute(\n",
    "                    f\"UPDATE products SET {set_clause} WHERE product_id=%s AND saas_edge_id=%s\",\n",
    "                    (*col_updates.values(), parent_product_id, self.saas_edge_id)\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"WARN: failed to append variant images to parent for {variant_sku}: {e}\")\n",
    "\n",
    " \n",
    "\n",
    "    def _apply_category_mapping_safe(self, product_id: int, request_obj: dict):\n",
    "        \"\"\"\n",
    "        Best-effort product\u2192category mapping with CREATE-IF-MISSING:\n",
    "          1) category_id \u2192 map directly\n",
    "          2) category/category_path string like 'A > B > C'\n",
    "             \u2192 ensure path exists (create missing nodes) \u2192 map to leaf\n",
    "        Never raises.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not product_id:\n",
    "                return\n",
    "    \n",
    "            if not any(k in request_obj and request_obj[k] not in (None, \"\", \"null\", \"NULL\")\n",
    "                       for k in (\"category_id\", \"category\", \"category_path\")):\n",
    "                return\n",
    "    \n",
    "            # 1) explicit id wins\n",
    "            cat_id = request_obj.get(\"category_id\")\n",
    "            if cat_id not in (None, \"\", \"null\", \"NULL\"):\n",
    "                try:\n",
    "                    self._map_product_to_category_id(product_id, int(cat_id))\n",
    "                    return\n",
    "                except Exception as e:\n",
    "                    print(f\"WARN: category_id mapping failed for product {product_id}: {e}\")\n",
    "    \n",
    "            # 2) path or single label \u2192 ensure + map\n",
    "            cat_val = request_obj.get(\"category\") or request_obj.get(\"category_path\")\n",
    "            if cat_val:\n",
    "                ok = self._ensure_category_and_map(product_id, cat_val)\n",
    "                if not ok:\n",
    "                    # optional: one last fuzzy try on leaf only\n",
    "                    leaf = str(cat_val).split(\">\")[-1].strip()\n",
    "                    if leaf:\n",
    "                        self._map_product_to_category_label(product_id, leaf)\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"WARN: category mapping skipped for product {product_id}: {e}\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def process_table_data(self, table_name, request_objects, batch_size=100):\n",
    "        \"\"\"Main method to route requests to appropriate handlers\"\"\"\n",
    "        handlers = {\n",
    "            'products': self._handle_products,\n",
    "            'productassets': self._handle_product_assets,\n",
    "            'productattributes': self._handle_product_attributes,\n",
    "            'product_parent_variants': self._handle_parent_and_variants\n",
    "        }\n",
    "        \n",
    "        handler = handlers.get(table_name)\n",
    "        \n",
    "        if not handler:\n",
    "            raise ValueError(f\"Unsupported table: {table_name}\")\n",
    "            \n",
    "        print(f\"INFO: Processing {len(request_objects)} objects for table: {table_name}\")\n",
    "        return handler(request_objects, batch_size)\n",
    "\n",
    "    def _handle_products(self, request_objects, batch_size):\n",
    "        success_count = failure_count = current_batch = 0\n",
    "        failed_items = []\n",
    "\n",
    "        try:\n",
    "            # Start a new transaction\n",
    "            self.cursor.execute(\"BEGIN\")\n",
    "            totalcount=len(request_objects)\n",
    "            for idx, request_obj in enumerate(request_objects, 1):\n",
    "                sp_name = f\"product_sp_{idx}\"\n",
    "                self.cursor.execute(f\"SAVEPOINT {sp_name}\")\n",
    "               \n",
    "                try:\n",
    "                    # Validate request object\n",
    "                    if not isinstance(request_obj, dict):\n",
    "                        raise ValueError(\"Invalid request object format\")\n",
    "\n",
    "                    request_obj['saas_edge_id'] = self.saas_edge_id\n",
    "                    id_field = 'sku' if request_obj.get('sku') else 'product_id'\n",
    "                    id_value = request_obj.get(id_field)\n",
    "                   \n",
    "\n",
    "                    if not id_value:\n",
    "                        raise ValueError(f\"{id_field.upper()} not found in request object\")\n",
    "\n",
    "                    # Check if product exists\n",
    "                    check_query = \"\"\"\n",
    "                    SELECT product_id, raw_product_data \n",
    "                    FROM products \n",
    "                    WHERE {} = %s AND saas_edge_id = %s\n",
    "                    \"\"\".format(id_field)\n",
    "\n",
    "                    self.cursor.execute(check_query, (id_value, self.saas_edge_id))\n",
    "                    existing_product = self.cursor.fetchone()\n",
    "\n",
    "                    if existing_product:\n",
    "                        # Update existing product\n",
    "                        product_id = existing_product[0]\n",
    "                        _raw_product_data = existing_product[1] or {}\n",
    "\n",
    "                        if isinstance(_raw_product_data, str):\n",
    "                            try:\n",
    "                                _raw_product_data = json.loads(_raw_product_data)\n",
    "                            except json.JSONDecodeError:\n",
    "                                _raw_product_data = {}\n",
    "\n",
    "                        update_fields = []\n",
    "                        update_values = []\n",
    "\n",
    "                        for key, value in request_obj.items():\n",
    "                            if key not in [id_field, 'saas_edge_id']:\n",
    "                                if isinstance(value, dict):\n",
    "                                    if key == \"raw_product_data\":\n",
    "                                        value = json.dumps({**_raw_product_data, **value})\n",
    "                                    else:\n",
    "                                        value = json.dumps(value)\n",
    "                                update_fields.append(f\"\\\"{key}\\\" = %s\")\n",
    "                                update_values.append(clean_value(value))\n",
    "                                \n",
    "\n",
    "                        if update_fields:\n",
    "                            update_values.extend([self.saas_edge_id, product_id])\n",
    "                            \n",
    "                            \n",
    "                            update_query = \"\"\"\n",
    "                            UPDATE products\n",
    "                            SET {}, last_update_ts = CURRENT_TIMESTAMP\n",
    "                            WHERE saas_edge_id = %s AND product_id = %s\n",
    "                            \"\"\".format(\", \".join(update_fields))\n",
    "                            \n",
    "                            \n",
    "\n",
    "                            self.cursor.execute(update_query, update_values)\n",
    "                            # if  request_obj.get(\"image_url\") or request_obj.get(\"assets\"):\n",
    "                            #     success, failure, failed_items=self._handle_product_assets(request_objects, batch_size)\n",
    "                            #     print(f\"success:{success},failure:{failure},failure_count:{failed_items}\")\n",
    "                            print(f\"INFO: Successfully updated product with {id_field}: {id_value}\")\n",
    "                        try:\n",
    "                            self._apply_category_create_and_map(product_id, request_obj or {})\n",
    "                        except Exception as e:\n",
    "                            print(f\"WARN: parent category mapping skipped for product {product_id}: {e}\")\n",
    "\n",
    "                    elif self.create_new_product:\n",
    "                        # Insert new product\n",
    "                        insert_fields = []\n",
    "                        insert_values = []\n",
    "\n",
    "                        for key, value in request_obj.items():\n",
    "                            insert_fields.append(f\"\\\"{key}\\\"\")\n",
    "                            \n",
    "                            if isinstance(value, dict):\n",
    "                                updated_value = value.copy()\n",
    "                                value = json.dumps(updated_value)\n",
    "                            insert_values.append(clean_value(value))\n",
    "\n",
    "                        placeholders = [\"%s\"] * len(insert_values)\n",
    "                        insert_query = \"\"\"\n",
    "                        INSERT INTO products ({}, last_update_ts)\n",
    "                        VALUES ({}, CURRENT_TIMESTAMP)\n",
    "                        \"\"\".format(\n",
    "                            \", \".join(insert_fields),\n",
    "                            \", \".join(placeholders)\n",
    "                        )\n",
    "\n",
    "                        self.cursor.execute(insert_query, insert_values)\n",
    "                        product_id = self.cursor.fetchone()[0]\n",
    "                        print(f\"INFO: Successfully created new product with {id_field}: {id_value}\")\n",
    "\n",
    "\n",
    "                        try:\n",
    "                            self._apply_category_create_and_map(product_id, request_obj)\n",
    "                        except Exception as e:\n",
    "                            print(f\"WARN: category mapping skipped for product {product_id}: {e}\")\n",
    "                        # if   request_obj.get(\"image_url\") or request_obj.get(\"assets\"):\n",
    "                        #     success, failure, failed_items=self._handle_product_assets(request_objects, batch_size)\n",
    "                        #     print(f\"success:{success},failure:{failure},failure_count:{failed_items}\")\n",
    "                        # print(f\"INFO: Successfully created new product with {id_field}: {id_value}\")\n",
    "                    else:\n",
    "                        raise ValueError(f\"Product {id_value} not found and create_new_product is False\")\n",
    "                    self.cursor.execute(f\"RELEASE SAVEPOINT {sp_name}\")\n",
    "\n",
    "\n",
    "                    success_count += 1\n",
    "                    current_batch += 1\n",
    "\n",
    "                except Exception as item_error:\n",
    "                    # Log the error for the current item\n",
    "                    self.cursor.execute(f\"ROLLBACK TO SAVEPOINT {sp_name}\")\n",
    "                    self.cursor.execute(f\"RELEASE SAVEPOINT {sp_name}\")\n",
    "                    failure_count += 1\n",
    "                    failed_items.append({\n",
    "                        'id_value': id_value if 'id_value' in locals() else None,\n",
    "                        'error': str(item_error),\n",
    "                        'request_obj': request_obj\n",
    "                    })\n",
    "                    print(f\"ERROR processing item: {str(item_error)}\")\n",
    "                if (success_count + failure_count) % 20 == 0:\n",
    "                    try:\n",
    "                        update_job_each_5= update_job_details(self.job_id,totalcount,success_count,failure_count)\n",
    "                        self.conn.commit()                      # commit 20-record chunk\n",
    "                        self.cursor.execute(\"BEGIN\")\n",
    "                        print(f\"INFO: Committed batch of 20 products. Total successful: {success_count}, failed: {failure_count}\")\n",
    "                    except Exception as commit_error:\n",
    "                        print(f\"ERROR: Failed to commit batch: {str(commit_error)}\")\n",
    "                        self.conn.rollback()\n",
    "                        self.cursor.execute(\"BEGIN\")\n",
    "                        \n",
    "\n",
    "               \n",
    "                if current_batch >= batch_size:\n",
    "                    try:\n",
    "                        self.conn.commit()\n",
    "                        print(f\"INFO: Committed batch of {current_batch} products. Total successful: {success_count}\")\n",
    "                    except Exception as commit_error:\n",
    "                        # Handle any commit errors\n",
    "                        print(f\"ERROR: Failed to commit batch: {str(commit_error)}\")\n",
    "                        self.conn.rollback()\n",
    "                    finally:\n",
    "                        current_batch = 0\n",
    "                        self.cursor.execute(\"BEGIN\")  # Start a new transaction for the next batch\n",
    "\n",
    "            # Commit any remaining items in the last batch\n",
    "            if current_batch > 0:\n",
    "                try:\n",
    "                    self.conn.commit()\n",
    "                    print(f\"INFO: Committed final batch of {current_batch} products\")\n",
    "                except Exception as commit_error:\n",
    "                    print(f\"ERROR: Failed to commit final batch: {str(commit_error)}\")\n",
    "                    self.conn.rollback()\n",
    "\n",
    "            self.conn.commit()\n",
    "            IMAGE_COLUMNS = [\n",
    "                \"image_url\",\"image_url_1\",\"image_url_2\",\"image_url_3\",\"image_url_4\",\n",
    "                \"image_url_5\",\"image_url_6\",\"image_url_7\",\"image_url_8\",\"image_url_9\"\n",
    "            ]\n",
    "            has_any_image = any(\n",
    "                any(request_obj.get(col) for col in IMAGE_COLUMNS)\n",
    "                for request_obj in request_objects\n",
    "            )\n",
    "            if has_any_image:\n",
    "                img_success, img_failure, img_failed_items = self._handle_product_assets(request_objects, batch_size)\n",
    "                print(f\"INFO: Assets processed: {img_success} succeeded, {img_failure} failed\")\n",
    "            else:\n",
    "                print(\"INFO: No image URLs found in any request object; skipping asset processing\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            print(f\"ERROR: Batch processing failed: {str(e)}\")\n",
    "            print(f\"ERROR: {traceback.format_exc()}\")\n",
    "\n",
    "        return totalcount,success_count, failure_count, failed_items\n",
    "\n",
    "    \n",
    "    # def _handle_parent_and_variants(self, request_objects, batch_size):\n",
    "    #     \"\"\"\n",
    "    #     Supports BOTH:\n",
    "    #       \u2022 sku=\"PARENT:CHILD\"      (old)\n",
    "    #       \u2022 sku=\"CHILD\", parent_sku=\"PARENT\"  (new)\n",
    "    \n",
    "    #     Parent rows: upload parent images to productassets slots 1..10 and mirror to products.image_url*.\n",
    "    #     Variant rows: keep variant flow; inventory fallback; options only if present/recognized; images use 101..110.\n",
    "    #     \"\"\"\n",
    "    #     IMAGE_COLUMNS = [\"image_url\"] + [f\"image_url_{i}\" for i in range(1, 10)]\n",
    "    \n",
    "    #     success_count = failure_count = current_batch = 0\n",
    "    #     failed_items, totalcount = [], len(request_objects)\n",
    "    \n",
    "    #     try:\n",
    "    #         self.cursor.execute(\"BEGIN\")\n",
    "    #         for idx, request_obj in enumerate(request_objects, 1):\n",
    "    #             sp_name = f\"pv_sp_{idx}\"\n",
    "    #             self.cursor.execute(f\"SAVEPOINT {sp_name}\")\n",
    "    \n",
    "    #             try:\n",
    "    #                 sku_field = request_obj.get(\"sku\")\n",
    "    #                 explicit_parent = (request_obj.get(\"parent_sku\") or \"\").strip()\n",
    "    #                 if not sku_field:\n",
    "    #                     raise ValueError(\"SKU is required\")\n",
    "    \n",
    "    #                 sku_str = sku_field.strip()\n",
    "    #                 if \":\" in sku_str:\n",
    "    #                     parent_sku, variant_sku = [x.strip() for x in sku_str.split(\":\", 1)]\n",
    "    #                     is_variant = True\n",
    "    #                 else:\n",
    "    #                     if explicit_parent and explicit_parent != sku_str:\n",
    "    #                         parent_sku, variant_sku = explicit_parent, sku_str\n",
    "    #                         is_variant = True\n",
    "    #                     else:\n",
    "    #                         parent_sku, variant_sku = sku_str, None\n",
    "    #                         is_variant = False\n",
    "    \n",
    "    #                     if is_variant:\n",
    "    #                         # Only allow parent-safe fields to touch the parent\n",
    "    #                         parent_payload = self._filter_parent_updates(request_obj)\n",
    "    #                         parent_payload[\"sku\"] = parent_sku        # make payload \u201clook\u201d like parent\n",
    "    #                     else:\n",
    "    #                         parent_payload = request_obj              # parent rows can update freely\n",
    "                        \n",
    "    #                     parent_id = self._upsert_parent(parent_sku, parent_payload)\n",
    "                        \n",
    "    #                 # parent-only row \u2192 handle parent images (slots 1..10) + mirror\n",
    "    #                 if not is_variant:\n",
    "    #                     try:\n",
    "    #                         if any(request_obj.get(col) for col in IMAGE_COLUMNS):\n",
    "    #                             ...\n",
    "    #                             print(f\"Synced parent image columns for SKU {parent_sku}\")\n",
    "    #                     except Exception as parent_img_err:\n",
    "    #                         print(f\"WARN: parent image processing failed for {parent_sku}: {parent_img_err}\")\n",
    "                    \n",
    "    #                     self.cursor.execute(f\"RELEASE SAVEPOINT {sp_name}\")\n",
    "    #                     success_count += 1\n",
    "    #                     current_batch += 1\n",
    "    #                     if current_batch >= batch_size:\n",
    "    #                         self.conn.commit(); current_batch = 0; self.cursor.execute(\"BEGIN\")\n",
    "    #                     continue\n",
    "                       \n",
    "    #                 # variant row\n",
    "    #                 variant_obj = dict(request_obj)\n",
    "    #                 variant_obj[\"sku\"] = variant_sku\n",
    "    #                 variant_obj.setdefault(\"variant_name\", variant_sku)\n",
    "    \n",
    "    #                 # inventory fallback: inventory_quantity \u2190 quantity \u2190 stock_quantity\n",
    "    #                 if variant_obj.get(\"inventory_quantity\") in (None, \"\", \"null\", \"NULL\"):\n",
    "    #                     if \"quantity\" in variant_obj and variant_obj[\"quantity\"] not in (None, \"\", \"null\", \"NULL\"):\n",
    "    #                         variant_obj[\"inventory_quantity\"] = variant_obj[\"quantity\"]\n",
    "    #                     elif \"stock_quantity\" in variant_obj and variant_obj[\"stock_quantity\"] not in (None, \"\", \"null\", \"NULL\"):\n",
    "    #                         variant_obj[\"inventory_quantity\"] = variant_obj[\"stock_quantity\"]\n",
    "                \n",
    "    #                 # \u2705 make sure this is a real int for PG\n",
    "    #                 variant_obj[\"inventory_quantity\"] = self._to_int(variant_obj.get(\"inventory_quantity\"), default=0)\n",
    "\n",
    "    \n",
    "    #                 # options: if caller didn't prebuild `options`, try explicit cols first\n",
    "    #                 if not variant_obj.get(\"options\"):\n",
    "    #                     pairs = []\n",
    "    #                     for prefix in (\"variant_option\", \"option\"):\n",
    "    #                         for i in range(1, 6):\n",
    "    #                             n = (variant_obj.get(f\"{prefix}_{i}_name\") or \"\").strip()\n",
    "    #                             v = (variant_obj.get(f\"{prefix}_{i}_value\") or \"\").strip()\n",
    "    #                             if n and v:\n",
    "    #                                 pairs.append({\"name\": n, \"value\": v})\n",
    "    #                     if pairs:\n",
    "    #                         variant_obj[\"options\"] = pairs\n",
    "    \n",
    "    #                 variant_id = self._upsert_variant(parent_id, variant_obj)\n",
    "    \n",
    "    #                 # extract final options (only real Size/Color or explicit ones)\n",
    "    #                 options = self._extract_options_from_variant(parent_sku, variant_sku, variant_obj)\n",
    "    #                 for opt in options:\n",
    "    #                     po_id, pov_id = self._ensure_option_and_value(parent_id, opt[\"name\"], opt[\"value\"])\n",
    "    #                     self._link_variant_option_value(variant_id, po_id, pov_id)\n",
    "    \n",
    "    #                 # images for variant (write at offset 101..110)\n",
    "    #                 self._handle_variant_assets_inline(variant_id, variant_sku, variant_obj)\n",
    "    \n",
    "    #                 self.cursor.execute(f\"RELEASE SAVEPOINT {sp_name}\")\n",
    "    #                 success_count += 1\n",
    "    #                 current_batch += 1\n",
    "    #                 if current_batch >= batch_size:\n",
    "    #                     self.conn.commit(); current_batch = 0; self.cursor.execute(\"BEGIN\")\n",
    "    \n",
    "    #             except Exception as item_error:\n",
    "    #                 self.cursor.execute(f\"ROLLBACK TO SAVEPOINT {sp_name}\")\n",
    "    #                 self.cursor.execute(f\"RELEASE SAVEPOINT {sp_name}\")\n",
    "    #                 failure_count += 1\n",
    "    #                 failed_items.append({'sku': request_obj.get(\"sku\"), 'error': str(item_error)})\n",
    "    #                 print(f\"ERROR parent+variant for {request_obj.get('sku')}: {item_error}\")\n",
    "    \n",
    "    #         if current_batch:\n",
    "    #             self.conn.commit()\n",
    "    \n",
    "    #     except Exception as e:\n",
    "    #         self.conn.rollback()\n",
    "    #         print(f\"ERROR: _handle_parent_and_variants batch failed: {e}\")\n",
    "    #         print(traceback.format_exc())\n",
    "    \n",
    "    #     return totalcount, success_count, failure_count, failed_items\n",
    "\n",
    "    # ======================= OPTION PIPELINE (ADD/REPLACE) =======================\n",
    "\n",
    "    import re\n",
    "\n",
    "    def _normalize_option_input_type(self, s: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Canonicalize UI type names \u2192 one of: 'Swatch', 'Rectangle', 'Dropdown', 'Radio'.\n",
    "        Accepts common synonyms.\n",
    "        \"\"\"\n",
    "        if not s:\n",
    "            return \"Dropdown\"\n",
    "        t = str(s).strip().lower()\n",
    "    \n",
    "        if t in {\"swatch\", \"colour-swatch\", \"color-swatch\"}:\n",
    "            return \"Swatch\"\n",
    "        if t in {\"rectangle\", \"rect\", \"pill\", \"tag\", \"tile\", \"chip\"}:\n",
    "            return \"Rectangle\"\n",
    "        if t in {\"dropdown\", \"select\", \"menu\"}:\n",
    "            return \"Dropdown\"\n",
    "        if t in {\"radio\", \"boolean\", \"yes/no\", \"yesno\"}:\n",
    "            return \"Radio\"\n",
    "        return \"Dropdown\"\n",
    "    \n",
    "    def _infer_option_input_type(self, option_name: str, option_value: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Heuristics to pick an input_type when none is provided.\n",
    "        Returns one of: 'Swatch', 'Rectangle', 'Dropdown', 'Radio'.\n",
    "        \"\"\"\n",
    "        n = (option_name or \"\").strip().lower()\n",
    "        v = (option_value or \"\").strip().lower() if option_value else \"\"\n",
    "    \n",
    "        common_colors = {\n",
    "            \"black\",\"white\",\"red\",\"blue\",\"navy\",\"green\",\"yellow\",\"orange\",\"purple\",\"pink\",\"grey\",\"gray\",\n",
    "            \"brown\",\"beige\",\"silver\",\"gold\",\"maroon\",\"teal\",\"turquoise\",\"cyan\",\"magenta\",\"olive\",\"indigo\",\n",
    "            \"charcoal\",\"ivory\",\"cream\",\"khaki\",\"tan\"\n",
    "        }\n",
    "    \n",
    "        color_keywords    = (\"color\",\"colour\",\"shade\",\"tone\",\"hue\")\n",
    "        pattern_keywords  = (\"pattern\",\"finish\",\"texture\",\"print\",\"graphic\")\n",
    "        size_keywords     = (\n",
    "            \"size\",\"casual size\",\"casual sizes\",\"apparel size\",\"shoe size\",\"footwear size\",\"waist\",\"chest\",\"length\",\n",
    "            \"width\",\"height\",\"inseam\",\"neck\",\"sleeve\",\"hip\",\"bust\"\n",
    "        )\n",
    "        material_keywords = (\"material\",\"fabric\",\"composition\",\"fibre\",\"fiber\",\"shell\",\"lining\")\n",
    "        rectangle_keywords= (\"fit\",\"style\",\"cut\",\"theme\",\"collection\",\"grade\",\"rating\",\"flavor\",\"flavour\",\"scent\",\"pack\",\"pack size\",\"model\",\"type\")\n",
    "    \n",
    "        if any(k in n for k in color_keywords):\n",
    "            return \"Swatch\"\n",
    "        if any(k in n for k in pattern_keywords):\n",
    "            if v in common_colors or (v.startswith(\"#\") and re.fullmatch(r\"#?[0-9a-f]{3,8}\", v)):\n",
    "                return \"Swatch\"\n",
    "            return \"Rectangle\"\n",
    "        if any(k in n for k in size_keywords):\n",
    "            return \"Dropdown\"\n",
    "        if any(k in n for k in material_keywords):\n",
    "            return \"Dropdown\"\n",
    "        if any(k in n for k in rectangle_keywords):\n",
    "            return \"Rectangle\"\n",
    "    \n",
    "        if v in {\"yes\",\"no\",\"true\",\"false\"}:\n",
    "            return \"Radio\"\n",
    "        if v.startswith(\"#\") and re.fullmatch(r\"#?[0-9a-f]{3,8}\", v):\n",
    "            return \"Swatch\"\n",
    "        if v in common_colors:\n",
    "            return \"Swatch\"\n",
    "        return \"Dropdown\"\n",
    "\n",
    "    \n",
    "    def _ensure_option_and_value(\n",
    "        self,\n",
    "        parent_product_id: int,\n",
    "        option_name: str,\n",
    "        option_value: str,\n",
    "        input_type: str  = None,\n",
    "        usage_type: str = \"default\",\n",
    "    ) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Upsert product_option/product_option_value for ANY option name (including Custom like 'Casual Sizes').\n",
    "        - Preserves the display name as provided (title-casing is NOT forced).\n",
    "        - Uses explicit input_type if provided; otherwise infers; then NORMALIZES to canonical set.\n",
    "        - usage_type kept as provided (default 'default').\n",
    "        \"\"\"\n",
    "        name_disp = (option_name or \"\").strip()\n",
    "        value     = (option_value or \"\").strip()\n",
    "        if not name_disp or not value:\n",
    "            return (None, None)\n",
    "    \n",
    "        # infer when missing\n",
    "        if not (input_type and input_type.strip()):\n",
    "            input_type = self._infer_option_input_type(name_disp, value)\n",
    "    \n",
    "        # normalize whatever we have (explicit or inferred)\n",
    "        input_type = self._normalize_option_input_type(input_type)\n",
    "        # print(\"-----------\",input_type)\n",
    "    \n",
    "        # product_option (product_id, name)\n",
    "        self.cursor.execute(\"\"\"\n",
    "            SELECT product_option_id, input_type\n",
    "            FROM product_option\n",
    "            WHERE product_id=%s AND name=%s\n",
    "            LIMIT 1\n",
    "        \"\"\", (parent_product_id, name_disp))\n",
    "        row = self.cursor.fetchone()\n",
    "    \n",
    "        if row:\n",
    "            product_option_id, existing_input_type = row[0], (row[1] or \"\")\n",
    "            existing_input_type = self._normalize_option_input_type(existing_input_type)\n",
    "            if input_type != existing_input_type or not existing_input_type:\n",
    "                self.cursor.execute(\"\"\"\n",
    "                    UPDATE product_option\n",
    "                       SET input_type=%s, usage_type=%s, updated_at=CURRENT_TIMESTAMP, is_active=true\n",
    "                     WHERE product_option_id=%s\n",
    "                \"\"\", (input_type, usage_type, product_option_id))\n",
    "            else:\n",
    "                self.cursor.execute(\"\"\"\n",
    "                    UPDATE product_option\n",
    "                       SET usage_type=%s, updated_at=CURRENT_TIMESTAMP, is_active=true\n",
    "                     WHERE product_option_id=%s\n",
    "                \"\"\", (usage_type, product_option_id))\n",
    "        else:\n",
    "            self.cursor.execute(\"\"\"\n",
    "                INSERT INTO product_option\n",
    "                    (product_id, name, input_type, usage_type, is_active, sort_order, created_at, updated_at)\n",
    "                VALUES\n",
    "                    (%s,%s,%s,%s,true,0,CURRENT_TIMESTAMP,CURRENT_TIMESTAMP)\n",
    "                RETURNING product_option_id\n",
    "            \"\"\", (parent_product_id, name_disp, input_type, usage_type))\n",
    "            product_option_id = self.cursor.fetchone()[0]\n",
    "    \n",
    "        # product_option_value (single value per variant per option)\n",
    "        self.cursor.execute(\"\"\"\n",
    "            SELECT product_option_value_id\n",
    "            FROM product_option_value\n",
    "            WHERE product_option_id=%s AND value=%s\n",
    "            LIMIT 1\n",
    "        \"\"\", (product_option_id, value))\n",
    "        row = self.cursor.fetchone()\n",
    "    \n",
    "        if row:\n",
    "            product_value_id = row[0]\n",
    "            self.cursor.execute(\"\"\"\n",
    "                UPDATE product_option_value\n",
    "                   SET updated_at=CURRENT_TIMESTAMP, is_active=true\n",
    "                 WHERE product_option_value_id=%s\n",
    "            \"\"\", (product_value_id,))\n",
    "        else:\n",
    "            self.cursor.execute(\"\"\"\n",
    "                INSERT INTO product_option_value\n",
    "                    (product_option_id, value, sort_order, is_active, created_at, updated_at)\n",
    "                VALUES\n",
    "                    (%s,%s,0,true,CURRENT_TIMESTAMP,CURRENT_TIMESTAMP)\n",
    "                RETURNING product_option_value_id\n",
    "            \"\"\", (product_option_id, value))\n",
    "            product_value_id = self.cursor.fetchone()[0]\n",
    "    \n",
    "        return product_option_id, product_value_id\n",
    "    \n",
    "    \n",
    "    def _link_variant_option_value(\n",
    "        self,\n",
    "        variant_id: int,\n",
    "        product_option_id: int \n",
    "        = None,\n",
    "        product_value_id: int  = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ensure single row per (variant_id, product_option_id).\n",
    "        If exists but value differs \u2192 update to new product_value_id.\n",
    "        \"\"\"\n",
    "        if not (product_option_id and product_value_id):\n",
    "            return\n",
    "    \n",
    "        self.cursor.execute(\"\"\"\n",
    "            SELECT id, product_value_id\n",
    "            FROM product_variant_option_value\n",
    "            WHERE variant_id=%s AND product_option_id=%s AND option_scope='product'\n",
    "            LIMIT 1\n",
    "        \"\"\", (variant_id, product_option_id))\n",
    "        row = self.cursor.fetchone()\n",
    "    \n",
    "        if row:\n",
    "            row_id, existing_val = row\n",
    "            if existing_val != product_value_id:\n",
    "                self.cursor.execute(\"\"\"\n",
    "                    UPDATE product_variant_option_value\n",
    "                       SET product_value_id=%s\n",
    "                     WHERE id=%s\n",
    "                \"\"\", (product_value_id, row_id))\n",
    "        else:\n",
    "            self.cursor.execute(\"\"\"\n",
    "                INSERT INTO product_variant_option_value\n",
    "                    (variant_id, option_scope, product_option_id, product_value_id)\n",
    "                VALUES (%s, 'product', %s, %s)\n",
    "            \"\"\", (variant_id, product_option_id, product_value_id))\n",
    "    \n",
    "    \n",
    "    def _kv_from_sources(self, obj: dict, *keys: str):\n",
    "        \"\"\"\n",
    "        Return first non-empty from obj[k], obj['raw_product_data'][k], obj['raw_variant_data'][k].\n",
    "        \"\"\"\n",
    "        rpd = obj.get(\"raw_product_data\") or {}\n",
    "        rvd = obj.get(\"raw_variant_data\") or {}\n",
    "        for k in keys:\n",
    "            for src in (obj, rpd, rvd):\n",
    "                if isinstance(src, dict):\n",
    "                    v = src.get(k)\n",
    "                    if v not in (None, \"\", \"null\", \"NULL\"):\n",
    "                        return v\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def _collect_op_fields(self, variant_obj: dict) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Parse OP_* family from root, raw_product_data, raw_variant_data:\n",
    "          OP_<Name> = value\n",
    "          OP_<Name>_type = Swatch/Dropdown/Radio/Rectangle (synonyms allowed)\n",
    "          OP_<Name>_usage = usage_type (optional, default 'default')\n",
    "        Returns list of dicts: {name, value, input_type?, usage_type?}\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        seen = set()\n",
    "    \n",
    "        def scan(d):\n",
    "            if not isinstance(d, dict):\n",
    "                return\n",
    "            for k, v in d.items():\n",
    "                if not isinstance(k, str):\n",
    "                    continue\n",
    "                if not k.lower().startswith(\"op_\"):\n",
    "                    continue\n",
    "                if v in (None, \"\", \"null\", \"NULL\"):\n",
    "                    continue\n",
    "    \n",
    "                # Base name & suffix (_type / _usage)\n",
    "                # e.g., OP_Casual Sizes, OP_Casual Sizes_type, OP_Casual Sizes_usage\n",
    "                if k.lower().endswith(\"_type\") or k.lower().endswith(\"_usage\"):\n",
    "                    continue\n",
    "    \n",
    "                base = k[3:]  # remove \"OP_\"\n",
    "                base = base.replace(\"_\", \" \").strip()  # tolerate underscores\n",
    "                name_disp = \" \".join(base.split())     # collapse spaces (keep user-given words)\n",
    "                key_base = name_disp.lower()\n",
    "    \n",
    "                # fetch optional type & usage from the same dict scope (or from other sources later)\n",
    "                t_key = f\"OP_{base}_type\"\n",
    "                u_key = f\"OP_{base}_usage\"\n",
    "                input_type = d.get(t_key) or d.get(t_key.lower()) or None\n",
    "                usage_type = d.get(u_key) or d.get(u_key.lower()) or \"default\"\n",
    "    \n",
    "                # Value (string) \u2013 assume one value for this variant\n",
    "                val = str(v).strip()\n",
    "                if not val:\n",
    "                    continue\n",
    "    \n",
    "                if key_base in seen:\n",
    "                    # don't duplicate same option name from multiple sources\n",
    "                    continue\n",
    "    \n",
    "                pairs.append({\"name\": name_disp, \"value\": val, \"input_type\": input_type, \"usage_type\": usage_type})\n",
    "                seen.add(key_base)\n",
    "    \n",
    "        scan(variant_obj)\n",
    "        scan(variant_obj.get(\"raw_product_data\") or {})\n",
    "        scan(variant_obj.get(\"raw_variant_data\") or {})\n",
    "    \n",
    "        return pairs\n",
    "    \n",
    "    \n",
    "    def _extract_options_from_variant(self, parent_sku: str, variant_sku: str, variant_obj: dict) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Return a list of {name, value, input_type} using ONLY explicit fields present in payload.\n",
    "        Sources (combined; first definition of a name wins):\n",
    "          1) 'options': [{name, value, input_type?}, ...]\n",
    "          2) OP_* family: OP_<Name> (=value), OP_<Name>_type, OP_<Name>_usage\n",
    "          3) option_name + option_value (comma-separated supported) [in root or raw_*]\n",
    "          4) numbered 'variant_option_i_*' / 'option_i_*'\n",
    "          5) raw_product_data/raw_variant_data 'option_name' + 'option_value'\n",
    "        \u274c No inference from SKU text.\n",
    "        \"\"\"\n",
    "        out: list[dict] = []\n",
    "        seen_names = set()\n",
    "    \n",
    "        def add_pair(name: str, value: str, input_type: str  = None):\n",
    "            n = (name or \"\").strip()\n",
    "            v = (value or \"\").strip()\n",
    "            if not n or not v:\n",
    "                return\n",
    "            key = n.lower()\n",
    "            if key in seen_names:\n",
    "                return\n",
    "            # type: explicit \u2192 normalize; else infer \u2192 normalize\n",
    "            if input_type and str(input_type).strip():\n",
    "                it = self._normalize_option_input_type(input_type)\n",
    "            else:\n",
    "                it = self._infer_option_input_type(n, v)\n",
    "            out.append({\"name\": n, \"value\": v, \"input_type\": it})\n",
    "            seen_names.add(key)\n",
    "    \n",
    "        # 1) explicit options list\n",
    "        if isinstance(variant_obj.get(\"options\"), list):\n",
    "            for p in variant_obj[\"options\"]:\n",
    "                n = (p.get(\"name\") or \"\").strip()\n",
    "                v = (p.get(\"value\") or \"\").strip()\n",
    "                t = p.get(\"input_type\")\n",
    "                if n and v:\n",
    "                    add_pair(n, v, t)\n",
    "    \n",
    "        # 2) OP_* family\n",
    "        for p in self._collect_op_fields(variant_obj):\n",
    "            add_pair(p[\"name\"], p[\"value\"], p.get(\"input_type\"))\n",
    "    \n",
    "        # 3) option_name + option_value on root\n",
    "        on = self._kv_from_sources(variant_obj, \"option_name\")\n",
    "        ov = self._kv_from_sources(variant_obj, \"option_value\")\n",
    "        if on and ov:\n",
    "            names  = [s.strip() for s in str(on).split(\",\") if s.strip()]\n",
    "            values = [s.strip() for s in str(ov).split(\",\") if s.strip()]\n",
    "            if names and len(names) == len(values):\n",
    "                for n, v in zip(names, values):\n",
    "                    add_pair(n, v, None)\n",
    "    \n",
    "        # 4) numbered fields (variant_option_i_* OR option_i_*)\n",
    "        numbered = []\n",
    "        for prefix in (\"variant_option\", \"option\"):\n",
    "            for i in range(1, 6):\n",
    "                n = (variant_obj.get(f\"{prefix}_{i}_name\") or \"\").strip()\n",
    "                v = (variant_obj.get(f\"{prefix}_{i}_value\") or \"\").strip()\n",
    "                t = (variant_obj.get(f\"{prefix}_{i}_type\")  or \"\").strip()\n",
    "                if n and v:\n",
    "                    numbered.append((n, v, t if t else None))\n",
    "        for n, v, t in numbered:\n",
    "            add_pair(n, v, t)\n",
    "    \n",
    "        # 5) raw_* option_name + option_value\n",
    "        for src in (\"raw_product_data\", \"raw_variant_data\"):\n",
    "            block = variant_obj.get(src) or {}\n",
    "            if isinstance(block, dict):\n",
    "                on2 = block.get(\"option_name\")\n",
    "                ov2 = block.get(\"option_value\")\n",
    "                if on2 and ov2:\n",
    "                    names  = [s.strip() for s in str(on2).split(\",\") if s.strip()]\n",
    "                    values = [s.strip() for s in str(ov2).split(\",\") if s.strip()]\n",
    "                    if names and len(names) == len(values):\n",
    "                        for n, v in zip(names, values):\n",
    "                            add_pair(n, v, None)\n",
    "    \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _handle_parent_and_variants(self, request_objects, batch_size):\n",
    "        IMAGE_COLUMNS = [\"image_url\"] + [f\"image_url_{i}\" for i in range(1, 10)]\n",
    "    \n",
    "        success_count = failure_count = current_batch = 0\n",
    "        failed_items, totalcount = [], len(request_objects)\n",
    "    \n",
    "        try:\n",
    "            self.cursor.execute(\"BEGIN\")\n",
    "            for idx, request_obj in enumerate(request_objects, 1):\n",
    "                sp_name = f\"pv_sp_{idx}\"\n",
    "                self.cursor.execute(f\"SAVEPOINT {sp_name}\")\n",
    "    \n",
    "                try:\n",
    "                    sku_field = request_obj.get(\"sku\")\n",
    "                    explicit_parent = (request_obj.get(\"parent_sku\") or \"\").strip()\n",
    "                    if not sku_field:\n",
    "                        raise ValueError(\"SKU is required\")\n",
    "    \n",
    "                    sku_str = sku_field.strip()\n",
    "    \n",
    "                    # --- Decide parent/variant ---\n",
    "                    if \":\" in sku_str:\n",
    "                        parent_sku, variant_sku = [x.strip() for x in sku_str.split(\":\", 1)]\n",
    "                        is_variant = True\n",
    "                    else:\n",
    "                        if explicit_parent and explicit_parent != sku_str:\n",
    "                            parent_sku, variant_sku = explicit_parent, sku_str\n",
    "                            is_variant = True\n",
    "                        else:\n",
    "                            parent_sku, variant_sku = sku_str, None\n",
    "                            is_variant = False\n",
    "    \n",
    "                    # --- Always upsert parent (with proper payload) ---\n",
    "                    if is_variant:\n",
    "                        # parent_payload = self._filter_parent_updates(request_obj)\n",
    "                        parent_payload[\"sku\"] = parent_sku  # ensure the payload targets the real parent\n",
    "                    else:\n",
    "                        parent_payload = request_obj  # parent row \u2192 full payload is allowed\n",
    "    \n",
    "                    parent_id = self._upsert_parent(parent_sku, parent_payload)\n",
    "    \n",
    "                    # --- Parent-only row: we're done (images handled inside _upsert_parent) ---\n",
    "                    if not is_variant:\n",
    "                        self.cursor.execute(f\"RELEASE SAVEPOINT {sp_name}\")\n",
    "                        success_count += 1\n",
    "                        current_batch += 1\n",
    "                        if current_batch >= batch_size:\n",
    "                            self.conn.commit(); current_batch = 0; self.cursor.execute(\"BEGIN\")\n",
    "                        continue\n",
    "    \n",
    "                    # --- Variant flow below ---\n",
    "                    variant_obj = dict(request_obj)\n",
    "                    variant_obj[\"sku\"] = variant_sku\n",
    "                    variant_obj.setdefault(\"variant_name\", variant_sku)\n",
    "                  \n",
    "                    # inventory fallback\n",
    "                    if variant_obj.get(\"inventory_quantity\") in (None, \"\", \"null\", \"NULL\"):\n",
    "                        if \"quantity\" in variant_obj and variant_obj[\"quantity\"] not in (None, \"\", \"null\", \"NULL\"):\n",
    "                            variant_obj[\"inventory_quantity\"] = variant_obj[\"quantity\"]\n",
    "                        elif \"stock_quantity\" in variant_obj and variant_obj[\"stock_quantity\"] not in (None, \"\", \"null\", \"NULL\"):\n",
    "                            variant_obj[\"inventory_quantity\"] = variant_obj[\"stock_quantity\"]\n",
    "                    variant_obj[\"inventory_quantity\"] = self._to_int(variant_obj.get(\"inventory_quantity\"), default=0)\n",
    "    \n",
    "                    # pre-build options list from numbered/legacy fields if not provided\n",
    "                    if not variant_obj.get(\"options\"):\n",
    "                        pairs = []\n",
    "                        for prefix in (\"variant_option\", \"option\"):\n",
    "                            for i in range(1, 6):\n",
    "                                n = (variant_obj.get(f\"{prefix}_{i}_name\") or \"\").strip()\n",
    "                                v = (variant_obj.get(f\"{prefix}_{i}_value\") or \"\").strip()\n",
    "                                t = (variant_obj.get(f\"{prefix}_{i}_type\") or \"\").strip()\n",
    "                                if n and v:\n",
    "                                    item = {\"name\": n, \"value\": v}\n",
    "                                    if t:\n",
    "                                        item[\"input_type\"] = t\n",
    "                                    pairs.append(item)\n",
    "                        if pairs:\n",
    "                            variant_obj[\"options\"] = pairs\n",
    "    \n",
    "                    variant_id = self._upsert_variant(parent_id, variant_obj)\n",
    "    \n",
    "                    # extract & persist options for this variant\n",
    "                    options = self._extract_options_from_variant(parent_sku, variant_sku, variant_obj)\n",
    "                    for opt in options:\n",
    "                       \n",
    "                        po_id, pov_id = self._ensure_option_and_value(parent_id, opt[\"name\"], opt[\"value\"],opt.get(\"input_type\"))\n",
    "                        # print(\"dadadadaaa\",po_id, pov_id )\n",
    "                        self._link_variant_option_value(variant_id, po_id, pov_id)\n",
    "    \n",
    "                    # images for variant (write at offset 101..110 and (optionally) sync)\n",
    "                    self._handle_variant_assets_inline(variant_id, variant_sku, variant_obj)\n",
    "    \n",
    "                    self.cursor.execute(f\"RELEASE SAVEPOINT {sp_name}\")\n",
    "                    success_count += 1\n",
    "                    current_batch += 1\n",
    "                    if current_batch >= batch_size:\n",
    "                        self.conn.commit(); current_batch = 0; self.cursor.execute(\"BEGIN\")\n",
    "    \n",
    "                except Exception as item_error:\n",
    "                    self.cursor.execute(f\"ROLLBACK TO SAVEPOINT {sp_name}\")\n",
    "                    self.cursor.execute(f\"RELEASE SAVEPOINT {sp_name}\")\n",
    "                    failure_count += 1\n",
    "                    failed_items.append({'sku': request_obj.get(\"sku\"), 'error': str(item_error)})\n",
    "                    print(f\"ERROR parent+variant for {request_obj.get('sku')}: {item_error}\")\n",
    "    \n",
    "            if current_batch:\n",
    "                self.conn.commit()\n",
    "    \n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            print(f\"ERROR: _handle_parent_and_variants batch failed: {e}\")\n",
    "            print(traceback.format_exc())\n",
    "    \n",
    "        return totalcount, success_count, failure_count, failed_items\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _handle_variants(self, request_objects, batch_size=100):\n",
    "        \"\"\"Insert/Update variants, their images, options, and option values.\"\"\"\n",
    "        IMAGE_COLUMNS = [\n",
    "            \"image_url\",\"image_url_1\",\"image_url_2\",\"image_url_3\",\"image_url_4\",\n",
    "            \"image_url_5\",\"image_url_6\",\"image_url_7\",\"image_url_8\",\"image_url_9\"\n",
    "        ]\n",
    "\n",
    "        success_count = failure_count = current_batch = 0\n",
    "        failed_items = []\n",
    "\n",
    "        try:\n",
    "            self.cursor.execute(\"BEGIN\")\n",
    "            for idx, request_obj in enumerate(request_objects, 1):\n",
    "                sp_name = f\"variant_sp_{idx}\"\n",
    "                self.cursor.execute(f\"SAVEPOINT {sp_name}\")\n",
    "                try:\n",
    "                    sku = request_obj.get(\"sku\")\n",
    "                    parent_sku = request_obj.get(\"parent_sku\")\n",
    "                    if not sku or not parent_sku:\n",
    "                        raise ValueError(\"Variant missing SKU or parent_sku\")\n",
    "\n",
    "                    parent_id = self.get_or_create_product(parent_sku)\n",
    "                    if not parent_id:\n",
    "                        raise ValueError(f\"Parent {parent_sku} not found\")\n",
    "\n",
    "                    # --- Insert/Update variant ---\n",
    "                    self.cursor.execute(\"\"\"\n",
    "                        INSERT INTO productvariants\n",
    "                        (product_id, saas_edge_id, sku, variant_name, price, inventory_quantity,\n",
    "                         barcode, image_url, raw_variant_data, created_at, updated_at)\n",
    "                        VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,CURRENT_TIMESTAMP,CURRENT_TIMESTAMP)\n",
    "                        ON CONFLICT (sku, saas_edge_id) DO UPDATE SET\n",
    "                            variant_name=EXCLUDED.variant_name,\n",
    "                            price=EXCLUDED.price,\n",
    "                            inventory_quantity=EXCLUDED.inventory_quantity,\n",
    "                            barcode=EXCLUDED.barcode,\n",
    "                            image_url=EXCLUDED.image_url,\n",
    "                            raw_variant_data=EXCLUDED.raw_variant_data,\n",
    "                            updated_at=CURRENT_TIMESTAMP\n",
    "                        RETURNING variant_id;\n",
    "                    \"\"\", (\n",
    "                        parent_id, self.saas_edge_id,\n",
    "                        sku, request_obj.get(\"variant_name\"),\n",
    "                        request_obj.get(\"price\"),\n",
    "                        request_obj.get(\"inventory_quantity\", 0),\n",
    "                        request_obj.get(\"barcode\"),\n",
    "                        request_obj.get(\"image_url\"),\n",
    "                        json.dumps(request_obj.get(\"raw_variant_data\", {}))\n",
    "                    ))\n",
    "                    variant_id = self.cursor.fetchone()[0]\n",
    "\n",
    "                    # --- Handle variant images ---\n",
    "                    for pos in range(10):\n",
    "                        col_name = \"image_url\" if pos == 0 else f\"image_url_{pos}\"\n",
    "                        source_url = request_obj.get(col_name) or (\n",
    "                            request_obj.get(\"images\")[pos] if \"images\" in request_obj and pos < len(request_obj[\"images\"]) else None\n",
    "                        )\n",
    "                        if not source_url:\n",
    "                            continue\n",
    "                        try:\n",
    "                            r = requests.get(source_url, stream=True, timeout=20)\n",
    "                            r.raise_for_status()\n",
    "                            file_data = BytesIO(r.content)\n",
    "                            file_name = f\"{sku}_{pos}.jpg\"\n",
    "                            subdir = f\"{self.saas_edge_id}/catalog_edge/dam/public/imported\"\n",
    "                            gcp_url = self.upload_asset_to_gcp_bucket(file_data, file_name, GCP_BUCKET_NAME, credentials, subdir)\n",
    "\n",
    "                            self.cursor.execute(\"\"\"\n",
    "                                INSERT INTO productassets_variants\n",
    "                                (variant_id, var_asset_pos, saas_edge_id, asset_url, created_at, updated_at, asset_type, channel_asset_pos, source_url)\n",
    "                                VALUES (%s,%s,%s,%s,CURRENT_TIMESTAMP,CURRENT_TIMESTAMP,'image',%s,%s)\n",
    "                                ON CONFLICT (variant_id,var_asset_pos,saas_edge_id) DO UPDATE SET\n",
    "                                    asset_url=EXCLUDED.asset_url, updated_at=CURRENT_TIMESTAMP;\n",
    "                            \"\"\", (variant_id, pos+1, self.saas_edge_id, gcp_url, pos+1, source_url))\n",
    "\n",
    "                        except Exception as e:\n",
    "                            failed_items.append({\"sku\": sku, \"slot\": pos+1, \"error\": str(e)})\n",
    "                            failure_count += 1\n",
    "\n",
    "                    # --- Sync flat image columns back into productvariants ---\n",
    "                    self.cursor.execute(\"\"\"\n",
    "                        SELECT asset_url, channel_asset_pos\n",
    "                        FROM productassets_variants\n",
    "                        WHERE variant_id = %s AND saas_edge_id = %s\n",
    "                        ORDER BY channel_asset_pos\n",
    "                    \"\"\", (variant_id, self.saas_edge_id))\n",
    "                    rows = self.cursor.fetchall()\n",
    "                    col_updates = {\n",
    "                        IMAGE_COLUMNS[pos-1]: url\n",
    "                        for url, pos in rows\n",
    "                        if 1 <= pos <= len(IMAGE_COLUMNS)\n",
    "                    }\n",
    "                    if col_updates:\n",
    "                        set_clause = \", \".join(f\"{col} = %s\" for col in col_updates)\n",
    "                        self.cursor.execute(\n",
    "                            f\"UPDATE productvariants SET {set_clause} WHERE variant_id = %s AND saas_edge_id = %s;\",\n",
    "                            (*col_updates.values(), variant_id, self.saas_edge_id)\n",
    "                        )\n",
    "\n",
    "                    # --- Handle variant options ---\n",
    "                    for opt in request_obj.get(\"options\", []):\n",
    "                        self.cursor.execute(\"\"\"\n",
    "                            INSERT INTO product_option (product_id, name, input_type, usage_type, is_active, sort_order, created_at, updated_at)\n",
    "                            VALUES (%s,%s,'Dropdown','default',true,0,CURRENT_TIMESTAMP,CURRENT_TIMESTAMP)\n",
    "                            ON CONFLICT (product_id,name) DO UPDATE SET updated_at=CURRENT_TIMESTAMP\n",
    "                            RETURNING product_option_id;\n",
    "                        \"\"\", (parent_id, opt[\"name\"]))\n",
    "                        product_option_id = self.cursor.fetchone()[0]\n",
    "\n",
    "                        self.cursor.execute(\"\"\"\n",
    "                            INSERT INTO product_option_value (product_option_id, value, sort_order, is_active, created_at, updated_at)\n",
    "                            VALUES (%s,%s,0,true,CURRENT_TIMESTAMP,CURRENT_TIMESTAMP)\n",
    "                            ON CONFLICT (product_option_id,value) DO UPDATE SET updated_at=CURRENT_TIMESTAMP\n",
    "                            RETURNING product_option_value_id;\n",
    "                        \"\"\", (product_option_id, opt[\"value\"]))\n",
    "                        product_value_id = self.cursor.fetchone()[0]\n",
    "\n",
    "                        self.cursor.execute(\"\"\"\n",
    "                            INSERT INTO product_variant_option_value (variant_id, option_scope, product_option_id, product_value_id)\n",
    "                            VALUES (%s,'product',%s,%s)\n",
    "                            ON CONFLICT (variant_id,product_option_id,product_value_id) DO NOTHING;\n",
    "                        \"\"\", (variant_id, product_option_id, product_value_id))\n",
    "\n",
    "                    self.cursor.execute(f\"RELEASE SAVEPOINT {sp_name}\")\n",
    "                    success_count += 1\n",
    "                    current_batch += 1\n",
    "\n",
    "                    if current_batch >= batch_size:\n",
    "                        self.conn.commit()\n",
    "                        print(f\"INFO: Committed {current_batch} variants\")\n",
    "                        current_batch = 0\n",
    "\n",
    "                except Exception as ve:\n",
    "                    self.cursor.execute(f\"ROLLBACK TO SAVEPOINT {sp_name}\")\n",
    "                    self.cursor.execute(f\"RELEASE SAVEPOINT {sp_name}\")\n",
    "                    failure_count += 1\n",
    "                    failed_items.append({\"sku\": request_obj.get(\"sku\"), \"error\": str(ve)})\n",
    "\n",
    "            if current_batch:\n",
    "                self.conn.commit()\n",
    "                print(f\"INFO: Final commit of {current_batch} variants\")\n",
    "\n",
    "            self.conn.commit()\n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            print(f\"ERROR: Variant batch failed: {e}\")\n",
    "\n",
    "        return success_count, failure_count, failed_items\n",
    "\n",
    "\n",
    "    def _assign_variant_image_to_parent(self):\n",
    "        \"\"\"Assign first variant image to parent if parent has no image\"\"\"\n",
    "        try:\n",
    "            self.cursor.execute(\"\"\"\n",
    "                SELECT DISTINCT p.product_id, p.sku\n",
    "                FROM products p\n",
    "                WHERE (p.image_url IS NULL OR p.image_url = '')\n",
    "                AND p.saas_edge_id = %s\n",
    "            \"\"\", (self.saas_edge_id,))\n",
    "            parents = self.cursor.fetchall()\n",
    "\n",
    "            for parent_id, parent_sku in parents:\n",
    "                self.cursor.execute(\"\"\"\n",
    "                    SELECT v.image_url\n",
    "                    FROM productvariants v\n",
    "                    WHERE v.product_id = %s AND v.saas_edge_id = %s\n",
    "                    AND v.image_url IS NOT NULL\n",
    "                    ORDER BY v.updated_at ASC\n",
    "                    LIMIT 1\n",
    "                \"\"\", (parent_id, self.saas_edge_id))\n",
    "                row = self.cursor.fetchone()\n",
    "                if row:\n",
    "                    first_img = row[0]\n",
    "                    self.cursor.execute(\"\"\"\n",
    "                        UPDATE products\n",
    "                        SET image_url = %s, updated_at = CURRENT_TIMESTAMP\n",
    "                        WHERE product_id = %s AND saas_edge_id = %s\n",
    "                    \"\"\", (first_img, parent_id, self.saas_edge_id))\n",
    "                    print(f\"INFO: Parent {parent_sku} inherited image {first_img}\")\n",
    "\n",
    "            self.conn.commit()\n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            print(f\"ERROR assigning parent image: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "    def _handle_product_assets(self, request_objects, batch_size):\n",
    "        \"\"\"\n",
    "        Robust import with SAVEPOINTs:\n",
    "          - BEGIN once\n",
    "          - per image slot: SAVEPOINT img_sp (rollback only that slot on failure)\n",
    "          - mirror to products.image_url* in its own SAVEPOINT (won't undo assets)\n",
    "          - NEVER commit inside a SKU; commit only after finishing a SKU (or after N SKUs)\n",
    "        \"\"\"\n",
    "        import time\n",
    "    \n",
    "        IMAGE_COLUMNS = [\n",
    "            \"image_url\", \"image_url_1\", \"image_url_2\", \"image_url_3\", \"image_url_4\",\n",
    "            \"image_url_5\", \"image_url_6\", \"image_url_7\", \"image_url_8\", \"image_url_9\"\n",
    "        ]\n",
    "    \n",
    "        success_count = failure_count = 0\n",
    "        current_batch = 0                 # count of images processed since last commit\n",
    "        skus_since_commit = 0             # count of SKUs since last commit (for chunking by SKU)\n",
    "        failed_items  = []\n",
    "    \n",
    "        try:\n",
    "            pa_cols = self._get_table_columns(\"productassets\")\n",
    "            self.cursor.execute(\"BEGIN\")  # explicit txn so SAVEPOINTs work\n",
    "    \n",
    "            for row_idx, asset_obj in enumerate(request_objects, 1):\n",
    "                sku = (asset_obj.get(\"sku\") or \"\").strip()\n",
    "                if not sku:\n",
    "                    print(f\" Row {row_idx}: missing SKU \u2013 skipped.\")\n",
    "                    continue\n",
    "    \n",
    "                try:\n",
    "                    product_id = self.get_or_create_product(sku)\n",
    "                    if not product_id:\n",
    "                        print(f\" Product not found for SKU {sku}\")\n",
    "                        failure_count += 1\n",
    "                        failed_items.append({\"sku\": sku, \"error\": \"Product not in DB\"})\n",
    "                        # move to next SKU\n",
    "                        skus_since_commit += 1\n",
    "                        # commit-by-SKU chunking\n",
    "                        if skus_since_commit >= max(1, batch_size // 10):\n",
    "                            try:\n",
    "                                self.conn.commit()\n",
    "                                self.cursor.execute(\"BEGIN\")\n",
    "                            except Exception as commit_err:\n",
    "                                print(f\"ERROR: Commit failed between SKUs: {commit_err}\")\n",
    "                                self.conn.rollback()\n",
    "                                self.cursor.execute(\"BEGIN\")\n",
    "                            skus_since_commit = 0\n",
    "                        continue\n",
    "    \n",
    "                    # --- per-image slots ---\n",
    "                    for slot_idx in range(10):\n",
    "                        col_name   = \"image_url\" if slot_idx == 0 else f\"image_url_{slot_idx}\"\n",
    "                        source_url = asset_obj.get(col_name)\n",
    "                        if not source_url:\n",
    "                            continue\n",
    "    \n",
    "                        # download/upload/meta with retries (network I/O, no SAVEPOINT)\n",
    "                        gcp_url = None\n",
    "                        saas_asset_id = None\n",
    "                        last_err = None\n",
    "                        for attempt in range(1, 4):  # 3 tries\n",
    "                            try:\n",
    "                                file_data, meta = self._download_image_with_meta(source_url)\n",
    "                                file_name = f\"{sku}.jpg\" if slot_idx == 0 else f\"{sku}_{slot_idx}.jpg\"\n",
    "                                subdir    = f\"{self.saas_edge_id}/catalog_edge/dam/public/imported\"\n",
    "                                gcp_url   = self.upload_asset_to_gcp_bucket(\n",
    "                                    file_data, file_name, GCP_BUCKET_NAME, credentials, subdir\n",
    "                                ) or source_url\n",
    "                                saas_asset_id = self._upsert_asset_and_get_id(gcp_url, source_url, meta)\n",
    "                                break\n",
    "                            except Exception as e:\n",
    "                                last_err = e\n",
    "                                print(f\"[{sku} slot {slot_idx+1}] download/upload/meta attempt {attempt}/3 failed: {e}\")\n",
    "                                time.sleep(1.0 * attempt)\n",
    "                        if gcp_url is None:\n",
    "                            failure_count += 1\n",
    "                            failed_items.append({\"sku\": sku, \"slot\": slot_idx + 1, \"error\": f\"{last_err}\"})\n",
    "                            continue  # next slot\n",
    "    \n",
    "                        # DB changes for this image slot under its own SAVEPOINT\n",
    "                        img_sp = f\"img_sp_{row_idx}_{slot_idx}\"\n",
    "                        self.cursor.execute(f\"SAVEPOINT {img_sp}\")\n",
    "                        try:\n",
    "                            # de-dupe strictly within parent slots 1..10\n",
    "                            if \"saas_asset_id\" in pa_cols:\n",
    "                                self.cursor.execute(\"\"\"\n",
    "                                    DELETE FROM productassets\n",
    "                                    WHERE product_id=%s AND saas_edge_id=%s\n",
    "                                      AND (asset_url=%s OR saas_asset_id=%s)\n",
    "                                      AND channel_asset_pos BETWEEN 1 AND 10\n",
    "                                      AND channel_asset_pos <> %s\n",
    "                                \"\"\", (product_id, self.saas_edge_id, gcp_url, saas_asset_id, slot_idx + 1))\n",
    "                            else:\n",
    "                                self.cursor.execute(\"\"\"\n",
    "                                    DELETE FROM productassets\n",
    "                                    WHERE product_id=%s AND saas_edge_id=%s\n",
    "                                      AND asset_url=%s\n",
    "                                      AND channel_asset_pos BETWEEN 1 AND 10\n",
    "                                      AND channel_asset_pos <> %s\n",
    "                                \"\"\", (product_id, self.saas_edge_id, gcp_url, slot_idx + 1))\n",
    "    \n",
    "                            # upsert slot\n",
    "                            self.cursor.execute(\"\"\"\n",
    "                                SELECT asset_id\n",
    "                                FROM productassets\n",
    "                                WHERE product_id=%s AND saas_edge_id=%s AND channel_asset_pos=%s\n",
    "                                LIMIT 1\n",
    "                            \"\"\", (product_id, self.saas_edge_id, slot_idx + 1))\n",
    "                            existing = self.cursor.fetchone()\n",
    "    \n",
    "                            role = \"main\" if slot_idx == 0 else \"linked\"\n",
    "    \n",
    "                            if existing:\n",
    "                                set_bits = [\"asset_url=%s\",\"source_url=%s\",\"asset_type='image'\",\"is_default=%s\"]\n",
    "                                params   = [gcp_url, source_url, (slot_idx == 0)]\n",
    "                                if \"saas_asset_id\" in pa_cols:\n",
    "                                    set_bits.append(\"saas_asset_id=%s\"); params.append(saas_asset_id)\n",
    "                                if \"role\" in pa_cols:\n",
    "                                    set_bits.append(\"role=%s\"); params.append(role)\n",
    "                                if \"position\" in pa_cols:\n",
    "                                    set_bits.append(\"position=%s\"); params.append(slot_idx + 1)\n",
    "    \n",
    "                                self.cursor.execute(f\"\"\"\n",
    "                                    UPDATE productassets\n",
    "                                       SET {\", \".join(set_bits)}\n",
    "                                     WHERE asset_id=%s\n",
    "                                \"\"\", (*params, existing[0]))\n",
    "                                print(f\"Replaced slot {slot_idx+1} with new image for SKU {sku}\")\n",
    "                            else:\n",
    "                                cols = [\"product_id\",\"saas_edge_id\",\"asset_url\",\"channel_asset_pos\",\"source_url\",\"asset_type\",\"is_default\"]\n",
    "                                ph   =  [\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\"]\n",
    "                                vals = [product_id, self.saas_edge_id, gcp_url, slot_idx + 1, source_url, \"image\", (slot_idx == 0)]\n",
    "    \n",
    "                                if \"saas_asset_id\" in pa_cols:\n",
    "                                    cols.append(\"saas_asset_id\"); ph.append(\"%s\"); vals.append(saas_asset_id)\n",
    "                                if \"role\" in pa_cols:\n",
    "                                    cols.append(\"role\"); ph.append(\"%s\"); vals.append(role)\n",
    "                                if \"position\" in pa_cols:\n",
    "                                    cols.append(\"position\"); ph.append(\"%s\"); vals.append(slot_idx + 1)\n",
    "    \n",
    "                                cols.append(\"created_at\"); ph.append(\"CURRENT_TIMESTAMP\")\n",
    "    \n",
    "                                self.cursor.execute(f\"\"\"\n",
    "                                    INSERT INTO productassets ({\", \".join(cols)})\n",
    "                                    VALUES ({\", \".join(ph)})\n",
    "                                \"\"\", tuple(vals))\n",
    "                                print(f\"Inserted image in slot {slot_idx+1} for SKU {sku}\")\n",
    "    \n",
    "                            success_count += 1\n",
    "                            current_batch += 1\n",
    "                            self.cursor.execute(f\"RELEASE SAVEPOINT {img_sp}\")\n",
    "    \n",
    "                        except Exception as slot_db_err:\n",
    "                            # roll back ONLY this slot\u2019s DB work; continue other slots and SKUs\n",
    "                            self.cursor.execute(f\"ROLLBACK TO SAVEPOINT {img_sp}\")\n",
    "                            self.cursor.execute(f\"RELEASE SAVEPOINT {img_sp}\")\n",
    "                            failure_count += 1\n",
    "                            failed_items.append({\"sku\": sku, \"slot\": slot_idx + 1, \"error\": f\"{slot_db_err}\"})\n",
    "                            print(f\"[{sku} slot {slot_idx+1}] DB error, kept other slots: {slot_db_err}\")\n",
    "    \n",
    "                    # mirror productassets (1..10) \u2192 products.image_url* under its own SAVEPOINT\n",
    "                    mirror_sp = f\"mirror_sp_{row_idx}\"\n",
    "                    self.cursor.execute(f\"SAVEPOINT {mirror_sp}\")\n",
    "                    try:\n",
    "                        self.cursor.execute(\"\"\"\n",
    "                            SELECT asset_url, channel_asset_pos\n",
    "                            FROM productassets\n",
    "                            WHERE product_id = %s AND saas_edge_id = %s\n",
    "                              AND channel_asset_pos BETWEEN 1 AND 10\n",
    "                            ORDER BY channel_asset_pos\n",
    "                        \"\"\", (product_id, self.saas_edge_id))\n",
    "                        rows = self.cursor.fetchall()\n",
    "                        col_updates = {\n",
    "                            IMAGE_COLUMNS[pos-1]: url\n",
    "                            for (url, pos) in rows\n",
    "                            if 1 <= pos <= len(IMAGE_COLUMNS)\n",
    "                        }\n",
    "                        if col_updates:\n",
    "                            set_clause = \", \".join(f\"{col} = %s\" for col in col_updates)\n",
    "                            self.cursor.execute(\n",
    "                                f\"UPDATE products SET {set_clause} WHERE product_id = %s AND saas_edge_id = %s;\",\n",
    "                                (*col_updates.values(), product_id, self.saas_edge_id)\n",
    "                            )\n",
    "                            # print(f\"Synced product image columns for SKU {sku}\")\n",
    "                        self.cursor.execute(f\"RELEASE SAVEPOINT {mirror_sp}\")\n",
    "                    except Exception as mirror_err:\n",
    "                        self.cursor.execute(f\"ROLLBACK TO SAVEPOINT {mirror_sp}\")\n",
    "                        self.cursor.execute(f\"RELEASE SAVEPOINT {mirror_sp}\")\n",
    "                        print(f\"WARN: mirror columns skipped for {sku}: {mirror_err}\")\n",
    "    \n",
    "                except Exception as sku_err:\n",
    "                    # isolate any unexpected error for this SKU; continue with next SKU\n",
    "                    failure_count += 1\n",
    "                    failed_items.append({\"sku\": sku, \"error\": f\"{sku_err}\"})\n",
    "                    print(f\"[{sku}] SKU-level error: {sku_err}\")\n",
    "    \n",
    "                # ---- commit/chunk only AFTER finishing the SKU (so no savepoints are dangling) ----\n",
    "                skus_since_commit += 1\n",
    "                if current_batch >= batch_size or skus_since_commit >= max(1, batch_size // 10):\n",
    "                    try:\n",
    "                        self.conn.commit()\n",
    "                        self.cursor.execute(\"BEGIN\")\n",
    "                    except Exception as commit_err:\n",
    "                        print(f\"ERROR: Commit failed: {commit_err}\")\n",
    "                        self.conn.rollback()\n",
    "                        self.cursor.execute(\"BEGIN\")\n",
    "                    current_batch = 0\n",
    "                    skus_since_commit = 0\n",
    "    \n",
    "            # final commit for the last chunk\n",
    "            try:\n",
    "                self.conn.commit()\n",
    "            except Exception as final_commit_err:\n",
    "                print(f\"ERROR: Final commit failed: {final_commit_err}\")\n",
    "                self.conn.rollback()\n",
    "    \n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            print(f\"Error in _handle_product_assets: {e}\")\n",
    "    \n",
    "        return success_count, failure_count, failed_items\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _handle_product_attributes(self, request_objects, batch_size):\n",
    "        \"\"\"Handle product attributes table operations\"\"\"\n",
    "        success_count = failure_count = current_batch = 0\n",
    "        failed_items = []\n",
    "        \n",
    "        try:\n",
    "            for idx, attr_obj in enumerate(request_objects, 1):\n",
    "                try:\n",
    "                    sku = attr_obj.get('sku')\n",
    "                    if not sku:\n",
    "                        raise ValueError(\"SKU is required for product attributes\")\n",
    "                        \n",
    "                    product_id = self.get_or_create_product(sku)\n",
    "                    if not product_id:\n",
    "                        raise ValueError(f\"Product with SKU {sku} not found and create_new_product is False\")\n",
    "\n",
    "                    attribute_key = attr_obj.get('attribute_key')\n",
    "                    attribute_value = attr_obj.get('attribute_value')\n",
    "                    \n",
    "                    # Check if attribute exists\n",
    "                    query = \"\"\"\n",
    "                    SELECT attribute_id FROM productattributes \n",
    "                    WHERE product_id = %s AND attribute_key = %s AND saas_edge_id = %s;\n",
    "                    \"\"\"\n",
    "                    self.cursor.execute(query, (product_id, attribute_key, self.saas_edge_id))\n",
    "                    existing_attr = self.cursor.fetchone()\n",
    "                    \n",
    "                    if existing_attr:\n",
    "                        # Update existing attribute\n",
    "                        query = \"\"\"\n",
    "                        UPDATE productattributes \n",
    "                        SET attribute_value = %s \n",
    "                        WHERE attribute_id = %s AND saas_edge_id = %s;\n",
    "                        \"\"\"\n",
    "                        self.cursor.execute(query, (attribute_value, existing_attr[0], self.saas_edge_id))\n",
    "                        print(f\"INFO: Updated attribute for product {sku}, key: {attribute_key}\")\n",
    "                    else:\n",
    "                        # Insert new attribute\n",
    "                        query = \"\"\"\n",
    "                        INSERT INTO productattributes \n",
    "                        (product_id, attribute_key, attribute_value, saas_edge_id)\n",
    "                        VALUES (%s, %s, %s, %s);\n",
    "                        \"\"\"\n",
    "                        self.cursor.execute(query, (product_id, attribute_key, \n",
    "                                                  attribute_value, self.saas_edge_id))\n",
    "                        print(f\"INFO: Created new attribute for product {sku}, key: {attribute_key}\")\n",
    "                    \n",
    "                    success_count += 1\n",
    "                    current_batch += 1\n",
    "                    \n",
    "                    if current_batch >= batch_size:\n",
    "                        self.conn.commit()\n",
    "                        print(f\"INFO: Committed batch of {current_batch} attributes\")\n",
    "                        current_batch = 0\n",
    "                        \n",
    "                except Exception as item_error:\n",
    "                    failure_count += 1\n",
    "                    failed_items.append({'sku': sku, 'error': str(item_error)})\n",
    "                    print(f\"ERROR processing attribute for SKU {sku}: {str(item_error)}\")\n",
    "                    \n",
    "            if current_batch > 0:\n",
    "                self.conn.commit()\n",
    "                print(f\"INFO: Committed final batch of {current_batch} attributes\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            print(f\"ERROR: Batch processing failed: {str(e)}\")\n",
    "            \n",
    "        return success_count, failure_count, failed_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Upload files to gcp\n",
    "def sanitize_path(path):\n",
    "    \"\"\"\n",
    "    Sanitize path by replacing forward slashes with underscores\n",
    "    \"\"\"\n",
    "    return path.replace('/', '_').replace(\".ipynb\",\"\").strip('')\n",
    "\n",
    "def get_output_filename(saas_edge_id, job_path,file_type):\n",
    "    \"\"\"\n",
    "    Generate standardized output filename with proper date formatting\n",
    "    \"\"\"\n",
    "    try:\n",
    "        current_date = datetime.now()\n",
    "        date_str = current_date.strftime('%Y-%m-%d')\n",
    "        timestamp_str = current_date.strftime('%H%M%S')\n",
    "        sanitized_job_path = sanitize_path(job_path)\n",
    "        \n",
    "        return f\"{saas_edge_id}/catalog-edge/job-reports/{sanitized_job_path}/{date_str}/import-failed-list-{timestamp_str}.{file_type}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating output filename: {str(e)}\")\n",
    "        # Fallback filename if something goes wrong\n",
    "        return f\"{saas_edge_id}/catalog-edge/job-reports/error_report.json\"\n",
    "\n",
    "def upload_to_gcp_bucket(file_data, file_name, bucket_name, base_path=\"\", credentials=None):\n",
    "    \"\"\"\n",
    "    Generic function to upload data to GCP bucket using existing credentials\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create credentials object from the service account info\n",
    "        gcp_credentials = service_account.Credentials.from_service_account_info(credentials)\n",
    "        storage_client = storage.Client(credentials=gcp_credentials)\n",
    "        \n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        \n",
    "        # Combine base path with file name\n",
    "        full_path = f\"{base_path.rstrip('/')}/{file_name}\" if base_path else file_name\n",
    "        blob = bucket.blob(full_path)\n",
    "        \n",
    "        # Add retry logic for upload\n",
    "        retry_count = 0\n",
    "        max_retries = 3\n",
    "        \n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                # Convert data to appropriate format for upload\n",
    "                if isinstance(file_data, (list, dict)):\n",
    "                    # Convert to JSON string for lists and dictionaries\n",
    "                    upload_data = json.dumps(file_data, indent=2)\n",
    "                    blob.upload_from_string(upload_data, content_type='application/json')\n",
    "                elif isinstance(file_data, str):\n",
    "                    blob.upload_from_string(file_data)\n",
    "                elif isinstance(file_data, bytes):\n",
    "                    blob.upload_from_string(file_data, content_type='application/octet-stream')\n",
    "                else:\n",
    "                    # For file-like objects\n",
    "                    blob.upload_from_file(file_data)\n",
    "                    \n",
    "                # Generate public URL\n",
    "                url = f\"https://storage.googleapis.com/{bucket_name}/{full_path}\"\n",
    "                print(f\"Successfully uploaded {file_name} to {url}\")\n",
    "                return True, url\n",
    "                \n",
    "            except Exception as upload_error:\n",
    "                retry_count += 1\n",
    "                if retry_count == max_retries:\n",
    "                    print(f\"Failed to upload {file_name} after {max_retries} attempts: {str(upload_error)}\")\n",
    "                    return False, \"\"\n",
    "                print(f\"Retry {retry_count} for {file_name}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to GCP bucket: {str(e)}\")\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return False, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_BUCKET_NAME = \"edge-assets\"\n",
    "credentials = {\n",
    "  \"type\": \"service_account\",\n",
    "  \"project_id\": \"saastify-base-wm\",\n",
    "  \"private_key_id\": \"d3ed03651cb920c119a78e2434459a52d3b9f541\",\n",
    "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEuwIBADANBgkqhkiG9w0BAQEFAASCBKUwggShAgEAAoIBAQC+M2925iyNZ34w\\n5UeOyN3LDo+OncMeHEjyBGnx7xtcBBv9/4rmrYn3cLu6iD31jrnhIkcjju6K2Fdz\\nOgNaXqQkoIT7OGqREh5ouFZAQQJWoy153BiyL8eUcd5CzBTMDTW2MpaOo4Y2jOii\\nzM6w94+Ajw7KoWHOi4o8KuItM9IDw394j2kCLeUFe0k2PQ/JNAJa9uKQgOd7bmAj\\nPhlDJuLRQPvpFOw93MLgPRi8JkSF88HeB/AiT/7Awnh7xAtaj36Kmycub2vIEufh\\nnKZ/U+0DEqhfLHSo9EixY+PCJSZsxjgwchDQQgyGB75zcuZVX/ArwzopFyGYO/uK\\nJCXxTNE7AgMBAAECgf8WqQoeoE2uiyX9rVNZL5U9G/7+fs1ASR5ntx7oNBSOYe7z\\n0/44fXRyhnvXPWQkXVzH9c2D7wN8h0nj8IV1vtDPjFBLne0UW5RD5bJg9V3R9J72\\nZcKLeCXPCcHxM19G8Ev16REG7XSQCzmsK7p0Wwo9xs18Vr3QXc+aW4GW4RWkXPG6\\nhoWOD6If0NtHVRZZqXEtSqShepptSz5ezcNGIgW5ksWeciMqziQssCFVbCM6E88+\\nADPHiB1VgtQvd3jqbh9KoUHEPSSZ9Lcp54k8bRx/9hpB9l9AtlQP8lh0oVYdVbuX\\n5+Eko6jH1tF/YELobR4TXnUqZg15z5g1y8vHkfUCgYEA4iRRYBRUEMczWlvOW/PC\\nvyO6JmoQVBHhU8J/6PfG2v+9c+sCf+e9dn4HTuPnjdI8QLy9VC1NR3/wXqCCf/Ea\\nIXPitUq3Gsezl01Zp2m7i3y8d4HZNoan9tcHMdIQ7bm17ZBbN/RE3Hw2dx/O+kFY\\nGvDhTTrw+GLfGPGnfiBLBK0CgYEA11BMuIQ8+HVD5aqux0jb7HAHsDYz24v87Z9L\\nwocBNy0J2lAtpiQ+SFs/0g5cVOyqm9MlThBshgUDGDUaczfqjZaz13rWc+qPBAjD\\nawUf9QfTO53UQIZP1dgNuybXBEi1PqMfd+yeUkjIDfF7+ntfwDr/BdH6XJ9tFMeR\\nlmUEAocCgYEApxW6Ykjiy/rCgJKwZ9Q1IdCd62AWbGdBmwdsRo88B/dI3WrYT/TD\\nUddQQwO0xF5/Uj2hjZ5jKN7olKH3idx0OB9NdDGeFFVU5geqpD1E6ozhG1N/UAAx\\n/flmQXM6OssqFjrAixkZ/+Zuv5lq7hB1roInlU5lWMCEogN6g4AMrYkCgYAI6ckT\\nRl4jxu71nfg4Rbrc8dJPqB7DcusYhySiu+X/+7xRrkoFe7CcXDKrJm8KEPYLF1WP\\nAr0LWz/Ci8g5htIN5HQzcmFYURh0iUxVrNOi2B0VdbYoqaa6aoQ/AB+cjMn7+tK9\\nqyzuqRanBR0lxF+1XHvcKNIdbXgdiRlsyWe+FwKBgHDn/KAFQ5yHEPUC5IUU8ktB\\no9W5eS33NGnsVZ6PKqncyGD2WUdpqdQeMSYQb/OSxULr49HVM71lrhiyqGHMsgb0\\nY/K62jVmlhnhDQ27F36rBojyidQPA+NxDr/8jgS78hktnydzh2j6cnMWGm3tJpmY\\nivU2NdIrfBP4gzRJXlzm\\n-----END PRIVATE KEY-----\\n\",\n",
    "  \"client_email\": \"wm-bulk-job-mgr@saastify-base-wm.iam.gserviceaccount.com\",\n",
    "  \"client_id\": \"104148745789000738235\",\n",
    "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/wm-bulk-job-mgr%40saastify-base-wm.iam.gserviceaccount.com\",\n",
    "  \"universe_domain\": \"googleapis.com\"\n",
    "}\n",
    "def process_data_import(saas_edge_id, saastify_table, request_objects, create_new_product=False):\n",
    "    failed_url = None\n",
    "    url_csv=None\n",
    "    try:\n",
    "        with ProductDataManager(saas_edge_id, create_new_product) as pdm:\n",
    "            totalcount, success_count, failure_count, failed_items = pdm.process_table_data(\n",
    "                saastify_table, \n",
    "                request_objects\n",
    "            )\n",
    "            \n",
    "            print(f\"Processing completed for {saastify_table}\")\n",
    "            print(f\"Successes: {success_count}\")\n",
    "            print(f\"Failures: {failure_count}\")\n",
    "            \n",
    "            if failed_items:\n",
    "                # print(\"Failed items:\", failed_items)\n",
    "                \n",
    "                try:\n",
    "                    output_filename = get_output_filename(\n",
    "                        saas_edge_id=saas_edge_id,\n",
    "                        job_path=job_path,\n",
    "                        file_type='json'\n",
    "                    )\n",
    "                    \n",
    "                    success, url = upload_to_gcp_bucket(\n",
    "                        file_data=failed_items,\n",
    "                        file_name=output_filename,\n",
    "                        bucket_name=GCP_BUCKET_NAME,\n",
    "                        credentials=credentials\n",
    "                    )\n",
    "\n",
    "                    try:\n",
    "                        df = pd.DataFrame(failed_items)\n",
    "                    except ValueError:\n",
    "                        # if failed_items is not list-of-dicts\n",
    "                        df = pd.json_normalize(failed_items)\n",
    "                \n",
    "                    csv_buffer = BytesIO()\n",
    "                    df.to_csv(csv_buffer, index=False)\n",
    "                    csv_buffer.seek(0)\n",
    "                \n",
    "                    csv_filename = get_output_filename(saas_edge_id, job_path, 'csv')\n",
    "                    success_csv, url_csv = upload_to_gcp_bucket(\n",
    "                        file_data=csv_buffer,\n",
    "                        file_name=csv_filename,\n",
    "                        bucket_name=GCP_BUCKET_NAME,\n",
    "                        credentials=credentials\n",
    "                    )\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    if success:\n",
    "                        print(f\"Failed items log uploaded to: {url}\")\n",
    "                        failed_url = url\n",
    "                    if success_csv:\n",
    "                        print(f\"Failed items log uploaded to: {url_csv}\")\n",
    "                        url_csv=url_csv\n",
    "                    else:\n",
    "                        print(\"ERROR: Failed to upload failed items log to GCP\")\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: Failed to process failed items: {str(e)}\")\n",
    "                    print(f\"ERROR: {traceback.format_exc()}\")\n",
    "            \n",
    "            # Update job details in database\n",
    "            updated_row = update_job_details(job_id,totalcount, success_count, failure_count,url_csv)\n",
    "            if not updated_row:\n",
    "                print(\"WARNING: Failed to update job details in database\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Data import failed: {str(e)}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_data_import(saas_edge_id, saastify_table, request_objects, create_new_product=create_new_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # def convert_to_us_central(date_str):\n",
    " #        \"\"\"Convert given date string to US Central Time (assuming it's in ISO 8601 or UTC).\"\"\"\n",
    " #        if not date_str:\n",
    " #            return None\n",
    " #        try:\n",
    " #            # Parse the string to a datetime object\n",
    " #            dt = datetime.fromisoformat(date_str.replace(\"Z\", \"+00:00\"))\n",
    " #            # Convert to US Central Time\n",
    " #            central_tz = pytz.timezone(\"US/Central\")\n",
    " #            dt_central = dt.astimezone(central_tz)\n",
    " #            # Return in ISO format without microseconds\n",
    " #            return dt_central.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    " #        except Exception as e:\n",
    " #            print(f\"Date conversion error for {date_str}: {e}\")\n",
    " #            return date_str  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_to_us_central(\"2025-09-04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}