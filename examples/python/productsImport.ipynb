{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<!-- <a href=\"https://colab.research.google.com/github/demystify-systems/edge-channel-suite/blob/main/demo/HelloSlackDocker2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gr9gc7arDWqF",
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# saas_edge_id = \"60ff8220-4d6f-4433-afd4-67952070e964\"\n",
    "# slack_webhook_url = \"os.getenv(\"SLACK_WEBHOOK_URL\", \"\")\"\n",
    "# job_id = \"a8ddf474-8fbe-4f0d-80a9-fc1fd80d103e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIb8HRHV6l_4"
   },
   "outputs": [],
   "source": [
    "!pip install pg8000 sqlalchemy pandas requests openpyxl zipfile36 psycopg2-binary --quiet\n",
    "!pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib google-cloud-storage --quiet\n",
    "\n",
    "import pg8000\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "from datetime import datetime,date\n",
    "import requests\n",
    "from uuid import UUID\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import mimetypes\n",
    "\n",
    "import pytz\n",
    "from jsonschema import validate, ValidationError, SchemaError\n",
    "import sys\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "import json\n",
    "from psycopg2 import sql as psql \n",
    "from typing import List, Dict,Tuple, Any, Optional,Set,Any\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from traceback import print_exc\n",
    "from openpyxl import load_workbook\n",
    "from io import BytesIO\n",
    "from slack_sdk import WebhookClient\n",
    "# global_message= f\"{customer_id} -- is executing the job from google cloud runner with DB access\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "\n",
    "# # Step 1: Build sdk path from current location\n",
    "# root_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "# sdk_path = os.path.join(root_path, 'sdk')\n",
    "\n",
    "# # Step 2: Clean sys.path and insert sdk_path\n",
    "# sys.path = [p for p in sys.path if 'sdk' not in p]\n",
    "# if sdk_path not in sys.path:\n",
    "#     sys.path.insert(0, sdk_path)\n",
    "\n",
    "# print(f\"SDK path set to: {sdk_path}\")\n",
    "\n",
    "# # Step 3: Import and force reload the module\n",
    "# import export_and_import_transformation\n",
    "# importlib.reload(export_and_import_transformation)\n",
    "\n",
    "# # Step 4: Now access your function\n",
    "# bulk_apply_pipe_rules = export_and_import_transformation.bulk_apply_pipe_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_user = os.getenv(\"DB_USER\", \"postgres\")                # Default to 'postgres' if env variable not set\n",
    "db_pass = os.getenv(\"DB_PASSWORD\", \"saasdbforwindmill2023\")\n",
    "db_name = os.getenv(\"DB_NAME\", \"catalog-edge-db\")\n",
    "db_host = \"/cloudsql/saastify-base-wm:us-central1:saastify-pgdb-us\"\n",
    "# db_host = \"127.0.0.1\"\n",
    "db_port = 5432"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDK Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this after the database configuration section (in[3])\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"\n",
    "    Creates and returns a database connection using global configuration.\n",
    "    Handles both Unix socket and TCP connections.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine if we're using Unix socket or TCP\n",
    "        if db_host.startswith('/cloudsql/'):\n",
    "            connection = pg8000.connect(\n",
    "                user=db_user,\n",
    "                password=db_pass,\n",
    "                database=db_name,\n",
    "                unix_sock=db_host + \"/.s.PGSQL.5432\"\n",
    "            )\n",
    "        else:\n",
    "            connection = pg8000.connect(\n",
    "                user=db_user,\n",
    "                password=db_pass,\n",
    "                database=db_name,\n",
    "                host=db_host,\n",
    "                port=db_port\n",
    "            )\n",
    "        return connection\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: Failed to establish database connection\")\n",
    "        print(f\"ERROR: {str(e)}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_url(url, file_name=\"\"):\n",
    "    get_response = requests.get(url, stream=True)\n",
    "    if file_name == \"\":\n",
    "        file_name = url.split(\"/\")[-1]\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        for chunk in get_response.iter_content(chunk_size=1024):\n",
    "            if chunk:  # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "    return file_name\n",
    "\n",
    "class FileParser(object):\n",
    "    def load(self, url):\n",
    "        self.url = url\n",
    "        self.file_name = download_url(url)\n",
    "        self.file_type = self.file_name.split(\".\")[-1]\n",
    "        print(\"The URL file type is : \", self.file_type)\n",
    "        method_name = \"parse_\" + self.file_type\n",
    "        method = getattr(self, method_name, lambda: \"Invalid\")\n",
    "        return method()\n",
    "\n",
    "    def infer_schema(self):\n",
    "        self.df.info()\n",
    "        self.columns = list(self.df.columns.values.tolist())\n",
    "        print(\"List of all columns are : \", self.columns)\n",
    "        print(\"##### Pandas inferred Schema\")\n",
    "        pandas_schema = self.df.columns.to_series().groupby(self.df.dtypes).groups\n",
    "        print(pandas_schema)\n",
    "\n",
    "    def parse_xlsx(self):\n",
    "        return self.parse_excel()\n",
    "\n",
    "    def parse_xlsm(self):\n",
    "        return self.parse_excel()\n",
    "\n",
    "    def parse_xls(self):\n",
    "        return self.parse_excel()\n",
    "\n",
    "    def parse_csv(self):\n",
    "        df = pd.read_csv(self.file_name, sep=\",\", header=0)\n",
    "        return df\n",
    "\n",
    "    def parse_zip(self):\n",
    "        with zipfile.ZipFile(self.file_name, \"r\") as zip_ref:\n",
    "            zip_ref.extractall()\n",
    "            extracted_files = zip_ref.namelist()\n",
    "            print(extracted_files)\n",
    "            return extracted_files\n",
    "\n",
    "    def parse_tsv(self):\n",
    "        df = pd.read_csv(self.file_name, sep=\"\\t\", header=0)\n",
    "        return df\n",
    "\n",
    "    def parse_json(self):\n",
    "        df = pd.read_json(self.file_name)\n",
    "        return df\n",
    "\n",
    "    def parse_txt(self):\n",
    "        df = pd.read_csv(self.file_name, sep=\" \")\n",
    "        return df\n",
    "\n",
    "    def parse_excel(self):\n",
    "        try:\n",
    "            xls = pd.ExcelFile(self.file_name)\n",
    "            # Read the first sheet by default\n",
    "            sheet_name = xls.sheet_names[0]\n",
    "            df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to parse Excel file: {str(e)}\")\n",
    "            print(f\"ERROR: {traceback.format_exc()}\")\n",
    "            return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_slack_message(message, webhook_url):\n",
    "    try:\n",
    "        print(\"INFO: Attempting to send a message to Slack...\")\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        payload = {\"text\": message}\n",
    "        response = requests.post(webhook_url, data=json.dumps(payload), headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"INFO: Slack message sent successfully.\")\n",
    "        else:\n",
    "            print(\"WARNING: Failed to send Slack message.\")\n",
    "            print(f\"WARNING: Status code: {response.status_code}\")\n",
    "            print(f\"WARNING: Response: {response.text}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: An error occurred while sending a Slack message.\")\n",
    "        print(f\"ERROR: {e}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database configuration - credentials should ideally be stored in environment variables for security\n",
    "\n",
    "\n",
    "def fetch_saas_edge_jobs_details(job_id, saas_edge_id):\n",
    "    try:\n",
    "        print(\"INFO: Attempting to connect to Cloud SQL instance...\")\n",
    "        \n",
    "        # Connect to the Cloud SQL instance\n",
    "        conn = get_db_connection()\n",
    "        print(\"INFO: Connected to Cloud SQL successfully.\")\n",
    "\n",
    "        # Define the query\n",
    "        query = \"\"\"\n",
    "        SELECT * FROM public.saas_edge_jobs\n",
    "        WHERE request_id = %s AND saas_edge_id = %s;\n",
    "        \"\"\"\n",
    "        print(f\"INFO: Executing query: {query} with job_id: {job_id} and saas_edge_id: {saas_edge_id}\")\n",
    "\n",
    "        # Execute the query\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query, (job_id, saas_edge_id))\n",
    "        result = cursor.fetchall()\n",
    "\n",
    "        if not result:\n",
    "            print(f\"WARNING: No results found for job_id: {job_id} and saas_edge_id: {saas_edge_id}\")\n",
    "        else:\n",
    "            print(f\"INFO: Query executed successfully. Number of records fetched: {len(result)}\")\n",
    "\n",
    "        # Process and convert datetime and UUID objects to strings for JSON serialization\n",
    "        row_data = [\n",
    "            {k: (v.isoformat() if isinstance(v, datetime) else str(v) if isinstance(v, UUID) else v) \n",
    "             for k, v in zip([col[0] for col in cursor.description], row)}\n",
    "            for row in result\n",
    "        ]\n",
    "        \n",
    "        print(\"INFO: Data processed for JSON serialization.\")\n",
    "\n",
    "        # Close the connection\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"INFO: Database connection closed.\")\n",
    "        \n",
    "        return row_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: An error occurred while connecting to Cloud SQL or executing the query.\")\n",
    "        print(f\"ERROR: {e}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")  # Full stack trace for debugging\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_channel_attr_template(saas_edge_id, template_id):\n",
    "    try:\n",
    "        print(\"INFO: Attempting to connect to Cloud SQL instance...\")\n",
    "        \n",
    "        conn = get_db_connection()\n",
    "        print(\"INFO: Connected to Cloud SQL successfully.\")\n",
    "\n",
    "        query = \"\"\"\n",
    "        SELECT * FROM public.saas_channel_attr_template\n",
    "        WHERE saas_edge_id = %s AND template_id = %s;\n",
    "        \"\"\"\n",
    "        print(f\"INFO: Executing query for saas_edge_id: {saas_edge_id} and template_id: {template_id}\")\n",
    "\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query, (saas_edge_id, template_id))\n",
    "        result = cursor.fetchall()\n",
    "\n",
    "        if not result:\n",
    "            print(f\"WARNING: No results found for saas_edge_id: {saas_edge_id} and template_id: {template_id}\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"INFO: Query executed successfully. Number of records fetched: {len(result)}\")\n",
    "\n",
    "        # Process results\n",
    "        row_data = [\n",
    "            {k: (v.isoformat() if isinstance(v, datetime) else str(v) if isinstance(v, UUID) else v) \n",
    "             for k, v in zip([col[0] for col in cursor.description], row)}\n",
    "            for row in result\n",
    "        ]\n",
    "        \n",
    "        print(\"INFO: Data processed for JSON serialization.\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"INFO: Database connection closed.\")\n",
    "        \n",
    "        return row_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: An error occurred while fetching channel attribute template data.\")\n",
    "        print(f\"ERROR: {e}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_saas_channel_templates(saas_edge_id, template_id):\n",
    "    try:\n",
    "        print(\"INFO: Attempting to connect to Cloud SQL instance...\")\n",
    "        \n",
    "        conn = get_db_connection()\n",
    "        print(\"INFO: Connected to Cloud SQL successfully.\")\n",
    "\n",
    "        query = \"\"\"\n",
    "        SELECT * FROM public.saas_channel_templates\n",
    "        WHERE saas_edge_id = %s AND template_id = %s;\n",
    "        \"\"\"\n",
    "        print(f\"INFO: Executing query for saas_edge_id: {saas_edge_id} and template_id: {template_id}\")\n",
    "\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query, (saas_edge_id, template_id))\n",
    "        result = cursor.fetchall()\n",
    "\n",
    "        if not result:\n",
    "            print(f\"WARNING: No results found for saas_edge_id: {saas_edge_id} and template_id: {template_id}\")\n",
    "            return None\n",
    "\n",
    "        print(f\"INFO: Query executed successfully. Number of records fetched: {len(result)}\")\n",
    "\n",
    "        # Process results and handle datetime/UUID serialization\n",
    "        row_data = [\n",
    "            {k: (v.isoformat() if isinstance(v, datetime) else str(v) if isinstance(v, UUID) else v) \n",
    "             for k, v in zip([col[0] for col in cursor.description], row)}\n",
    "            for row in result\n",
    "        ]\n",
    "        \n",
    "        print(\"INFO: Data processed for JSON serialization.\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"INFO: Database connection closed.\")\n",
    "        \n",
    "        return row_data[0] if row_data else None  # Return first row since template_id should be unique\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: An error occurred while fetching channel template data.\")\n",
    "        print(f\"ERROR: {e}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main execution code\n",
    "try:\n",
    "    job_id = job_id  # Replace with the actual job_id you want to query\n",
    "    saas_edge_id = saas_edge_id  # Replace with the actual saas_edge_id you want to query\n",
    "    slack_url = os.getenv(\"SLACK_WEBHOOK_URL\", slack_webhook_url)\n",
    "    \n",
    "    if not slack_url:\n",
    "        raise ValueError(\"Slack webhook URL not set in environment variables\")\n",
    "\n",
    "    # Fetch data from the database\n",
    "    print(f\"INFO: Fetching details for job_id: {job_id} and saas_edge_id: {saas_edge_id}\")\n",
    "    row_data = fetch_saas_edge_jobs_details(job_id, saas_edge_id)\n",
    "\n",
    "    if not row_data:\n",
    "        print(\"WARNING: No data retrieved or an error occurred during the fetch operation.\")\n",
    "\n",
    "except Exception as main_e:\n",
    "    print(\"ERROR: An unexpected error occurred in the main execution block.\")\n",
    "    print(f\"ERROR: {main_e}\")\n",
    "    print(f\"ERROR: {traceback.format_exc()}\")  # Full stack trace for debugging\n",
    "\n",
    "\n",
    "request_args = row_data[0].get(\"request_args\",{})\n",
    "\n",
    "print(request_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_path = request_args.get(\"job_path\", \"\")\n",
    "print(\"job_pathh\",job_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to get the database configuration\n",
    "db_config = {\n",
    "    # \"host\":\"localhost\",\n",
    "    \"host\": \"/cloudsql/saastify-base-wm:us-central1:saastify-pgdb-us\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"catalog-edge-db\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"saasdbforwindmill2023\"\n",
    "}\n",
    "\n",
    "\n",
    "# Function to construct the PostgreSQL connection string\n",
    "def create_connection_string() -> str:\n",
    "    try:\n",
    "        connection_string = (\n",
    "            f\"dbname={db_config['database']} \"\n",
    "            f\"user={db_config['user']} \"\n",
    "            f\"password={db_config['password']} \"\n",
    "            f\"host={db_config['host']} \"\n",
    "            f\"port={db_config['port']} \"\n",
    "        )\n",
    "        return connection_string\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing key in DB configuration: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating connection string: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Helper function to convert datetime and decimal objects to serializable format\n",
    "def custom_serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    elif isinstance(obj, Decimal):\n",
    "        return float(obj)\n",
    "    raise TypeError(f\"Type {obj.__class__.__name__} not serializable\")\n",
    "\n",
    "\n",
    "# Function to fetch template attributes\n",
    "def fetch_template_attributes(\n",
    "    saas_edge_id: str, template_id: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    connection_string = create_connection_string()\n",
    "    connection = None\n",
    "    try:\n",
    "        # Establish the connection\n",
    "        connection = psycopg2.connect(connection_string, cursor_factory=RealDictCursor)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Prepare the query to fetch attributes for the given saas_edge_id and template_id\n",
    "        query = \"\"\"\n",
    "            SELECT * FROM saas_channel_attr_template \n",
    "            WHERE saas_edge_id = %s AND template_id = %s \n",
    "            ORDER BY column_pos ASC NULLS LAST, column_name ASC\n",
    "        \"\"\"\n",
    "        print(\"Executing query:\", query)\n",
    "\n",
    "        # Execute the query with saas_edge_id and template_id as parameters\n",
    "        cursor.execute(query, (saas_edge_id, template_id))\n",
    "        response_data = cursor.fetchall()\n",
    "\n",
    "        # If no data is fetched, handle gracefully\n",
    "        if not response_data:\n",
    "            print(\n",
    "                f\"No template attributes found for saas_edge_id: {saas_edge_id} and template_id: {template_id}\"\n",
    "            )\n",
    "            return []\n",
    "\n",
    "        print(\"Number of rows fetched:\", len(response_data))\n",
    "        return response_data\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching template attributes: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "\n",
    "# Function to generate JSON Schema dynamically, including hierarchy support\n",
    "# Also generates the order array based on column_pos or alphabetic order\n",
    "\n",
    "\n",
    "def generate_json_schema(template_attrs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    try:\n",
    "        # Base schema\n",
    "        schema = {\n",
    "            \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": [],\n",
    "            \"order\": [],\n",
    "        }\n",
    "\n",
    "        # Helper to handle hierarchical properties\n",
    "        def add_property(\n",
    "            properties: Dict[str, Any],\n",
    "            attr: Dict[str, Any],\n",
    "            parent_property: Optional[str] = None,\n",
    "        ):\n",
    "            column_name = attr[\"column_name\"]\n",
    "            data_type = (attr.get(\"data_type\") or \"text\").lower()\n",
    "            data_type_format = attr.get(\"data_type_format\")\n",
    "\n",
    "            # Map data_type to JSON schema types\n",
    "            json_type_mapping = {\n",
    "                \"text\": \"string\",\n",
    "                \"number\": \"number\",\n",
    "                \"integer\": \"integer\",\n",
    "                \"boolean\": \"boolean\",\n",
    "                \"array\": \"array\",\n",
    "                \"object\": \"object\",\n",
    "                \"decimal\": \"number\",\n",
    "                \"timestamp\": \"string\",\n",
    "                \"url\": \"string\",\n",
    "                \"date\": \"string\",\n",
    "                \"datetime\": \"string\",\n",
    "                \"time\": \"string\",\n",
    "            }\n",
    "            field_type = json_type_mapping.get(data_type, \"string\")\n",
    "\n",
    "            # Define the property schema\n",
    "            property_schema = {\n",
    "                \"title\": attr.get(\"custom_title\", column_name),\n",
    "                \"type\": field_type,\n",
    "                \"description\": attr.get(\"description\", f\"Field for {column_name}\"),\n",
    "                \"default\": attr.get(\"default_value\"),\n",
    "                \"nullable\": attr.get(\"is_nullable\", True),\n",
    "            }\n",
    "\n",
    "            # Handle data_type_format if available\n",
    "            if data_type_format:\n",
    "                property_schema[\"format\"] = data_type_format\n",
    "\n",
    "            # Handle multi-valued fields (array)\n",
    "            if attr.get(\"accepts_multiple_values\", False):\n",
    "                property_schema = {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": field_type},\n",
    "                    \"description\": f\"Array of {data_type} values for {column_name}\",\n",
    "                }\n",
    "\n",
    "            # Add validation rules\n",
    "            validation_rules = attr.get(\"validation_rules\", {})\n",
    "            if validation_rules:\n",
    "                if \"minLength\" in validation_rules:\n",
    "                    property_schema[\"minLength\"] = validation_rules[\"minLength\"]\n",
    "                if \"maxLength\" in validation_rules:\n",
    "                    property_schema[\"maxLength\"] = validation_rules[\"maxLength\"]\n",
    "                if \"pattern\" in validation_rules:\n",
    "                    property_schema[\"pattern\"] = validation_rules[\"pattern\"]\n",
    "                if \"enum\" in validation_rules:\n",
    "                    property_schema[\"enum\"] = validation_rules[\"enum\"]\n",
    "\n",
    "            # Add to the appropriate place in the properties structure\n",
    "            if parent_property:\n",
    "                if \"properties\" not in properties[parent_property]:\n",
    "                    properties[parent_property][\"properties\"] = {}\n",
    "                properties[parent_property][\"properties\"][column_name] = property_schema\n",
    "            else:\n",
    "                properties[column_name] = property_schema\n",
    "\n",
    "            # Add to required list if mandatory\n",
    "            if attr.get(\"mandatory\", False):\n",
    "                if parent_property:\n",
    "                    if \"required\" not in properties[parent_property]:\n",
    "                        properties[parent_property][\"required\"] = []\n",
    "                    properties[parent_property][\"required\"].append(column_name)\n",
    "                else:\n",
    "                    schema[\"required\"].append(column_name)\n",
    "\n",
    "            # Add to order list\n",
    "            if not parent_property:\n",
    "                schema[\"order\"].append(column_name)\n",
    "\n",
    "        # Process attributes considering hierarchy\n",
    "        for attr in template_attrs:\n",
    "            parent_id = attr.get(\"parent_id\")\n",
    "            if parent_id:\n",
    "                # Find the parent property in existing attributes\n",
    "                parent_attr = next(\n",
    "                    (a for a in template_attrs if a[\"template_attr_id\"] == parent_id),\n",
    "                    None,\n",
    "                )\n",
    "                if parent_attr:\n",
    "                    add_property(schema[\"properties\"], attr, parent_attr[\"column_name\"])\n",
    "            else:\n",
    "                add_property(schema[\"properties\"], attr)\n",
    "\n",
    "        return schema\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating JSON schema: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Function to insert or update the generated schema in the database\n",
    "def insert_or_update_schema(\n",
    "    saas_edge_id: str, template_id: str, schema: Dict[str, Any]\n",
    ") -> None:\n",
    "    connection_string = create_connection_string()\n",
    "    connection = None\n",
    "    try:\n",
    "        # Establish the connection\n",
    "        connection = psycopg2.connect(connection_string)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Convert schema dictionary to JSON string\n",
    "        schema_json = json.dumps(schema)\n",
    "\n",
    "        # Insert or update the schema using ON CONFLICT clause\n",
    "        query = \"\"\"\n",
    "            INSERT INTO saas_template_schema (saas_edge_id, template_id, schema, created_at, updated_at)\n",
    "            VALUES (%s, %s, %s, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)\n",
    "            ON CONFLICT (saas_edge_id, template_id)\n",
    "            DO UPDATE SET\n",
    "                schema = EXCLUDED.schema,\n",
    "                updated_at = CURRENT_TIMESTAMP;\n",
    "        \"\"\"\n",
    "        print(\"Executing query:\", query)\n",
    "\n",
    "        # Execute the query with provided parameters\n",
    "        cursor.execute(query, (saas_edge_id, template_id, schema_json))\n",
    "        connection.commit()\n",
    "        print(\"Schema inserted or updated successfully.\")\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database error while inserting/updating schema: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error in insert_or_update_schema function: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "\n",
    "def get_existing_schema(saas_edge_id: str, template_id: str) -> Any:\n",
    "    \"\"\"\n",
    "    Fetch the existing schema for the given saas_edge_id and template_id from the saas_template_schema table.\n",
    "    \"\"\"\n",
    "    connection_string = create_connection_string()\n",
    "    connection = None\n",
    "    try:\n",
    "        # Establish database connection\n",
    "        connection = psycopg2.connect(connection_string, cursor_factory=RealDictCursor)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Query to check for existing schema\n",
    "        query = \"\"\"\n",
    "            SELECT schema FROM saas_template_schema\n",
    "            WHERE saas_edge_id = %s AND template_id = %s\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (saas_edge_id, template_id))\n",
    "        result = cursor.fetchone()\n",
    "\n",
    "        if result:\n",
    "            return result[\"schema\"]\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching schema: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "\n",
    "def genarate_template_schema(saas_edge_id: str, template_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main function to fetch or generate schema for saas_template_schema.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Check if schema already exists\n",
    "        existing_schema = get_existing_schema(saas_edge_id, template_id)\n",
    "        if existing_schema:\n",
    "            print(\"Schema found in the database.\")\n",
    "            return existing_schema\n",
    "\n",
    "        print(\"No schema found. Generating and saving schema...\")\n",
    "        # Step 2: Fetch template attributes and generate schema\n",
    "        template_attributes = fetch_template_attributes(saas_edge_id, template_id)\n",
    "        if not template_attributes:\n",
    "            raise ValueError(\n",
    "                \"No attributes found for the given saas_edge_id and template_id.\"\n",
    "            )\n",
    "\n",
    "        generated_schema = generate_json_schema(template_attributes)\n",
    "\n",
    "        # Step 3: Save the schema to the database\n",
    "        insert_or_update_schema(saas_edge_id, template_id, generated_schema)\n",
    "        return generated_schema\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n",
    "\n",
    "\n",
    "genarate_template_schema(saas_edge_id, request_args.get(\"template_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "try:\n",
    "    template_id = request_args.get(\"template_id\")\n",
    "    if not template_id:\n",
    "        raise ValueError(\"template_id not found in request_args\")\n",
    "        \n",
    "    template_data = fetch_channel_attr_template(saas_edge_id, template_id)\n",
    "    template_details = fetch_saas_channel_templates(saas_edge_id, template_id)  \n",
    "    if not template_data:\n",
    "        print(\"WARNING: No template data retrieved or an error occurred during the fetch operation.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"ERROR: An unexpected error occurred while fetching template data.\")\n",
    "    print(f\"ERROR: {e}\")\n",
    "    print(f\"ERROR: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "  # Return original value if conversion fails\n",
    "\n",
    "\n",
    "\n",
    "filtered_template_data = {}\n",
    "\n",
    "try:\n",
    "    if template_data:\n",
    "        # filtered_template_data = [\n",
    "        #     {\n",
    "        #         'column_name': item.get('column_name', ''),  # Default to empty string if not found\n",
    "        #         'field_mapping': item.get('field_mapping', {})  # Default to empty dict if not found\n",
    "        #     }\n",
    "        #     for item in template_data\n",
    "        #     if isinstance(item, dict)  # Ensure item is a dictionary\n",
    "        # ]\n",
    "        filtered_template_data = {\n",
    "            item.get('column_name', ''): {\n",
    "                \"field_mapping\":item.get('field_mapping', \"\"),\n",
    "                \"accepts_multiple_values\":item.get('accepts_multiple_values', False),\n",
    "                \"transformation_function\":item.get(\"transformation_rules\", \"\"),\n",
    "                \"data_type\":item.get(\"data_type\", \"text\")\n",
    "            } \n",
    "            for item in template_data\n",
    "            \n",
    "        }\n",
    "        if not filtered_template_data:\n",
    "            print(\"WARNING: No valid template data entries found after filtering\")\n",
    "    else:\n",
    "        print(\"WARNING: template_data is None or empty\")\n",
    "\n",
    "    print(\"Filtered template data:\", filtered_template_data)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to process template data: {str(e)}\")\n",
    "    print(f\"ERROR: {traceback.format_exc()}\")\n",
    "    filtered_template_data = {}  # Ensure we always return a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = request_args.get(\"file_url\", {}).get(\"url\")\n",
    "create_new_product = request_args.get(\"feed_settings\", {}).get(\"create_new_product\", False)\n",
    "print(file_url)\n",
    "\n",
    "if template_details.get(\"meta\",{}):\n",
    "    saastify_table = template_details.get(\"meta\",{}).get(\"saastify_table\", \"\")\n",
    "else:\n",
    "    saastify_table = \"products\"\n",
    "    # create_new_product = template_details.get(\"meta\", {}).get(\"create_new_product\", False)\n",
    "print(saastify_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # saastify_table = \"product_parent_variants\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    file_parse = FileParser()\n",
    "    print(file_url)\n",
    "    data_df = file_parse.load(file_url)\n",
    "    properties_schema = list(data_df)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to parse the file: {str(e)}\")\n",
    "    print(f\"ERROR: {traceback.format_exc()}\")\n",
    "    properties_schema = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(properties_schema)\n",
    "# print(filtered_template_data)\n",
    "# print(template_data)\n",
    "\n",
    "# replace|||value|||C:\\OrderWise Data\\||https://storage.googleapis.com/edge-assets/8dddba20-2928-45b2-a26f-381515f9ea68/catalog-edge/dam/public/|;|replace|||value||| ||%20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO Support for multiple rules\n",
    "\n",
    "# def apply_transformation(value, transformation_rules: str):\n",
    "#     \"\"\"\n",
    "#     Apply multiple transformation rules to a value with comprehensive error handling.\n",
    "#     Rules are separated by |;| character sequence.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Handle None/empty values\n",
    "#         if value is None or transformation_rules is None:\n",
    "#             return value\n",
    "            \n",
    "#         # Ensure transformation_rules is a string\n",
    "#         if not isinstance(transformation_rules, str):\n",
    "#             print(f\"WARNING: Invalid transformation rule type: {type(transformation_rules)}\")\n",
    "#             return value\n",
    "        \n",
    "#         # Normalize backslashes in the input value if it's a string\n",
    "#         if isinstance(value, str):\n",
    "#             transformed_value = value.replace('\\\\', '/')\n",
    "#         else:\n",
    "#             transformed_value = value\n",
    "            \n",
    "#         # Split rules by |;| character sequence and process each rule in sequence\n",
    "#         rules = [rule.strip() for rule in transformation_rules.split('|;|')]\n",
    "#         for rule in rules:\n",
    "#             # Basic string transformations\n",
    "#             if rule == \"uppercase\":\n",
    "#                 transformed_value = transformed_value.upper() if isinstance(transformed_value, str) else transformed_value\n",
    "#             elif rule == \"lowercase\":\n",
    "#                 transformed_value = transformed_value.lower() if isinstance(transformed_value, str) else transformed_value\n",
    "#             elif rule == \"strip\":\n",
    "#                 transformed_value = transformed_value.strip() if isinstance(transformed_value, str) else transformed_value\n",
    "#             elif rule == \"split_comma\":\n",
    "#                 if not isinstance(transformed_value, str):\n",
    "#                     continue\n",
    "#                 # Handle potential backslashes in comma-separated values\n",
    "#                 transformed_value = [item.strip().replace('\\\\', '/') \n",
    "#                                    for item in transformed_value.split(\",\") \n",
    "#                                    if item.strip()]\n",
    "                \n",
    "#             elif rule.startswith(\"set|||\"):\n",
    "#                 try:\n",
    "#                     parts = rule.split(\"|||\")\n",
    "#                     if len(parts) != 2:\n",
    "#                         print(f\"WARNING: Invalid set rule format: {rule}. Expected 2 parts.\")\n",
    "#                         continue\n",
    "#                     _, value = parts\n",
    "#                     transformed_value = value\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"WARNING: Error processing set rule: {rule}. Error: {str(e)}\")\n",
    "#                     continue\n",
    "#             elif rule == \"date_only\":\n",
    "#                 if isinstance(transformed_value, (pd.Timestamp, datetime, date)):\n",
    "#                     transformed_value = transformed_value.strftime('%Y-%m-%d')  # Convert datetime to string with only date\n",
    "#                 elif isinstance(transformed_value, str):\n",
    "#                     try:\n",
    "#                         # Handle cases where value is a string with datetime format\n",
    "#                         parsed_date = pd.to_datetime(transformed_value, errors='coerce')\n",
    "#                         if not pd.isnull(parsed_date):  \n",
    "#                             transformed_value = parsed_date.strftime('%Y-%m-%d')\n",
    "#                             print(\"new\",transformed_value)\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"WARNING: Could not parse date for value '{transformed_value}'. Error: {str(e)}\")\n",
    "#             # Complex transformations\n",
    "#             elif rule.startswith(\"replace|||\"):\n",
    "#                 if not isinstance(transformed_value, str):\n",
    "#                     continue\n",
    "#                 try:\n",
    "#                     # Split using ||| delimiter\n",
    "#                     parts = rule.split(\"|||\")\n",
    "#                     if len(parts) != 3:\n",
    "#                         print(f\"WARNING: Invalid replace rule format: {rule}. Expected 3 parts.\")\n",
    "#                         continue\n",
    "                    \n",
    "#                     _, _, replace_rule = parts\n",
    "#                     original, new = replace_rule.split(\"||\", 1)\n",
    "                    \n",
    "#                     original = original.strip().replace('\\\\', '/')\n",
    "#                     new = new.strip().replace('\\\\', '/')\n",
    "#                     original = original.strip().replace('\\\\', '/')\n",
    "#                     new = new.strip().replace('\\\\', '/')\n",
    "#                     transformed_value = transformed_value.replace(original, new)\n",
    "                    \n",
    "#                 except (IndexError, ValueError) as e:\n",
    "#                     print(f\"WARNING: Invalid replace rule format: {rule}. Error: {str(e)}\")\n",
    "#                     continue\n",
    "                    \n",
    "#             elif rule == \"vlookup:map\":\n",
    "#                 if not isinstance(transformed_value, str):\n",
    "#                     continue\n",
    "#                 mapping = {\n",
    "#                     \"yes\": True,\n",
    "#                     \"no\": False,\n",
    "#                     \"true\": True,\n",
    "#                     \"false\": False,\n",
    "#                     \"1\": True,\n",
    "#                     \"0\": False,\n",
    "#                     \"y\": True,\n",
    "#                     \"n\": False\n",
    "#                 }\n",
    "#                 # Convert to lowercase and strip before mapping\n",
    "#                 lookup_value = transformed_value.lower().strip()\n",
    "#                 transformed_value = mapping.get(lookup_value, transformed_value)\n",
    "            \n",
    "#             # New transformation rule for adjusting negative numbers to zero\n",
    "#             elif rule == \"adjust_negative_to_zero\":\n",
    "#                 print(\"transformed_value\",transformed_value)\n",
    "#                 try:\n",
    "#                     if isinstance(transformed_value, (int, float)):\n",
    "#                         transformed_value = max(0, transformed_value)\n",
    "#                     elif isinstance(transformed_value, list):\n",
    "#                         transformed_value = [max(0, float(val)) if isinstance(val, (int, float, str)) else val \n",
    "#                                           for val in transformed_value]\n",
    "#                     elif isinstance(transformed_value, str):\n",
    "#                         try:\n",
    "#                             num_value = float(transformed_value)\n",
    "#                             transformed_value = str(max(0, num_value))\n",
    "#                         except ValueError:\n",
    "#                             print(f\"WARNING: Could not convert string '{transformed_value}' to number for negative adjustment\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"WARNING: Error applying negative adjustment: {str(e)}\")\n",
    "#                     continue\n",
    "            \n",
    "#             else:\n",
    "#                 print(f\"WARNING: Unknown transformation rule: {rule}\")\n",
    "#                 continue\n",
    "        \n",
    "#         return transformed_value\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"ERROR: Transformation failed for value '{value}' with rules '{transformation_rules}': {str(e)}\")\n",
    "#         return value  # Return original value on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime, date\n",
    "\n",
    "\n",
    "# \u2500\u2500\u2500 Sentinel \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "class RejectRow(Exception):\n",
    "    pass\n",
    "\n",
    "# \u2500\u2500\u2500 Transform helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def uppercase(v, **kw): return v.upper() if isinstance(v, str) else v\n",
    "def lowercase(v, **kw): return v.lower() if isinstance(v, str) else v\n",
    "def strip(v, chars=None, **kw): return v.strip(chars) if isinstance(v, str) else v\n",
    "def title_case(v, **kw): return v.title() if isinstance(v, str) else v\n",
    "def capitalize(v, **kw): return v.capitalize() if isinstance(v, str) else v\n",
    "def split_comma(v, **kw): return [x.strip() for x in v.split(',')] if isinstance(v, str) else v\n",
    "def split(v, delimiter, **kw): return v.split(delimiter) if isinstance(v, str) else v\n",
    "def join(v, delimiter, **kw):\n",
    "    if isinstance(v, str): v = v.split()\n",
    "    return delimiter.join(str(x) for x in v) if isinstance(v, (list, tuple)) else v\n",
    "def replace(v, old, new, **kw): return v.replace(old, new) if isinstance(v, str) else v\n",
    "def replace_regex(v, pattern, repl, **kw): return re.sub(pattern, repl, v) if isinstance(v, str) else v\n",
    "def clean_numeric_value(v, **kw): return float(re.sub(r'[^\\d.]', '', v)) if isinstance(v, str) else v\n",
    "def clean_upc(v, **kw): return re.sub(r'[^\\d]', '', v) if isinstance(v, str) else v\n",
    "def clean_html(v, **kw): return re.sub(r'<.*?>', '', v) if isinstance(v, str) else v\n",
    "def date_only(v, **kw):\n",
    "    if isinstance(v, str):\n",
    "        try: return datetime.strptime(v.strip(), \"%Y-%m-%d %H:%M:%S\").strftime(\"%Y-%m-%d\")\n",
    "        except ValueError: return v.split()[0] if \" \" in v else v\n",
    "    if isinstance(v, (datetime, date)): return v.strftime(\"%Y-%m-%d\")\n",
    "    return v\n",
    "def set_value(v, value, **kw): return value\n",
    "def set_number(v, value, **kw): return int(value)\n",
    "def zero_padding(v, value, **kw): return str(v).zfill(int(value))\n",
    "def addition(v, amount, **kw): return float(v) + float(amount)\n",
    "def subtraction(v, amount, **kw): return float(v) - float(amount)\n",
    "def multiplication(v, factor, **kw): return float(v) * float(factor)\n",
    "def division(v, divisor, **kw): return float(v) / float(divisor) if float(divisor) != 0 else None\n",
    "def percentage(v, factor=100, **kw): return float(v) * float(factor)\n",
    "def adjust_negative_to_zero(v, **kw):\n",
    "    try: return max(0, float(v))\n",
    "    except: return v\n",
    "def vlookup_map(v, mapping=None, **kw):\n",
    "    if isinstance(v, str) and mapping: return mapping.get(v.lower(), v)\n",
    "    return v\n",
    "def prefix(v, prefix_str=\"-\", **kw):\n",
    "    if isinstance(v, (list, tuple)): return [prefix_str + str(x) for x in v]\n",
    "    if isinstance(v, str): return prefix_str + v\n",
    "    return v\n",
    "def suffix(v, suffix_str=\"_\", **kw):\n",
    "    if isinstance(v, (list, tuple)): return [str(x) + suffix_str for x in v]\n",
    "    if isinstance(v, str): return v + suffix_str\n",
    "    return v\n",
    "def copy(v, **kw): return v\n",
    "def rejects(v, **kw): raise RejectRow()\n",
    "\n",
    "# \u2500\u2500\u2500 Registry \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "TRANSFORMS = {\n",
    "    \"uppercase\": uppercase, \"lowercase\": lowercase, \"strip\": strip, \"title_case\": title_case,\n",
    "    \"capitalize\": capitalize, \"split_comma\": split_comma, \"split\": split, \"join\": join,\n",
    "    \"replace\": replace, \"replace_regex\": replace_regex, \"clean_numeric_value\": clean_numeric_value,\n",
    "    \"clean_upc\": clean_upc, \"clean_html\": clean_html, \"date_only\": date_only,\n",
    "    \"set\": set_value, \"set_number\": set_number, \"zero_padding\": zero_padding,\n",
    "    \"addition\": addition, \"subtraction\": subtraction, \"multiplication\": multiplication,\n",
    "    \"division\": division, \"percentage\": percentage, \"adjust_negative_to_zero\": adjust_negative_to_zero,\n",
    "    \"vlookup_map\": vlookup_map, \"prefix\": prefix, \"suffix\": suffix,\n",
    "    \"copy\": copy, \"rejects\": rejects\n",
    "}\n",
    "\n",
    "# \u2500\u2500\u2500 Run one pipeline \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def apply_transformations(val, steps):\n",
    "    v = val\n",
    "    for st in steps:\n",
    "        func = TRANSFORMS[st[\"name\"]]\n",
    "        try: v = func(v, **st.get(\"params\", {}))\n",
    "        except RejectRow: return None, True\n",
    "    return v, False\n",
    "\n",
    "# \u2500\u2500\u2500 DSL engine with broadcasting \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def bulk_apply_pipe_rules(values_list, rule_strings):\n",
    "    if not isinstance(values_list, list):\n",
    "        raise ValueError(\"values_list must be a list\")\n",
    "\n",
    "    if isinstance(rule_strings, str):\n",
    "        rule_strings = [rule_strings] * len(values_list)\n",
    "    elif isinstance(rule_strings, list):\n",
    "        if len(rule_strings) == 1 and len(values_list) > 1:\n",
    "            rule_strings *= len(values_list)\n",
    "        elif len(values_list) == 1 and len(rule_strings) > 1:\n",
    "            values_list *= len(rule_strings)\n",
    "        elif len(rule_strings) != len(values_list):\n",
    "            raise ValueError(\"Length mismatch between values_list and rule_strings\")\n",
    "    else:\n",
    "        raise ValueError(\"rule_strings must be str or list[str]\")\n",
    "\n",
    "    results = []\n",
    "    for val, rule in zip(values_list, rule_strings):\n",
    "        if not rule:\n",
    "            results.append(val)\n",
    "            continue\n",
    "\n",
    "        steps = []\n",
    "        for token_raw in rule.replace('+', '|;|').split('|;|'):\n",
    "            token = token_raw.lstrip()\n",
    "            if not token:\n",
    "                continue\n",
    "\n",
    "            # ---------- split ----------\n",
    "            if token.startswith(\"split|\"):\n",
    "                raw = token[6:]\n",
    "                if raw in (\"|||\", r\"\\|\"):\n",
    "                    delim = \"|\"\n",
    "                elif raw.startswith(\"|||\"):\n",
    "                    delim = raw[3:] or \"|\"\n",
    "                elif raw.startswith(\"||\"):\n",
    "                    tail = raw[2:]\n",
    "                    delim = \" \" if tail in (\"\", \" \") else tail\n",
    "                elif raw.startswith(r\"\\|\"):\n",
    "                    delim = \"|\" + raw[2:]\n",
    "                else:\n",
    "                    delim = raw\n",
    "                if delim != \" \": delim = delim.strip()\n",
    "                if delim == \"\": raise ValueError(\"split requires non-empty delimiter\")\n",
    "                steps.append({\"name\": \"split\", \"params\": {\"delimiter\": delim}})\n",
    "                continue\n",
    "\n",
    "            # ---------- join ----------\n",
    "            if token.startswith(\"join|\"):\n",
    "                steps.append({\"name\": \"join\", \"params\": {\"delimiter\": token[5:]}})\n",
    "                continue\n",
    "\n",
    "            # ---------- prefix / suffix ----------\n",
    "            if token.startswith(\"prefix|\"):\n",
    "                steps.append({\"name\": \"prefix\", \"params\": {\"prefix_str\": token[7:]}})\n",
    "                continue\n",
    "            if token.startswith(\"suffix|\"):\n",
    "                steps.append({\"name\": \"suffix\", \"params\": {\"suffix_str\": token[7:]}})\n",
    "                continue\n",
    "\n",
    "            # ---------- replace_regex ----------\n",
    "            if token.startswith(\"replace_regex|\"):\n",
    "                pat, *rep = token[14:].split(\"||\", 1)\n",
    "                steps.append({\"name\": \"replace_regex\",\n",
    "                              \"params\": {\"pattern\": pat, \"repl\": rep[0] if rep else \"\"}})\n",
    "                continue\n",
    "\n",
    "            # ---------- replace ----------\n",
    "            if token.startswith(\"replace|\"):\n",
    "                try:\n",
    "                    old, new = token[8:].split('|', 1)\n",
    "                except ValueError:\n",
    "                    raise ValueError(f\"replace requires two parameters: {token}\")\n",
    "                steps.append({\"name\": \"replace\",\n",
    "                              \"params\": {\"old\": old, \"new\": new}})\n",
    "                continue\n",
    "\n",
    "            # ---------- vlookup ----------\n",
    "            if token.startswith(\"vlookup|\"):\n",
    "                mapping = {}\n",
    "                for pair in token[8:].rstrip('|').split(','):\n",
    "                    k, v = pair.split(':', 1)\n",
    "                    v = v.strip()\n",
    "                    mapping[k.strip().lower()] = \\\n",
    "                        True if v.lower() == \"true\" else \\\n",
    "                        False if v.lower() == \"false\" else \\\n",
    "                        float(v) if re.match(r'^-?\\d+\\.\\d+$', v) else \\\n",
    "                        int(v) if v.isdigit() else v\n",
    "                steps.append({\"name\": \"vlookup_map\", \"params\": {\"mapping\": mapping}})\n",
    "                continue\n",
    "\n",
    "            # ---------- arithmetic ----------\n",
    "            for op in (\"addition\", \"subtraction\", \"multiplication\", \"division\", \"percentage\"):\n",
    "                if token.startswith(f\"{op}|\"):\n",
    "                    param = token.split('|', 1)[1]\n",
    "                    key = (\"amount\" if op in (\"addition\", \"subtraction\") else\n",
    "                           \"factor\" if op in (\"multiplication\", \"percentage\") else\n",
    "                           \"divisor\")\n",
    "                    steps.append({\"name\": op, \"params\": {key: param}})\n",
    "                    break\n",
    "            else:\n",
    "                # ---------- strip with chars / set / zero_padding ----------\n",
    "                if token.startswith(\"strip|\"):\n",
    "                    steps.append({\"name\": \"strip\",\n",
    "                                  \"params\": {\"chars\": token.split('|',1)[1]}})\n",
    "                elif token.startswith(\"set|\"):\n",
    "                    steps.append({\"name\": \"set\",\n",
    "                                  \"params\": {\"value\": token.split('|',1)[1]}})\n",
    "                elif token.startswith(\"set_number|\"):\n",
    "                    steps.append({\"name\": \"set_number\",\n",
    "                                  \"params\": {\"value\": token.split('|',1)[1]}})\n",
    "                elif token.startswith(\"zero_padding|\"):\n",
    "                    steps.append({\"name\": \"zero_padding\",\n",
    "                                  \"params\": {\"value\": token.split('|',1)[1]}})\n",
    "                elif token == \"rejects\":\n",
    "                    steps.append({\"name\": \"rejects\"})\n",
    "                else:\n",
    "                    steps.append({\"name\": token})\n",
    "\n",
    "        out, _ = apply_transformations(val, steps)\n",
    "        results.append(out)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def apply_transformation(value, transformation_rules: str):\n",
    "    \"\"\"\n",
    "    Adapter that preserves your old call site but normalizes legacy tokens:\n",
    "    - 'set|||VALUE'        -> 'set|VALUE'\n",
    "    - 'replace|||OLD||NEW' -> 'replace|OLD|NEW'   (optional but handy)\n",
    "    Then forwards to bulk_apply_pipe_rules unchanged.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if transformation_rules is None:\n",
    "            return value\n",
    "        if not isinstance(transformation_rules, str):\n",
    "            print(f\"WARNING: Invalid transformation rule type: {type(transformation_rules)}\")\n",
    "            return value\n",
    "\n",
    "        # Optional: keep your backslash \u2192 slash normalization\n",
    "        if isinstance(value, str):\n",
    "            value = value.replace(\"\\\\\", \"/\")\n",
    "\n",
    "        # --- Normalize legacy rule formats (minimal, safe) ---\n",
    "        fixed_rules = transformation_rules\n",
    "\n",
    "        # set|||VALUE  -> set|VALUE\n",
    "        fixed_rules = re.sub(r'(?<!\\w)set\\|\\|\\|', 'set|', fixed_rules)\n",
    "\n",
    "        # replace|||OLD||NEW -> replace|OLD|NEW\n",
    "        # (only converts the first '||' after 'replace|||' to keep the rest intact)\n",
    "        fixed_rules = re.sub(r'(?<!\\w)replace\\|\\|\\|([^|]+)\\|\\|', r'replace|\\1|', fixed_rules)\n",
    "\n",
    "        # Hand off directly to your DSL (no extra splitting here)\n",
    "        return bulk_apply_pipe_rules([value], fixed_rules)[0]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Transformation failed for value '{value}' with rules '{transformation_rules}': {str(e)}\")\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_value(value, data_type):\n",
    "    import datetime\n",
    "\n",
    "    \"\"\"\n",
    "    Converts the given value to the specified data type.\n",
    "    If the value is already correct, it remains unchanged.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if data_type == \"text\":\n",
    "            return str(value) if value is not None else \"\"\n",
    "\n",
    "        elif data_type == \"integer\":\n",
    "            return int(value) if isinstance(value, (int, float, str)) and str(value).isdigit() else 0\n",
    "\n",
    "        elif data_type == \"float\":\n",
    "            return float(value) if isinstance(value, (int, float, str)) and str(value).replace('.', '', 1).isdigit() else 0.0\n",
    "\n",
    "        elif data_type == \"boolean\":\n",
    "            return str(value).lower() in [\"true\", \"1\", \"yes\"]\n",
    "\n",
    "        elif data_type == \"datetime\":\n",
    "            if isinstance(value, (datetime.datetime, datetime.date, pd.Timestamp)):\n",
    "                return value.strftime('%Y-%m-%d %H:%M:%S')  # Convert datetime to string\n",
    "            try: \n",
    "                return str(pd.to_datetime(value))  # Convert if possible\n",
    "            except:\n",
    "                return str(value)\n",
    "        elif data_type == \"timestamp\":\n",
    "            if isinstance(value, (datetime.datetime, datetime.date, pd.Timestamp)):\n",
    "                return value.strftime('%Y-%m-%d %H:%M:%S')  # Convert datetime to string\n",
    "            try: \n",
    "                return str(pd.to_datetime(value))  # Convert if possible\n",
    "            except:\n",
    "                return str(value)\n",
    "        \n",
    "                \n",
    "        elif data_type == \"date\":\n",
    "            if isinstance(value, (datetime.datetime, datetime.date, pd.Timestamp)):\n",
    "                return value.strftime('%Y-%m-%d ')  # Convert timestamp to YYYY-MM-DD (No time)\n",
    "            try:\n",
    "                return pd.to_datetime(value).strftime('%Y-%m-%d')  # Convert if possible\n",
    "            except:\n",
    "                return str(value)\n",
    "\n",
    "        elif data_type == \"jsonb\":\n",
    "            if isinstance(value, str):\n",
    "                try:\n",
    "                    return json.loads(value)  # Convert JSON string to dict\n",
    "                except json.JSONDecodeError:\n",
    "                    return {}  # Return empty dict if invalid JSON\n",
    "            return value  # If already dict, return as is\n",
    "\n",
    "        elif data_type == \"array\":\n",
    "            return value if isinstance(value, list) else [value]  # Ensure it's always a list\n",
    "\n",
    "        else:\n",
    "            return value  # Keep as is for unknown data types\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Failed to convert value '{value}' to {data_type}: {e}\")\n",
    "        return value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_request_objects(data_df, filtered_template_data):\n",
    "    request_objects = []\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        for index, row in data_df.iterrows():\n",
    "            \n",
    "            \n",
    "            request_obj = {}\n",
    "            # Convert row to dictionary and handle NaN values\n",
    "            row_dict = {\n",
    "                k: (None if pd.isna(v) else v) \n",
    "                for k, v in row.to_dict().items()\n",
    "            }\n",
    "            \n",
    "            for col_name in data_df.columns:\n",
    "                \n",
    "                if col_name not in filtered_template_data:\n",
    "                    continue  # column not in template, skip\n",
    "\n",
    "                value = row_dict.get(col_name)\n",
    "                # if value is None or (isinstance(value, str) and value.strip() == \"\"):\n",
    "                #     continue  # value is empty, skip\n",
    "\n",
    "                field_config = filtered_template_data[col_name]\n",
    "                if not field_config or not field_config.get(\"field_mapping\"):\n",
    "                    continue\n",
    "                try:\n",
    "                    if pd.notnull(value):\n",
    "                        # Only clean if it's numeric (int/float) and ends in .0\n",
    "                        if isinstance(value, (int, float)) and float(value).is_integer():\n",
    "                            value = str(int(value))  # 123456.0 \u2192 \"123456\"\n",
    "                        else:\n",
    "                            value = str(value).strip()  # Keep as-is for strings like \"123456.0lkw\"\n",
    "                except Exception as e:\n",
    "                    print(f\"WARNING: Error cleaning value '{value}': {e}\")\n",
    "\n",
    "                if value is None:\n",
    "                    value = None\n",
    "                \n",
    "                field_mapping = field_config['field_mapping']\n",
    "                accepts_multiple = field_config.get('accepts_multiple_values', False)\n",
    "                transformation_function = field_config.get(\"transformation_function\", \"\")\n",
    "                data_type = field_config.get(\"data_type\", \"\")\n",
    "                \n",
    "\n",
    "                if data_type:\n",
    "                    value = convert_value(value, data_type)\n",
    "                   \n",
    "                # trasnformation needs to be applied to value\n",
    "                if transformation_function:\n",
    "                    value = apply_transformation(value, transformation_function)\n",
    "                    # while isinstance(temp_value, list) and temp_value:\n",
    "                    #     value = temp_value[0]\n",
    "                \n",
    "                # Handle nested mappings (e.g., \"raw_product_data.field_name\")\n",
    "                if \".\" in field_mapping:\n",
    "                    parent_key, child_key = field_mapping.split(\".\", 1)\n",
    "                    \n",
    "                    # Initialize parent dictionary if it doesn't exist\n",
    "                    if parent_key not in request_obj:\n",
    "                        request_obj[parent_key] = {}\n",
    "                        \n",
    "                    request_obj[parent_key][child_key] = value.strip() if isinstance(value, str) else value\n",
    "                else:\n",
    "                    # Handle non-nested fields\n",
    "                    if accepts_multiple and isinstance(value, str):\n",
    "                        request_obj[field_mapping] = [v.strip() for v in value.split(',') if v.strip()]\n",
    "                    else:\n",
    "                        request_obj[field_mapping] = value.strip() if isinstance(value, str) else value\n",
    "            \n",
    "            request_objects.append(request_obj)\n",
    "            \n",
    "        print(f\"INFO: Successfully created {len(request_objects)} request objects\")\n",
    "        return request_objects\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to build request objects: {str(e)}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "request_objects = build_request_objects(data_df, filtered_template_data)\n",
    "# print(request_objects)\n",
    "# print(data_df)\n",
    "# print(filtered_template_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update job details\n",
    "def update_job_details(job_id, total_count,success_count, failure_count, failed_url=None,failed_report=None):\n",
    "    \"\"\"\n",
    "    Update job details in the database with success/failure counts and failed job summary link\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        job_response = {\n",
    "            \"total\": total_count,\n",
    "            \"success\": success_count,\n",
    "            \"failed\": failure_count,\n",
    "        }\n",
    "\n",
    "        update_fields = [\"job_response = %s\"]\n",
    "        update_values = [json.dumps(job_response)]\n",
    "        \n",
    "\n",
    "        # Add failed_job_summary_link if present\n",
    "        if failed_url:\n",
    "            update_fields.append(\"failed_job_summary_link = %s\")\n",
    "            update_values.append(failed_url)\n",
    "\n",
    "        # Add job_id for WHERE clause\n",
    "        update_values.append(job_id)\n",
    "        print(\"update_values\",update_values)\n",
    "\n",
    "        query = f\"\"\"\n",
    "        UPDATE saas_edge_jobs\n",
    "        SET {\", \".join(update_fields)}\n",
    "        WHERE request_id = %s\n",
    "        RETURNING *;\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(query, update_values)\n",
    "        updated_row = cursor.fetchone()\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"INFO: Successfully updated job details for job_id: {job_id}\")\n",
    "        return updated_row\n",
    "\n",
    "    except Exception as e:\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "        print(f\"ERROR: Failed to update job details: {str(e)}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Upload files to gcp\n",
    "def sanitize_path(path):\n",
    "    \"\"\"\n",
    "    Sanitize path by replacing forward slashes with underscores\n",
    "    \"\"\"\n",
    "    return path.replace('/', '_').replace(\".ipynb\",\"\").strip('')\n",
    "\n",
    "def get_output_filename(saas_edge_id, job_path,file_type):\n",
    "    \"\"\"\n",
    "    Generate standardized output filename with proper date formatting\n",
    "    \"\"\"\n",
    "    try:\n",
    "        current_date = datetime.now()\n",
    "        date_str = current_date.strftime('%Y-%m-%d')\n",
    "        timestamp_str = current_date.strftime('%H%M%S')\n",
    "        sanitized_job_path = sanitize_path(job_path)\n",
    "        \n",
    "        return f\"{saas_edge_id}/catalog-edge/job-reports/{sanitized_job_path}/{date_str}/import-failed-list-{timestamp_str}.{file_type}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating output filename: {str(e)}\")\n",
    "        # Fallback filename if something goes wrong\n",
    "        return f\"{saas_edge_id}/catalog-edge/job-reports/error_report.json\"\n",
    "\n",
    "def upload_to_gcp_bucket(file_data, file_name, bucket_name, base_path=\"\", credentials=None):\n",
    "    \"\"\"\n",
    "    Generic function to upload data to GCP bucket using existing credentials\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create credentials object from the service account info\n",
    "        gcp_credentials = service_account.Credentials.from_service_account_info(credentials)\n",
    "        storage_client = storage.Client(credentials=gcp_credentials)\n",
    "        \n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        \n",
    "        # Combine base path with file name\n",
    "        full_path = f\"{base_path.rstrip('/')}/{file_name}\" if base_path else file_name\n",
    "        blob = bucket.blob(full_path)\n",
    "        \n",
    "        # Add retry logic for upload\n",
    "        retry_count = 0\n",
    "        max_retries = 3\n",
    "        \n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                # Convert data to appropriate format for upload\n",
    "                if isinstance(file_data, (list, dict)):\n",
    "                    # Convert to JSON string for lists and dictionaries\n",
    "                    upload_data = json.dumps(file_data, indent=2)\n",
    "                    blob.upload_from_string(upload_data, content_type='application/json')\n",
    "                elif isinstance(file_data, str):\n",
    "                    blob.upload_from_string(file_data)\n",
    "                elif isinstance(file_data, bytes):\n",
    "                    blob.upload_from_string(file_data, content_type='application/octet-stream')\n",
    "                else:\n",
    "                    # For file-like objects\n",
    "                    blob.upload_from_file(file_data)\n",
    "                    \n",
    "                # Generate public URL\n",
    "                url = f\"https://storage.googleapis.com/{bucket_name}/{full_path}\"\n",
    "                print(f\"Successfully uploaded {file_name} to {url}\")\n",
    "                return True, url\n",
    "                \n",
    "            except Exception as upload_error:\n",
    "                retry_count += 1\n",
    "                if retry_count == max_retries:\n",
    "                    print(f\"Failed to upload {file_name} after {max_retries} attempts: {str(upload_error)}\")\n",
    "                    return False, \"\"\n",
    "                print(f\"Retry {retry_count} for {file_name}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to GCP bucket: {str(e)}\")\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return False, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP credentials should be loaded from GOOGLE_APPLICATION_CREDENTIALS environment variable\n",
    "# or use Application Default Credentials\n",
    "# Example: os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_data_import(saas_edge_id, saastify_table, request_objects, create_new_product=create_new_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # def convert_to_us_central(date_str):\n",
    " #        \"\"\"Convert given date string to US Central Time (assuming it's in ISO 8601 or UTC).\"\"\"\n",
    " #        if not date_str:\n",
    " #            return None\n",
    " #        try:\n",
    " #            # Parse the string to a datetime object\n",
    " #            dt = datetime.fromisoformat(date_str.replace(\"Z\", \"+00:00\"))\n",
    " #            # Convert to US Central Time\n",
    " #            central_tz = pytz.timezone(\"US/Central\")\n",
    " #            dt_central = dt.astimezone(central_tz)\n",
    " #            # Return in ISO format without microseconds\n",
    " #            return dt_central.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    " #        except Exception as e:\n",
    " #            print(f\"Date conversion error for {date_str}: {e}\")\n",
    " #            return date_str  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_to_us_central(\"2025-09-04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}