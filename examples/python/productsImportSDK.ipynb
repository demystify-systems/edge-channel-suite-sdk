{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Products Import Job (SDK-Based)\n",
        "\n",
        "This notebook runs a products import job using the SaaStify Edge SDK.\n",
        "\n",
        "## Parameters (Papermill)\n",
        "\n",
        "This notebook accepts the following parameters (matching productsImport.ipynb pattern):\n",
        "- `job_id`: Request/job identifier (request_id in saas_edge_jobs table)\n",
        "- `saas_edge_id`: Tenant identifier\n",
        "\n",
        "The notebook will fetch job details from `saas_edge_jobs` table and extract:\n",
        "- `file_url`, `file_name` from `request_args.file_url`\n",
        "- `template_id` from `request_args.template_id` (or look up from `import_type`)\n",
        "- `feed_settings` from `request_args.feed_settings`\n",
        "- `job_path` from `request_args.job_path`\n",
        "- `import_type` from `request_args.import_type`\n",
        "\n",
        "## Environment Variables Required\n",
        "\n",
        "Set these before running:\n",
        "- `DB_MODE`: Database connection mode (direct/proxy/local)\n",
        "- `DB_INSTANCE`: Cloud SQL instance (for direct mode)\n",
        "- `DB_NAME`: Database name\n",
        "- `DB_USER`: Database user\n",
        "- `DB_PASSWORD`: Database password\n",
        "- `GOOGLE_APPLICATION_CREDENTIALS`: Path to GCS service account JSON (optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Papermill parameters - these will be injected when running with papermill\n",
        "# Parameters match productsImport.ipynb pattern\n",
        "job_id = \"your-job-id-here\"  # request_id in saas_edge_jobs table\n",
        "saas_edge_id = \"your-saas-edge-id-here\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies and Import SDK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (matching productsImport.ipynb)\n",
        "%pip install saastify-edge-sdk pg8000 psycopg2-binary pandas requests openpyxl google-cloud-storage --quiet\n",
        "\n",
        "# Import required modules\n",
        "import asyncio\n",
        "import json\n",
        "import os\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any, Optional\n",
        "from uuid import UUID\n",
        "\n",
        "# Import SDK components\n",
        "from saastify_edge import ImportPipelineConfig, run_product_import\n",
        "from saastify_edge.db import PostgreSQLClient, get_db_config, JobStatusUpdater\n",
        "from saastify_edge.utils import setup_logging, set_job_context, get_or_generate_template_schema\n",
        "\n",
        "print(\"‚úÖ Dependencies installed and SDK imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fetch Job Details from Database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize database client to fetch job details\n",
        "db_config = get_db_config()\n",
        "db_client = PostgreSQLClient(db_config)\n",
        "await db_client.connect()\n",
        "\n",
        "# Fetch job details from saas_edge_jobs table (matching productsImport.ipynb pattern)\n",
        "job_manager = JobStatusUpdater(db_client)\n",
        "\n",
        "try:\n",
        "    # Get job by request_id (matching productsImport.ipynb)\n",
        "    job_record = await job_manager.get_job_by_request_id(job_id, saas_edge_id)\n",
        "    \n",
        "    if not job_record:\n",
        "        raise ValueError(f\"No job found with request_id={job_id} and saas_edge_id={saas_edge_id}\")\n",
        "    \n",
        "    # Extract request_args (matching productsImport.ipynb)\n",
        "    request_args = job_record.get(\"request_args\", {})\n",
        "    \n",
        "    print(\"‚úÖ Job details fetched successfully\")\n",
        "    print(f\"Job Name: {job_record.get('job_name', 'N/A')}\")\n",
        "    print(f\"Job Status: {job_record.get('job_status', 'N/A')}\")\n",
        "    print(f\"\\nRequest Args:\")\n",
        "    for key, value in request_args.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    \n",
        "    # Extract parameters from request_args (matching productsImport.ipynb structure)\n",
        "    file_url_obj = request_args.get(\"file_url\", {})\n",
        "    file_url = file_url_obj.get(\"url\") if isinstance(file_url_obj, dict) else file_url_obj\n",
        "    file_name = file_url_obj.get(\"file_name\", \"\") if isinstance(file_url_obj, dict) else request_args.get(\"file_name\", \"\")\n",
        "    job_name = request_args.get(\"job_name\", job_record.get(\"job_name\", \"Products Import\"))\n",
        "    job_path = request_args.get(\"job_path\", \"\")\n",
        "    template_id = request_args.get(\"template_id\")\n",
        "    import_type = request_args.get(\"import_type\", \"products\")\n",
        "    feed_settings = request_args.get(\"feed_settings\", {})\n",
        "    \n",
        "    print(f\"\\n‚úÖ Extracted parameters:\")\n",
        "    print(f\"  File URL: {file_url}\")\n",
        "    print(f\"  File Name: {file_name}\")\n",
        "    print(f\"  Template ID: {template_id}\")\n",
        "    print(f\"  Import Type: {import_type}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to fetch job details: {e}\")\n",
        "    traceback.print_exc()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Generate Template Schema (if needed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate or fetch template schema (matching productsImport.ipynb pattern)\n",
        "if not template_id:\n",
        "    # Look up template_id from import_type (matching productsImport.ipynb)\n",
        "    template_query = \"\"\"\n",
        "        SELECT template_id \n",
        "        FROM saas_channel_templates \n",
        "        WHERE saas_edge_id = $1 \n",
        "          AND import_type = $2 \n",
        "          AND is_active = true\n",
        "        ORDER BY created_at DESC\n",
        "        LIMIT 1\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        template_result = await db_client.fetch_one(template_query, saas_edge_id, import_type)\n",
        "        if template_result:\n",
        "            template_id = template_result.get(\"template_id\")\n",
        "            print(f\"‚úÖ Found template: {template_id}\")\n",
        "        else:\n",
        "            raise ValueError(f\"No active template found for import_type: {import_type}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Template lookup failed: {e}\")\n",
        "        raise\n",
        "\n",
        "# Generate or fetch template schema\n",
        "try:\n",
        "    template_schema = await get_or_generate_template_schema(db_client, saas_edge_id, template_id)\n",
        "    print(f\"‚úÖ Template schema ready\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Schema generation failed: {e}\")\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Setup Logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup structured logging (matching productsImport.ipynb)\n",
        "setup_logging(level=\"INFO\", structured=True)\n",
        "\n",
        "# Set job context for logging\n",
        "set_job_context(\n",
        "    job_name=job_name,\n",
        "    saas_edge_id=saas_edge_id,\n",
        "    import_type=import_type,\n",
        "    template_id=template_id\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Logging configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Configure Import Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure file loader for GCS/HTTP (matching productsImport.ipynb)\n",
        "file_loader_config = {}\n",
        "\n",
        "# If using GCS and need explicit credentials\n",
        "if file_url and (file_url.startswith(\"gs://\") or \"storage.googleapis.com\" in file_url):\n",
        "    if os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"):\n",
        "        file_loader_config[\"gcs_credentials_path\"] = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
        "        print(f\"‚úÖ GCS credentials configured\")\n",
        "\n",
        "# Create import pipeline configuration\n",
        "import_config = ImportPipelineConfig(\n",
        "    file_source=file_url,  # HTTP URL or gs:// URL\n",
        "    template_id=template_id,\n",
        "    saas_edge_id=saas_edge_id,\n",
        "    job_name=job_name,\n",
        "    batch_size=500,  # Process 500 rows per batch\n",
        "    max_workers=4,   # Use 4 concurrent workers\n",
        "    file_loader_config=file_loader_config,\n",
        "    db_client=db_client  # Real database client (not mock)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Import pipeline configured\")\n",
        "print(f\"  File source: {file_url}\")\n",
        "print(f\"  Template ID: {template_id}\")\n",
        "print(f\"  Batch size: {import_config.batch_size}\")\n",
        "print(f\"  Max workers: {import_config.max_workers}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Add Utility Functions (Matching Products Notebooks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility functions matching productsImport.ipynb pattern\n",
        "from google.cloud import storage\n",
        "from google.oauth2 import service_account\n",
        "from io import BytesIO\n",
        "import pandas as pd\n",
        "\n",
        "GCP_BUCKET_NAME = os.getenv(\"GCP_BUCKET_NAME\", \"edge-assets\")\n",
        "\n",
        "def sanitize_path(path):\n",
        "    \"\"\"Sanitize path by replacing forward slashes with underscores\"\"\"\n",
        "    return path.replace('/', '_').replace(\".ipynb\", \"\").strip()\n",
        "\n",
        "def get_output_filename(saas_edge_id, job_path, file_type):\n",
        "    \"\"\"Generate standardized output filename with proper date formatting\"\"\"\n",
        "    try:\n",
        "        current_date = datetime.now()\n",
        "        date_str = current_date.strftime('%Y-%m-%d')\n",
        "        timestamp_str = current_date.strftime('%H%M%S')\n",
        "        sanitized_job_path = sanitize_path(job_path)\n",
        "        return f\"{saas_edge_id}/catalog-edge/job-reports/{sanitized_job_path}/{date_str}/import-failed-list-{timestamp_str}.{file_type}\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating output filename: {str(e)}\")\n",
        "        return f\"{saas_edge_id}/catalog-edge/job-reports/error_report.json\"\n",
        "\n",
        "async def upload_to_gcp_bucket(file_data, file_name, bucket_name, base_path=\"\"):\n",
        "    \"\"\"Upload data to GCP bucket using Application Default Credentials\"\"\"\n",
        "    try:\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        full_path = f\"{base_path.rstrip('/')}/{file_name}\" if base_path else file_name\n",
        "        blob = bucket.blob(full_path)\n",
        "        \n",
        "        if isinstance(file_data, (list, dict)):\n",
        "            upload_data = json.dumps(file_data, indent=2)\n",
        "            blob.upload_from_string(upload_data, content_type='application/json')\n",
        "        elif isinstance(file_data, str):\n",
        "            blob.upload_from_string(file_data)\n",
        "        elif isinstance(file_data, bytes):\n",
        "            blob.upload_from_string(file_data, content_type='application/octet-stream')\n",
        "        else:\n",
        "            blob.upload_from_file(file_data)\n",
        "        \n",
        "        url = f\"https://storage.googleapis.com/{bucket_name}/{full_path}\"\n",
        "        print(f\"Successfully uploaded {file_name} to {url}\")\n",
        "        return True, url\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading to GCP bucket: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        return False, \"\"\n",
        "\n",
        "async def update_job_details(job_id, total_count, success_count, failure_count, failed_url=None):\n",
        "    \"\"\"Update job details in database (matching productsImport.ipynb pattern)\"\"\"\n",
        "    try:\n",
        "        job_response = {\n",
        "            \"total\": total_count,\n",
        "            \"success\": success_count,\n",
        "            \"failed\": failure_count,\n",
        "        }\n",
        "        \n",
        "        update_data = {\n",
        "            \"job_response\": job_response,\n",
        "            \"updated_at\": datetime.utcnow()\n",
        "        }\n",
        "        \n",
        "        if failed_url:\n",
        "            update_data[\"failed_job_summary_link\"] = failed_url\n",
        "        \n",
        "        await db_client.update(\n",
        "            \"saas_edge_jobs\",\n",
        "            {\"request_id\": job_id},\n",
        "            update_data\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úÖ Job details updated: {success_count} success, {failure_count} failed\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to update job details: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "print(\"‚úÖ Utility functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Configure Import Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure file loader for GCS/HTTP (matching productsImport.ipynb)\n",
        "file_loader_config = {}\n",
        "\n",
        "# If using GCS and need explicit credentials\n",
        "if file_url and (file_url.startswith(\"gs://\") or \"storage.googleapis.com\" in file_url):\n",
        "    if os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"):\n",
        "        file_loader_config[\"gcs_credentials_path\"] = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
        "        print(f\"‚úÖ GCS credentials configured\")\n",
        "\n",
        "# Create import pipeline configuration\n",
        "import_config = ImportPipelineConfig(\n",
        "    file_source=file_url,  # HTTP URL or gs:// URL\n",
        "    template_id=template_id,\n",
        "    saas_edge_id=saas_edge_id,\n",
        "    job_name=job_name,\n",
        "    batch_size=500,  # Process 500 rows per batch\n",
        "    max_workers=4,   # Use 4 concurrent workers\n",
        "    file_loader_config=file_loader_config,\n",
        "    db_client=db_client  # Real database client (not mock)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Import pipeline configured\")\n",
        "print(f\"  File source: {file_url}\")\n",
        "print(f\"  Template ID: {template_id}\")\n",
        "print(f\"  Batch size: {import_config.batch_size}\")\n",
        "print(f\"  Max workers: {import_config.max_workers}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Run Import Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the import pipeline (matching productsImport.ipynb pattern)\n",
        "print(f\"üöÄ Starting import job: {job_name}\")\n",
        "print(f\"   File: {file_name}\")\n",
        "print(f\"   Template: {template_id}\")\n",
        "print(f\"   Tenant: {saas_edge_id}\")\n",
        "print()\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "try:\n",
        "    # Execute import pipeline using SDK\n",
        "    results = await run_product_import(import_config)\n",
        "    \n",
        "    end_time = datetime.now()\n",
        "    duration = (end_time - start_time).total_seconds()\n",
        "    \n",
        "    # Extract results\n",
        "    pipeline_job_id = results.get(\"job_id\")\n",
        "    total_processed = results.get(\"total_processed\", 0)\n",
        "    total_errors = results.get(\"total_errors\", 0)\n",
        "    valid_rows = total_processed - total_errors\n",
        "    \n",
        "    print(\"‚úÖ Import pipeline completed successfully!\")\n",
        "    print(f\"\\nPipeline Results:\")\n",
        "    print(f\"  Job ID: {pipeline_job_id}\")\n",
        "    print(f\"  Total rows processed: {total_processed}\")\n",
        "    print(f\"  Valid rows: {valid_rows}\")\n",
        "    print(f\"  Error rows: {total_errors}\")\n",
        "    print(f\"  Duration: {duration:.2f} seconds\")\n",
        "    \n",
        "    if total_processed > 0:\n",
        "        throughput = total_processed / duration if duration > 0 else 0\n",
        "        print(f\"  Throughput: {throughput:.0f} rows/second\")\n",
        "    \n",
        "    # Store results for next cell\n",
        "    import_results = results\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Import pipeline failed: {e}\")\n",
        "    traceback.print_exc()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Write Products to Database and Handle Failed Items\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch valid records from completeness cache (matching productsImport.ipynb pattern)\n",
        "from saastify_edge.db import CompletenessReader\n",
        "import uuid\n",
        "\n",
        "completeness_reader = CompletenessReader(db_client=db_client)\n",
        "\n",
        "# Get valid records for this job\n",
        "pipeline_job_id = import_results.get(\"job_id\")\n",
        "valid_records = await db_client.query(\n",
        "    \"product_template_completeness\",\n",
        "    filters={\n",
        "        \"job_id\": pipeline_job_id,\n",
        "        \"is_valid\": True,\n",
        "        \"saas_edge_id\": saas_edge_id\n",
        "    }\n",
        ")\n",
        "\n",
        "# Get failed records\n",
        "failed_records = await db_client.query(\n",
        "    \"product_template_completeness\",\n",
        "    filters={\n",
        "        \"job_id\": pipeline_job_id,\n",
        "        \"is_valid\": False,\n",
        "        \"saas_edge_id\": saas_edge_id\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Found {len(valid_records)} valid records and {len(failed_records)} failed records\")\n",
        "\n",
        "# Upsert products to database\n",
        "upserted_count = 0\n",
        "created_count = 0\n",
        "updated_count = 0\n",
        "failed_items = []\n",
        "\n",
        "for record in valid_records:\n",
        "    try:\n",
        "        transformed_data = record.get(\"transformed_response\", {})\n",
        "        product_sku = transformed_data.get(\"sku\") or transformed_data.get(\"product_sku\")\n",
        "        \n",
        "        if not product_sku:\n",
        "            continue\n",
        "        \n",
        "        # Check if product exists\n",
        "        existing = await db_client.fetch_one(\n",
        "            \"SELECT product_id FROM products WHERE saas_edge_id = $1 AND sku = $2\",\n",
        "            saas_edge_id,\n",
        "            product_sku\n",
        "        )\n",
        "        \n",
        "        product_data = {\n",
        "            \"name\": transformed_data.get(\"name\") or transformed_data.get(\"product_title\") or transformed_data.get(\"title\"),\n",
        "            \"updated_at\": datetime.utcnow()\n",
        "        }\n",
        "        \n",
        "        # Add other transformed fields to product_data\n",
        "        for key, value in transformed_data.items():\n",
        "            if key not in [\"sku\", \"product_sku\", \"name\", \"product_title\", \"title\"]:\n",
        "                product_data[key] = value\n",
        "        \n",
        "        if existing:\n",
        "            # Update existing product\n",
        "            if feed_settings.get(\"update_products\", True):\n",
        "                await db_client.update(\n",
        "                    \"products\",\n",
        "                    {\"saas_edge_id\": saas_edge_id, \"sku\": product_sku},\n",
        "                    product_data\n",
        "                )\n",
        "                updated_count += 1\n",
        "        else:\n",
        "            # Create new product\n",
        "            if feed_settings.get(\"create_new_products\", True):\n",
        "                product_data[\"product_id\"] = str(uuid.uuid4())\n",
        "                product_data[\"saas_edge_id\"] = saas_edge_id\n",
        "                product_data[\"sku\"] = product_sku\n",
        "                product_data[\"created_at\"] = datetime.utcnow()\n",
        "                \n",
        "                await db_client.insert(\"products\", product_data)\n",
        "                created_count += 1\n",
        "        \n",
        "        upserted_count += 1\n",
        "        \n",
        "    except Exception as e:\n",
        "        if not feed_settings.get(\"skip_errors\", False):\n",
        "            raise\n",
        "        failed_items.append({\"record\": record.get(\"file_row_number\"), \"error\": str(e)})\n",
        "\n",
        "# Prepare failed items for upload\n",
        "failed_url = None\n",
        "if failed_records or failed_items:\n",
        "    try:\n",
        "        failed_data = []\n",
        "        for record in failed_records:\n",
        "            failed_data.append({\n",
        "                \"row_number\": record.get(\"file_row_number\"),\n",
        "                \"validation_errors\": record.get(\"validation_errors\"),\n",
        "                \"transformed_response\": record.get(\"transformed_response\")\n",
        "            })\n",
        "        failed_data.extend(failed_items)\n",
        "        \n",
        "        # Upload failed items to GCS\n",
        "        output_filename = get_output_filename(saas_edge_id, job_path, 'json')\n",
        "        success, failed_url = await upload_to_gcp_bucket(failed_data, output_filename, GCP_BUCKET_NAME)\n",
        "        \n",
        "        if success:\n",
        "            print(f\"‚úÖ Failed items uploaded to: {failed_url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Failed to upload failed items: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(f\"\\n‚úÖ Database write completed:\")\n",
        "print(f\"  Upserted: {upserted_count}\")\n",
        "print(f\"  Created: {created_count}\")\n",
        "print(f\"  Updated: {updated_count}\")\n",
        "print(f\"  Failed: {len(failed_records) + len(failed_items)}\")\n",
        "\n",
        "# Update job details in database\n",
        "await update_job_details(\n",
        "    job_id=job_id,\n",
        "    total_count=total_processed,\n",
        "    success_count=upserted_count,\n",
        "    failure_count=len(failed_records) + len(failed_items),\n",
        "    failed_url=failed_url\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Final Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate final summary (matching productsImport.ipynb pattern)\n",
        "summary = {\n",
        "    \"job_id\": job_id,\n",
        "    \"pipeline_job_id\": pipeline_job_id,\n",
        "    \"job_name\": job_name,\n",
        "    \"status\": \"completed\",\n",
        "    \"file_name\": file_name,\n",
        "    \"file_url\": file_url,\n",
        "    \"template_id\": template_id,\n",
        "    \"import_type\": import_type,\n",
        "    \"total_rows\": total_processed,\n",
        "    \"valid_rows\": valid_rows,\n",
        "    \"error_rows\": total_errors,\n",
        "    \"upserted_count\": upserted_count,\n",
        "    \"created_count\": created_count,\n",
        "    \"updated_count\": updated_count,\n",
        "    \"failed_url\": failed_url,\n",
        "    \"duration_seconds\": duration,\n",
        "    \"completed_at\": datetime.utcnow().isoformat()\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PRODUCTS IMPORT JOB SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "for key, value in summary.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# Store summary as JSON for papermill output\n",
        "summary_json = json.dumps(summary, indent=2)\n",
        "print(\"\\nSummary JSON:\")\n",
        "print(summary_json)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Disconnect from database\n",
        "await db_client.disconnect()\n",
        "print(\"‚úÖ Database connection closed\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
