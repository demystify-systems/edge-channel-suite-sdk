{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/demystify-systems/edge-channel-suite/blob/main/demo/HelloSlackDocker2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gr9gc7arDWqF",
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# saas_edge_id = \"8dddba20-2928-45b2-a26f-381515f9ea68\"  \n",
    "# slack_webhook_url = \"os.getenv(\"SLACK_WEBHOOK_URL\", \"\")\"\n",
    "# job_id=\"1255a3e5-db10-45a2-bb14-ef6a4f30c68e\"\n",
    "# saas_edge_id = \"8dddba20-2928-45b2-a26f-381515f9ea68\"\n",
    "# slack_webhook_url = \"os.getenv(\"SLACK_WEBHOOK_URL\", \"\")\"\n",
    "# job_id = \"42aeb20d-81e2-4769-8282-c109f29db7d1\"\n",
    "# saas_edge_id = \"2a504287-373c-4ef6-8ebd-1c48e3b1a1bb\"\n",
    "# slack_webhook_url = \"os.getenv(\"SLACK_WEBHOOK_URL\", \"\")\"\n",
    "# job_id = \"564199f8-bf56-45f4-a5ac-b14a9554ae94\"\n",
    "# saas_edge_id = \"0ce8b38e-9121-4a07-a12f-cbb00bb7e945\"\n",
    "# slack_webhook_url = \"os.getenv(\"SLACK_WEBHOOK_URL\", \"\")\"\n",
    "# job_id = \"bd253e0a-d7ce-48c7-946c-0c6e11469b2e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# saas_edge_id = \"0ce8b38e-9121-4a07-a12f-cbb00bb7e945\"\n",
    "# slack_webhook_url = \"os.getenv(\"SLACK_WEBHOOK_URL\", \"\")\"\n",
    "# job_id = \"6fe7ecda-0a65-4dfc-a787-4201af2a7a04\"\n",
    "# Parameters\n",
    "# saas_edge_id = \"0ce8b38e-9121-4a07-a12f-cbb00bb7e945\"\n",
    "# slack_webhook_url = \"os.getenv(\"SLACK_WEBHOOK_URL\", \"\")\"\n",
    "# job_id = \"de447320-5fb9-4dd3-aa7a-6ce59bb74566\"\n",
    "# saas_edge_id = \"f8fe3edf-9511-49c5-b9cf-02f55c4a851c\"\n",
    "# slack_webhook_url = \"os.getenv(\"SLACK_WEBHOOK_URL\", \"\")\"\n",
    "# job_id = \"31cf440f-3161-45e7-826e-ea4c658cbe4d\"\n",
    "\n",
    "\n",
    "# saas_edge_id = \"b6b2785c-5a16-4744-b726-4b18eadc2024\"\n",
    "# slack_webhook_url = \"os.getenv(\"SLACK_WEBHOOK_URL\", \"\")\"\n",
    "# job_id = \"62cd2c5a-fe14-498b-a41b-b7c47e6179a6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIb8HRHV6l_4"
   },
   "outputs": [],
   "source": [
    "!pip install pg8000 sqlalchemy pandas requests openpyxl zipfile36 psycopg2-binary --quiet\n",
    "!pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib google-cloud-storage python-git --quiet \n",
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import re\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from uuid import UUID\n",
    "import copy\n",
    "from traceback import print_exc\n",
    "\n",
    "# Third-party library imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from slack_sdk import WebhookClient\n",
    "import pytz\n",
    "from jsonschema import validate, ValidationError, SchemaError\n",
    "import sys\n",
    "# Google Cloud imports\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "# Database library imports\n",
    "import pg8000\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)  # Redirect logs to stdout for Cloud Run\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Redirect `print` to use logging\n",
    "print = lambda *args, **kwargs: logging.info(' '.join(map(str, args)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_user = os.getenv(\"DB_USER\", \"postgres\")                \n",
    "db_pass = os.getenv(\"DB_PASSWORD\", \"saasdbforwindmill2023\")\n",
    "db_name = os.getenv(\"DB_NAME\", \"catalog-edge-db\")\n",
    "\n",
    "\n",
    "# Database configuration\n",
    "db_user = os.getenv(\"DB_USER\", \"postgres\")\n",
    "db_pass = os.getenv(\"DB_PASSWORD\", \"saasdbforwindmill2023\")\n",
    "db_name = os.getenv(\"DB_NAME\", \"catalog-edge-db\")\n",
    "\n",
    "# Detect environment\n",
    "is_local = os.getenv(\"ENV\", \"product\") == \"local\"\n",
    "# is_local = True\n",
    "if is_local:\n",
    "    db_host = \"127.0.0.1\" \n",
    "else:\n",
    "    db_host = \"/cloudsql/saastify-base-wm:us-central1:saastify-pgdb-us\"  \n",
    "\n",
    "db_port = 5432\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"\n",
    "    Creates and returns a database connection using global configuration.\n",
    "    Handles both Unix socket and TCP connections.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if is_local:\n",
    "            # Local development - use TCP\n",
    "            connection = pg8000.connect(\n",
    "                user=db_user,\n",
    "                password=db_pass,\n",
    "                database=db_name,\n",
    "                host=db_host,\n",
    "                port=db_port\n",
    "            )\n",
    "        else:\n",
    "            # Cloud environment - use Unix socket\n",
    "            connection = pg8000.connect(\n",
    "                user=db_user,\n",
    "                password=db_pass,\n",
    "                database=db_name,\n",
    "                unix_sock=db_host + \"/.s.PGSQL.5432\"\n",
    "            )\n",
    "        return connection\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: Failed to establish database connection\")\n",
    "        print(f\"ERROR: {str(e)}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDK Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_BUCKET_NAME = \"edge-assets\"\n",
    "credentials = {\n",
    "  \"type\": \"service_account\",\n",
    "  \"project_id\": \"saastify-base-wm\",\n",
    "  \"private_key_id\": \"d3ed03651cb920c119a78e2434459a52d3b9f541\",\n",
    "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEuwIBADANBgkqhkiG9w0BAQEFAASCBKUwggShAgEAAoIBAQC+M2925iyNZ34w\\n5UeOyN3LDo+OncMeHEjyBGnx7xtcBBv9/4rmrYn3cLu6iD31jrnhIkcjju6K2Fdz\\nOgNaXqQkoIT7OGqREh5ouFZAQQJWoy153BiyL8eUcd5CzBTMDTW2MpaOo4Y2jOii\\nzM6w94+Ajw7KoWHOi4o8KuItM9IDw394j2kCLeUFe0k2PQ/JNAJa9uKQgOd7bmAj\\nPhlDJuLRQPvpFOw93MLgPRi8JkSF88HeB/AiT/7Awnh7xAtaj36Kmycub2vIEufh\\nnKZ/U+0DEqhfLHSo9EixY+PCJSZsxjgwchDQQgyGB75zcuZVX/ArwzopFyGYO/uK\\nJCXxTNE7AgMBAAECgf8WqQoeoE2uiyX9rVNZL5U9G/7+fs1ASR5ntx7oNBSOYe7z\\n0/44fXRyhnvXPWQkXVzH9c2D7wN8h0nj8IV1vtDPjFBLne0UW5RD5bJg9V3R9J72\\nZcKLeCXPCcHxM19G8Ev16REG7XSQCzmsK7p0Wwo9xs18Vr3QXc+aW4GW4RWkXPG6\\nhoWOD6If0NtHVRZZqXEtSqShepptSz5ezcNGIgW5ksWeciMqziQssCFVbCM6E88+\\nADPHiB1VgtQvd3jqbh9KoUHEPSSZ9Lcp54k8bRx/9hpB9l9AtlQP8lh0oVYdVbuX\\n5+Eko6jH1tF/YELobR4TXnUqZg15z5g1y8vHkfUCgYEA4iRRYBRUEMczWlvOW/PC\\nvyO6JmoQVBHhU8J/6PfG2v+9c+sCf+e9dn4HTuPnjdI8QLy9VC1NR3/wXqCCf/Ea\\nIXPitUq3Gsezl01Zp2m7i3y8d4HZNoan9tcHMdIQ7bm17ZBbN/RE3Hw2dx/O+kFY\\nGvDhTTrw+GLfGPGnfiBLBK0CgYEA11BMuIQ8+HVD5aqux0jb7HAHsDYz24v87Z9L\\nwocBNy0J2lAtpiQ+SFs/0g5cVOyqm9MlThBshgUDGDUaczfqjZaz13rWc+qPBAjD\\nawUf9QfTO53UQIZP1dgNuybXBEi1PqMfd+yeUkjIDfF7+ntfwDr/BdH6XJ9tFMeR\\nlmUEAocCgYEApxW6Ykjiy/rCgJKwZ9Q1IdCd62AWbGdBmwdsRo88B/dI3WrYT/TD\\nUddQQwO0xF5/Uj2hjZ5jKN7olKH3idx0OB9NdDGeFFVU5geqpD1E6ozhG1N/UAAx\\n/flmQXM6OssqFjrAixkZ/+Zuv5lq7hB1roInlU5lWMCEogN6g4AMrYkCgYAI6ckT\\nRl4jxu71nfg4Rbrc8dJPqB7DcusYhySiu+X/+7xRrkoFe7CcXDKrJm8KEPYLF1WP\\nAr0LWz/Ci8g5htIN5HQzcmFYURh0iUxVrNOi2B0VdbYoqaa6aoQ/AB+cjMn7+tK9\\nqyzuqRanBR0lxF+1XHvcKNIdbXgdiRlsyWe+FwKBgHDn/KAFQ5yHEPUC5IUU8ktB\\no9W5eS33NGnsVZ6PKqncyGD2WUdpqdQeMSYQb/OSxULr49HVM71lrhiyqGHMsgb0\\nY/K62jVmlhnhDQ27F36rBojyidQPA+NxDr/8jgS78hktnydzh2j6cnMWGm3tJpmY\\nivU2NdIrfBP4gzRJXlzm\\n-----END PRIVATE KEY-----\\n\",\n",
    "  \"client_email\": \"wm-bulk-job-mgr@saastify-base-wm.iam.gserviceaccount.com\",\n",
    "  \"client_id\": \"104148745789000738235\",\n",
    "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/wm-bulk-job-mgr%40saastify-base-wm.iam.gserviceaccount.com\",\n",
    "  \"universe_domain\": \"googleapis.com\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this after the database configuration section (in[3])\n",
    "\n",
    "# def get_db_connection():\n",
    "#     \"\"\"\n",
    "#     Creates and returns a database connection using global configuration.\n",
    "#     Handles both Unix socket and TCP connections.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Determine if we're using Unix socket or TCP\n",
    "#         if db_host.startswith('/cloudsql/'):\n",
    "#             connection = pg8000.connect(\n",
    "#                 user=db_user,\n",
    "#                 password=db_pass,\n",
    "#                 database=db_name,\n",
    "#                 unix_sock=db_host + \"/.s.PGSQL.5432\"\n",
    "#             )\n",
    "#         else:\n",
    "#             connection = pg8000.connect(\n",
    "#                 user=db_user,\n",
    "#                 password=db_pass,\n",
    "#                 database=db_name,\n",
    "#                 host=db_host,\n",
    "#                 port=db_port\n",
    "#             )\n",
    "#         return connection\n",
    "#     except Exception as e:\n",
    "#         print(\"ERROR: Failed to establish database connection\")\n",
    "#         print(f\"ERROR: {str(e)}\")\n",
    "#         print(f\"ERROR: {traceback.format_exc()}\")\n",
    "#         raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_url(url, file_name=\"\"):\n",
    "    get_response = requests.get(url, stream=True)\n",
    "    if file_name == \"\":\n",
    "        file_name = url.split(\"/\")[-1]\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        for chunk in get_response.iter_content(chunk_size=1024):\n",
    "            if chunk:  # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "    return file_name\n",
    "\n",
    "class FileParser(object):\n",
    "    def load(self, url):\n",
    "        self.url = url\n",
    "        self.file_name = download_url(url)\n",
    "        self.file_type = self.file_name.split(\".\")[-1]\n",
    "        print(\"The URL file type is : \", self.file_type)\n",
    "        method_name = \"parse_\" + self.file_type\n",
    "        method = getattr(self, method_name, lambda: \"Invalid\")\n",
    "        return method()\n",
    "\n",
    "    def infer_schema(self):\n",
    "        self.df.info()\n",
    "        self.columns = list(self.df.columns.values.tolist())\n",
    "        print(\"List of all columns are : \", self.columns)\n",
    "        print(\"##### Pandas inferred Schema\")\n",
    "        pandas_schema = self.df.columns.to_series().groupby(self.df.dtypes).groups\n",
    "        print(pandas_schema)\n",
    "\n",
    "    def parse_xlsx(self):\n",
    "        return self.parse_excel()\n",
    "\n",
    "    def parse_xlsm(self):\n",
    "        return self.parse_excel()\n",
    "\n",
    "    def parse_xls(self):\n",
    "        return self.parse_excel()\n",
    "\n",
    "    def parse_csv(self):\n",
    "        df = pd.read_csv(self.file_name, sep=\",\", header=0)\n",
    "        return df\n",
    "\n",
    "    def parse_zip(self):\n",
    "        with zipfile.ZipFile(self.file_name, \"r\") as zip_ref:\n",
    "            zip_ref.extractall()\n",
    "            extracted_files = zip_ref.namelist()\n",
    "            print(extracted_files)\n",
    "            return extracted_files\n",
    "\n",
    "    def parse_tsv(self):\n",
    "        df = pd.read_csv(self.file_name, sep=\"\\t\", header=0)\n",
    "        return df\n",
    "\n",
    "    def parse_json(self):\n",
    "        df = pd.read_json(self.file_name)\n",
    "        return df\n",
    "\n",
    "    def parse_txt(self):\n",
    "        df = pd.read_csv(self.file_name, sep=\" \")\n",
    "        return df\n",
    "\n",
    "    def parse_excel(self):\n",
    "        try:\n",
    "            xls = pd.ExcelFile(self.file_name)\n",
    "            # Read the first sheet by default\n",
    "            sheet_name = xls.sheet_names[0]\n",
    "            df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to parse Excel file: {str(e)}\")\n",
    "            print(f\"ERROR: {traceback.format_exc()}\")\n",
    "            return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_slack_message(message, webhook_url):\n",
    "    try:\n",
    "        print(\"INFO: Attempting to send a message to Slack...\")\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        payload = {\"text\": message}\n",
    "        response = requests.post(webhook_url, data=json.dumps(payload), headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"INFO: Slack message sent successfully.\")\n",
    "        else:\n",
    "            print(\"WARNING: Failed to send Slack message.\")\n",
    "            print(f\"WARNING: Status code: {response.status_code}\")\n",
    "            print(f\"WARNING: Response: {response.text}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: An error occurred while sending a Slack message.\")\n",
    "        print(f\"ERROR: {e}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database configuration - credentials should ideally be stored in environment variables for security\n",
    "\n",
    "\n",
    "def fetch_saas_edge_jobs_details(job_id, saas_edge_id):\n",
    "    try:\n",
    "        print(\"INFO: Attempting to connect to Cloud SQL instance...\")\n",
    "        \n",
    "        # Connect to the Cloud SQL instance\n",
    "        conn = get_db_connection()\n",
    "        print(\"INFO: Connected to Cloud SQL successfully.\")\n",
    "\n",
    "        # Define the query\n",
    "        query = \"\"\"\n",
    "        SELECT * FROM public.saas_edge_jobs\n",
    "        WHERE request_id = %s AND saas_edge_id = %s;\n",
    "        \"\"\"\n",
    "        print(f\"INFO: Executing query: {query} with job_id: {job_id} and saas_edge_id: {saas_edge_id}\")\n",
    "\n",
    "        # Execute the query\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query, (job_id, saas_edge_id))\n",
    "        result = cursor.fetchall()\n",
    "\n",
    "        if not result:\n",
    "            print(f\"WARNING: No results found for job_id: {job_id} and saas_edge_id: {saas_edge_id}\")\n",
    "        else:\n",
    "            print(f\"INFO: Query executed successfully. Number of records fetched: {len(result)}\")\n",
    "\n",
    "        # Process and convert datetime and UUID objects to strings for JSON serialization\n",
    "        row_data = [\n",
    "            {k: (v.isoformat() if isinstance(v, datetime) else str(v) if isinstance(v, UUID) else v) \n",
    "             for k, v in zip([col[0] for col in cursor.description], row)}\n",
    "            for row in result\n",
    "        ]\n",
    "        \n",
    "        print(\"INFO: Data processed for JSON serialization.\")\n",
    "\n",
    "        # Close the connection\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"INFO: Database connection closed.\")\n",
    "        \n",
    "        return row_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: An error occurred while connecting to Cloud SQL or executing the query.\")\n",
    "        print(f\"ERROR: {e}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")  # Full stack trace for debugging\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_channel_attr_template(saas_edge_id, template_id):\n",
    "    try:\n",
    "        print(\"INFO: Attempting to connect to Cloud SQL instance...\")\n",
    "        \n",
    "        conn = get_db_connection()\n",
    "        print(\"INFO: Connected to Cloud SQL successfully.\")\n",
    "\n",
    "        query = \"\"\"\n",
    "        SELECT * FROM public.saas_channel_attr_template\n",
    "        WHERE saas_edge_id = %s AND template_id = %s;\n",
    "        \"\"\"\n",
    "        print(f\"INFO: Executing query for saas_edge_id: {saas_edge_id} and template_id: {template_id}\")\n",
    "\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query, (saas_edge_id, template_id))\n",
    "        result = cursor.fetchall()\n",
    "\n",
    "        if not result:\n",
    "            print(f\"WARNING: No results found for saas_edge_id: {saas_edge_id} and template_id: {template_id}\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"INFO: Query executed successfully. Number of records fetched: {len(result)}\")\n",
    "\n",
    "        # Process results\n",
    "        row_data = [\n",
    "            {k: (v.isoformat() if isinstance(v, datetime) else str(v) if isinstance(v, UUID) else v) \n",
    "             for k, v in zip([col[0] for col in cursor.description], row)}\n",
    "            for row in result\n",
    "        ]\n",
    "        \n",
    "        print(\"INFO: Data processed for JSON serialization.\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"INFO: Database connection closed.\")\n",
    "        \n",
    "        return row_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: An error occurred while fetching channel attribute template data.\")\n",
    "        print(f\"ERROR: {e}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_saas_channel_templates(saas_edge_id, template_id):\n",
    "    try:\n",
    "        print(\"INFO: Attempting to connect to Cloud SQL instance...\")\n",
    "        \n",
    "        conn = get_db_connection()\n",
    "        print(\"INFO: Connected to Cloud SQL successfully.\")\n",
    "\n",
    "        query = \"\"\"\n",
    "        SELECT * FROM public.saas_channel_templates\n",
    "        WHERE saas_edge_id = %s AND template_id = %s;\n",
    "        \"\"\"\n",
    "        print(f\"INFO: Executing query for saas_edge_id: {saas_edge_id} and template_id: {template_id}\")\n",
    "\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query, (saas_edge_id, template_id))\n",
    "        result = cursor.fetchall()\n",
    "\n",
    "        if not result:\n",
    "            print(f\"WARNING: No results found for saas_edge_id: {saas_edge_id} and template_id: {template_id}\")\n",
    "            return None\n",
    "\n",
    "        print(f\"INFO: Query executed successfully. Number of records fetched: {len(result)}\")\n",
    "\n",
    "        # Process results and handle datetime/UUID serialization\n",
    "        row_data = [\n",
    "            {k: (v.isoformat() if isinstance(v, datetime) else str(v) if isinstance(v, UUID) else v) \n",
    "             for k, v in zip([col[0] for col in cursor.description], row)}\n",
    "            for row in result\n",
    "        ]\n",
    "        \n",
    "        print(\"INFO: Data processed for JSON serialization.\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"INFO: Database connection closed.\")\n",
    "        \n",
    "        return row_data[0] if row_data else None  # Return first row since template_id should be unique\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: An error occurred while fetching channel template data.\")\n",
    "        print(f\"ERROR: {e}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_job_details(job_id, saas_edge_id):\n",
    "    # Main execution code\n",
    "    try:\n",
    "        job_id = job_id  # Replace with the actual job_id you want to query\n",
    "        saas_edge_id = saas_edge_id  # Replace with the actual saas_edge_id you want to query\n",
    "\n",
    "        # Fetch data from the database\n",
    "        print(f\"INFO: Fetching details for job_id: {job_id} and saas_edge_id: {saas_edge_id}\")\n",
    "        row_data = fetch_saas_edge_jobs_details(job_id, saas_edge_id)\n",
    "\n",
    "        if not row_data:\n",
    "            print(\"WARNING: No data retrieved or an error occurred during the fetch operation.\")\n",
    "\n",
    "    except Exception as main_e:\n",
    "        print(\"ERROR: An unexpected error occurred in the main execution block.\")\n",
    "        print(f\"ERROR: {main_e}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")  # Full stack trace for debugging\n",
    "\n",
    "\n",
    "    request_args = row_data[0].get(\"request_args\",{})\n",
    "\n",
    "    print(request_args)\n",
    "\n",
    "    return request_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usage example:\n",
    "# try:\n",
    "#     template_id = request_args.get(\"template_id\")\n",
    "#     if not template_id:\n",
    "#         raise ValueError(\"template_id not found in request_args\")\n",
    "        \n",
    "#     template_data_function  = fetch_channel_attr_template(saas_edge_id, template_id)\n",
    "#     template_details = fetch_saas_channel_templates(saas_edge_id, template_id)  \n",
    "#     # print(template_data)\n",
    "#     if not template_data_function:\n",
    "#         print(\"WARNING: No template data retrieved or an error occurred during the fetch operation.\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(\"ERROR: An unexpected error occurred while fetching template data.\")\n",
    "#     print(f\"ERROR: {e}\")\n",
    "#     print(f\"ERROR: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO Support for multiple rules\n",
    "\n",
    "# def apply_transformation(value, transformation_rules: str):\n",
    "#     \"\"\"\n",
    "#     Apply multiple transformation rules to a value with comprehensive error handling.\n",
    "#     Rules are separated by |;| character sequence.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Handle None/empty values\n",
    "#         if value is None or transformation_rules is None:\n",
    "#             return value\n",
    "            \n",
    "#         # Ensure transformation_rules is a string\n",
    "#         if not isinstance(transformation_rules, str):\n",
    "#             print(f\"WARNING: Invalid transformation rule type: {type(transformation_rules)}\")\n",
    "#             return value\n",
    "        \n",
    "#         # Normalize backslashes in the input value if it's a string\n",
    "#         if isinstance(value, str):\n",
    "#             transformed_value = value.replace('\\\\', '/')\n",
    "#         else:\n",
    "#             transformed_value = value\n",
    "            \n",
    "#         # Split rules by |;| character sequence and process each rule in sequence\n",
    "#         rules = [rule.strip() for rule in transformation_rules.split('|;|')]\n",
    "#         for rule in rules:\n",
    "#             # Basic string transformations\n",
    "#             if rule == \"uppercase\":\n",
    "#                 transformed_value = transformed_value.upper() if isinstance(transformed_value, str) else transformed_value\n",
    "#             elif rule == \"lowercase\":\n",
    "#                 transformed_value = transformed_value.lower() if isinstance(transformed_value, str) else transformed_value\n",
    "#             elif rule == \"strip\":\n",
    "#                 transformed_value = transformed_value.strip() if isinstance(transformed_value, str) else transformed_value\n",
    "#             elif rule == \"split_comma\":\n",
    "#                 if not isinstance(transformed_value, str):\n",
    "#                     continue\n",
    "#                 # Handle potential backslashes in comma-separated values\n",
    "#                 transformed_value = [item.strip().replace('\\\\', '/') \n",
    "#                                    for item in transformed_value.split(\",\") \n",
    "#                                    if item.strip()]\n",
    "                \n",
    "#             elif rule.startswith(\"set|||\"):\n",
    "#                 try:\n",
    "#                     parts = rule.split(\"|||\")\n",
    "#                     if len(parts) != 2:\n",
    "#                         print(f\"WARNING: Invalid set rule format: {rule}. Expected 2 parts.\")\n",
    "#                         continue\n",
    "#                     _, value = parts\n",
    "#                     transformed_value = value\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"WARNING: Error processing set rule: {rule}. Error: {str(e)}\")\n",
    "#                     continue\n",
    "                \n",
    "#             # Complex transformations\n",
    "#             elif rule.startswith(\"replace|||\"):\n",
    "#                 if not isinstance(transformed_value, str):\n",
    "#                     continue\n",
    "#                 try:\n",
    "#                     # Split using ||| delimiter\n",
    "#                     parts = rule.split(\"|||\")\n",
    "#                     if len(parts) != 3:\n",
    "#                         print(f\"WARNING: Invalid replace rule format: {rule}. Expected 3 parts.\")\n",
    "#                         continue\n",
    "                    \n",
    "#                     _, _, replace_rule = parts\n",
    "#                     original, new = replace_rule.split(\"||\", 1)\n",
    "                    \n",
    "#                     original = original.strip().replace('\\\\', '/')\n",
    "#                     new = new.strip().replace('\\\\', '/')\n",
    "#                     original = original.strip().replace('\\\\', '/')\n",
    "#                     new = new.strip().replace('\\\\', '/')\n",
    "#                     transformed_value = transformed_value.replace(original, new)\n",
    "                    \n",
    "#                 except (IndexError, ValueError) as e:\n",
    "#                     print(f\"WARNING: Invalid replace rule format: {rule}. Error: {str(e)}\")\n",
    "#                     continue\n",
    "                    \n",
    "#             elif rule == \"vlookup:map\":\n",
    "#                 if not isinstance(transformed_value, str):\n",
    "#                     continue\n",
    "#                 mapping = {\n",
    "#                     \"yes\": True,\n",
    "#                     \"no\": False,\n",
    "#                     \"true\": True,\n",
    "#                     \"false\": False,\n",
    "#                     \"1\": True,\n",
    "#                     \"0\": False,\n",
    "#                     \"y\": True,\n",
    "#                     \"n\": False\n",
    "#                 }\n",
    "#                 # Convert to lowercase and strip before mapping\n",
    "#                 lookup_value = transformed_value.lower().strip()\n",
    "#                 transformed_value = mapping.get(lookup_value, transformed_value)\n",
    "            \n",
    "#             # New transformation rule for adjusting negative numbers to zero\n",
    "#             elif rule == \"adjust_negative_to_zero\":\n",
    "#                 print(\"transformed_value\",transformed_value)\n",
    "#                 try:\n",
    "#                     if isinstance(transformed_value, (int, float)):\n",
    "#                         transformed_value = max(0, transformed_value)\n",
    "#                     elif isinstance(transformed_value, list):\n",
    "#                         transformed_value = [max(0, float(val)) if isinstance(val, (int, float, str)) else val \n",
    "#                                           for val in transformed_value]\n",
    "#                     elif isinstance(transformed_value, str):\n",
    "#                         try:\n",
    "#                             num_value = float(transformed_value)\n",
    "#                             transformed_value = str(max(0, num_value))\n",
    "#                         except ValueError:\n",
    "#                             print(f\"WARNING: Could not convert string '{transformed_value}' to number for negative adjustment\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"WARNING: Error applying negative adjustment: {str(e)}\")\n",
    "#                     continue\n",
    "            \n",
    "#             else:\n",
    "#                 print(f\"WARNING: Unknown transformation rule: {rule}\")\n",
    "#                 continue\n",
    "        \n",
    "#         return transformed_value\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"ERROR: Transformation failed for value '{value}' with rules '{transformation_rules}': {str(e)}\")\n",
    "#         return value  # Return original value on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #example code for transformer\n",
    "\n",
    "# def build_request_objects(data_df, filtered_template_data):\n",
    "#     request_objects = []\n",
    "#     # print(\"data_df\",data_df)\n",
    "    \n",
    "#     try:\n",
    "#         # Iterate through each row in the dataframe\n",
    "#         for index, row in data_df.iterrows():\n",
    "#             request_obj = {}\n",
    "            \n",
    "#             # Convert row to dictionary and handle NaN values\n",
    "#             row_dict = {\n",
    "#                 k: (None if pd.isna(v) else v) \n",
    "#                 for k, v in row.to_dict().items()\n",
    "#             }\n",
    "            \n",
    "#             # Process all fields with their mappings\n",
    "#             for col, field_config in filtered_template_data.items():\n",
    "#                 if not field_config or not field_config.get('field_mapping'):\n",
    "#                     continue\n",
    "                    \n",
    "#                 value = row_dict.get(col)\n",
    "#                 if value is None:\n",
    "#                     continue\n",
    "                \n",
    "#                 field_mapping = field_config['field_mapping']\n",
    "#                 accepts_multiple = field_config.get('accepts_multiple_values', False)\n",
    "#                 transformation_function = field_config.get(\"transformation_function\", \"\")\n",
    "                \n",
    "#                 # trasnformation needs to be applied to value\n",
    "#                 if transformation_function:\n",
    "#                     value = apply_transformation(value, transformation_function)\n",
    "                \n",
    "#                 # Handle nested mappings (e.g., \"raw_product_data.field_name\")\n",
    "#                 if \".\" in field_mapping:\n",
    "#                     parent_key, child_key = field_mapping.split(\".\", 1)\n",
    "                    \n",
    "#                     # Initialize parent dictionary if it doesn't exist\n",
    "#                     if parent_key not in request_obj:\n",
    "#                         request_obj[parent_key] = {}\n",
    "                        \n",
    "#                     request_obj[parent_key][child_key] = value.strip() if isinstance(value, str) else value\n",
    "#                 else:\n",
    "#                     # Handle non-nested fields\n",
    "#                     if accepts_multiple and isinstance(value, str):\n",
    "#                         request_obj[field_mapping] = [v.strip() for v in value.split(',') if v.strip()]\n",
    "#                     else:\n",
    "#                         request_obj[field_mapping] = value.strip() if isinstance(value, str) else value\n",
    "            \n",
    "#             request_objects.append(request_obj)\n",
    "            \n",
    "#         print(f\"INFO: Successfully created {len(request_objects)} request objects\")\n",
    "#         return request_objects\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"ERROR: Failed to build request objects: {str(e)}\")\n",
    "#         print(f\"ERROR: {traceback.format_exc()}\")\n",
    "#         return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload a file to GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Upload files to gcp\n",
    "def sanitize_path(path):\n",
    "    \"\"\"\n",
    "    Sanitize path by replacing forward slashes with underscores\n",
    "    \"\"\"\n",
    "    return path.replace('/', '_').replace(\".ipynb\",\"\").strip('')\n",
    "\n",
    "def get_output_filename(saas_edge_id, job_path,file_type):\n",
    "    \"\"\"\n",
    "    Generate standardized output filename with proper date formatting\n",
    "    \"\"\"\n",
    "    try:\n",
    "        current_date = datetime.now()\n",
    "        date_str = current_date.strftime('%Y-%m-%d')\n",
    "        timestamp_str = current_date.strftime('%H%M%S')\n",
    "        sanitized_job_path = sanitize_path(job_path)\n",
    "        \n",
    "        return f\"{saas_edge_id}/catalog-edge/export-job-reports/{sanitized_job_path}/{date_str}/export-file-{timestamp_str}.{file_type}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating output filename: {str(e)}\")\n",
    "        # Fallback filename if something goes wrong\n",
    "        return f\"{saas_edge_id}/catalog-edge/job-reports/error_report.json\"\n",
    "\n",
    "def get_output_filename_failed(saas_edge_id, job_path,file_type):\n",
    "    \"\"\"\n",
    "    Generate standardized output filename with proper date formatting\n",
    "    \"\"\"\n",
    "    try:\n",
    "        current_date = datetime.now()\n",
    "        date_str = current_date.strftime('%Y-%m-%d')\n",
    "        timestamp_str = current_date.strftime('%H%M%S')\n",
    "        sanitized_job_path = sanitize_path(job_path)\n",
    "        \n",
    "        return f\"{saas_edge_id}/catalog-edge/export-job-reports/{sanitized_job_path}/{date_str}/export-file-failed{timestamp_str}.{file_type}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating output filename: {str(e)}\")\n",
    "        # Fallback filename if something goes wrong\n",
    "        return f\"{saas_edge_id}/catalog-edge/job-reports/failed_report.json\"\n",
    "def upload_to_gcp_bucket(file_data, file_name, bucket_name, base_path=\"\", credentials=None):\n",
    "    \"\"\"\n",
    "    Generic function to upload data to GCP bucket using existing credentials\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create credentials object from the service account info\n",
    "        gcp_credentials = service_account.Credentials.from_service_account_info(credentials)\n",
    "        storage_client = storage.Client(credentials=gcp_credentials)\n",
    "        \n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        \n",
    "        # Combine base path with file name\n",
    "        full_path = f\"{base_path.rstrip('/')}/{file_name}\" if base_path else file_name\n",
    "        blob = bucket.blob(full_path)\n",
    "        \n",
    "        # Add retry logic for upload\n",
    "        retry_count = 0\n",
    "        max_retries = 3\n",
    "        \n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                # Check if file_data is a file path\n",
    "                if isinstance(file_data, str) and os.path.isfile(file_data):\n",
    "                    with open(file_data, 'rb') as file:\n",
    "                        blob.upload_from_file(file)\n",
    "                elif isinstance(file_data, bytes):\n",
    "                    blob.upload_from_string(file_data, content_type='application/octet-stream')\n",
    "                else:\n",
    "                    # For file-like objects\n",
    "                    blob.upload_from_file(file_data)\n",
    "                    \n",
    "                # Generate public URL\n",
    "                url = f\"https://storage.googleapis.com/{bucket_name}/{full_path}\"\n",
    "                print(f\"Successfully uploaded {file_name} to {url}\")\n",
    "                return True, url\n",
    "                \n",
    "            except Exception as upload_error:\n",
    "                retry_count += 1\n",
    "                if retry_count == max_retries:\n",
    "                    print(f\"Failed to upload {file_name} after {max_retries} attempts: {str(upload_error)}\")\n",
    "                    return False, \"\"\n",
    "                print(f\"Retry {retry_count} for {file_name}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to GCP bucket: {str(e)}\")\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return False, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_json_to_gcp_bucket( job_path,json_data,gcp_bucket_name =GCP_BUCKET_NAME, creds=credentials):\n",
    "\n",
    "    # File upload Invocation \n",
    "    try:\n",
    "        output_filename = get_output_filename(\n",
    "            saas_edge_id=saas_edge_id,\n",
    "            job_path=job_path\n",
    "        )\n",
    "        \n",
    "        success, url = upload_to_gcp_bucket(\n",
    "            file_data=json_data,\n",
    "            file_name=output_filename,\n",
    "            bucket_name=gcp_bucket_name,\n",
    "            credentials=creds\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            print(f\"Failed items log uploaded to: {url}\")\n",
    "            failed_url = url\n",
    "        else:\n",
    "            print(\"ERROR: Failed to upload failed items log to GCP\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to process failed items: {str(e)}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update job details\n",
    "def update_job_details(job_id, job_response, failed_url=None, success_url=None):\n",
    "    \"\"\"\n",
    "    Update job details in the database with success/failure counts and failed job summary link\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        update_fields = [\"job_response = %s\"]\n",
    "        update_values = [json.dumps(job_response)]\n",
    "\n",
    "        # Add failed_job_summary_link if present\n",
    "        if failed_url:\n",
    "            update_fields.append(\"failed_job_summary_link = %s\")\n",
    "            update_values.append(failed_url)\n",
    "        if success_url:\n",
    "            update_fields.append(\"success_job_summary_link = %s\")\n",
    "            update_values.append(success_url)\n",
    "\n",
    "        # Add job_id for WHERE clause\n",
    "        update_values.append(job_id)\n",
    "        \n",
    "\n",
    "        query = f\"\"\"\n",
    "        UPDATE saas_edge_jobs\n",
    "        SET {\", \".join(update_fields)}\n",
    "        WHERE request_id = %s\n",
    "        RETURNING *;\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(query, update_values)\n",
    "        updated_row = cursor.fetchone()\n",
    "        conn.commit()\n",
    "\n",
    "        \n",
    "        return updated_row\n",
    "\n",
    "    except Exception as e:\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "        print(f\"ERROR: Failed to update job details: {str(e)}\")\n",
    "        print(f\"ERROR: {traceback.format_exc()}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_args = get_job_details(job_id, saas_edge_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_path = request_args.get(\"job_path\", \"\")\n",
    "filter_params = request_args.get('filter_params', {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the database configuration\n",
    "db_config = {\n",
    "    # \"host\":\"localhost\",\n",
    "    \"host\": \"/cloudsql/saastify-base-wm:us-central1:saastify-pgdb-us\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"catalog-edge-db\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"saasdbforwindmill2023\"\n",
    "}\n",
    "\n",
    "\n",
    "# Function to construct the PostgreSQL connection string\n",
    "def create_connection_string() -> str:\n",
    "    try:\n",
    "        connection_string = (\n",
    "            f\"dbname={db_config['database']} \"\n",
    "            f\"user={db_config['user']} \"\n",
    "            f\"password={db_config['password']} \"\n",
    "            f\"host={db_config['host']} \"\n",
    "            f\"port={db_config['port']} \"\n",
    "        )\n",
    "        return connection_string\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing key in DB configuration: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating connection string: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Helper function to convert datetime and decimal objects to serializable format\n",
    "def custom_serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    elif isinstance(obj, Decimal):\n",
    "        return float(obj)\n",
    "    raise TypeError(f\"Type {obj.__class__.__name__} not serializable\")\n",
    "\n",
    "\n",
    "# Function to fetch template attributes\n",
    "def fetch_template_attributes(\n",
    "    saas_edge_id: str, template_id: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    connection_string = create_connection_string()\n",
    "    connection = None\n",
    "    try:\n",
    "        # Establish the connection\n",
    "        connection = psycopg2.connect(connection_string, cursor_factory=RealDictCursor)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Prepare the query to fetch attributes for the given saas_edge_id and template_id\n",
    "        query = \"\"\"\n",
    "            SELECT * FROM saas_channel_attr_template \n",
    "            WHERE saas_edge_id = %s AND template_id = %s \n",
    "            ORDER BY column_pos ASC NULLS LAST, column_name ASC\n",
    "        \"\"\"\n",
    "        print(\"Executing query:\", query)\n",
    "\n",
    "        # Execute the query with saas_edge_id and template_id as parameters\n",
    "        cursor.execute(query, (saas_edge_id, template_id))\n",
    "        response_data = cursor.fetchall()\n",
    "\n",
    "        # If no data is fetched, handle gracefully\n",
    "        if not response_data:\n",
    "            print(\n",
    "                f\"No template attributes found for saas_edge_id: {saas_edge_id} and template_id: {template_id}\"\n",
    "            )\n",
    "            return []\n",
    "\n",
    "        print(\"Number of rows fetched:\", len(response_data))\n",
    "        return response_data\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching template attributes: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "\n",
    "# Function to generate JSON Schema dynamically, including hierarchy support\n",
    "# Also generates the order array based on column_pos or alphabetic order\n",
    "\n",
    "\n",
    "def generate_json_schema(template_attrs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    try:\n",
    "        # Base schema\n",
    "        schema = {\n",
    "            \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": [],\n",
    "            \"order\": [],\n",
    "        }\n",
    "\n",
    "        # Helper to handle hierarchical properties\n",
    "        def add_property(\n",
    "            properties: Dict[str, Any],\n",
    "            attr: Dict[str, Any],\n",
    "            parent_property: Optional[str] = None,\n",
    "        ):\n",
    "            column_name = attr[\"column_name\"]\n",
    "            data_type = (attr.get(\"data_type\") or \"text\").lower()\n",
    "            data_type_format = attr.get(\"data_type_format\")\n",
    "\n",
    "            # Map data_type to JSON schema types\n",
    "            json_type_mapping = {\n",
    "                \"text\": \"string\",\n",
    "                \"number\": \"number\",\n",
    "                \"integer\": \"integer\",\n",
    "                \"boolean\": \"boolean\",\n",
    "                \"array\": \"array\",\n",
    "                \"object\": \"object\",\n",
    "                \"decimal\": \"number\",\n",
    "                \"timestamp\": \"string\",\n",
    "                \"url\": \"string\",\n",
    "                \"date\": \"string\",\n",
    "                \"datetime\": \"string\",\n",
    "                \"time\": \"string\",\n",
    "            }\n",
    "            field_type = json_type_mapping.get(data_type, \"string\")\n",
    "\n",
    "            # Define the property schema\n",
    "            property_schema = {\n",
    "                \"title\": attr.get(\"custom_title\", column_name),\n",
    "                \"type\": field_type,\n",
    "                \"description\": attr.get(\"description\", f\"Field for {column_name}\"),\n",
    "                \"default\": attr.get(\"default_value\"),\n",
    "                \"nullable\": attr.get(\"is_nullable\", True),\n",
    "            }\n",
    "\n",
    "            # Handle data_type_format if available\n",
    "            if data_type_format:\n",
    "                property_schema[\"format\"] = data_type_format\n",
    "\n",
    "            # Handle multi-valued fields (array)\n",
    "            if attr.get(\"accepts_multiple_values\", False):\n",
    "                property_schema = {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": field_type},\n",
    "                    \"description\": f\"Array of {data_type} values for {column_name}\",\n",
    "                }\n",
    "\n",
    "            # Add validation rules\n",
    "            validation_rules = attr.get(\"validation_rules\", {})\n",
    "            if validation_rules:\n",
    "                if \"minLength\" in validation_rules:\n",
    "                    property_schema[\"minLength\"] = validation_rules[\"minLength\"]\n",
    "                if \"maxLength\" in validation_rules:\n",
    "                    property_schema[\"maxLength\"] = validation_rules[\"maxLength\"]\n",
    "                if \"pattern\" in validation_rules:\n",
    "                    property_schema[\"pattern\"] = validation_rules[\"pattern\"]\n",
    "                if \"enum\" in validation_rules:\n",
    "                    property_schema[\"enum\"] = validation_rules[\"enum\"]\n",
    "\n",
    "            # Add to the appropriate place in the properties structure\n",
    "            if parent_property:\n",
    "                if \"properties\" not in properties[parent_property]:\n",
    "                    properties[parent_property][\"properties\"] = {}\n",
    "                properties[parent_property][\"properties\"][column_name] = property_schema\n",
    "            else:\n",
    "                properties[column_name] = property_schema\n",
    "\n",
    "            # Add to required list if mandatory\n",
    "            if attr.get(\"mandatory\", False):\n",
    "                if parent_property:\n",
    "                    if \"required\" not in properties[parent_property]:\n",
    "                        properties[parent_property][\"required\"] = []\n",
    "                    properties[parent_property][\"required\"].append(column_name)\n",
    "                else:\n",
    "                    schema[\"required\"].append(column_name)\n",
    "\n",
    "            # Add to order list\n",
    "            if not parent_property:\n",
    "                schema[\"order\"].append(column_name)\n",
    "\n",
    "        # Process attributes considering hierarchy\n",
    "        for attr in template_attrs:\n",
    "            parent_id = attr.get(\"parent_id\")\n",
    "            if parent_id:\n",
    "                # Find the parent property in existing attributes\n",
    "                parent_attr = next(\n",
    "                    (a for a in template_attrs if a[\"template_attr_id\"] == parent_id),\n",
    "                    None,\n",
    "                )\n",
    "                if parent_attr:\n",
    "                    add_property(schema[\"properties\"], attr, parent_attr[\"column_name\"])\n",
    "            else:\n",
    "                add_property(schema[\"properties\"], attr)\n",
    "\n",
    "        return schema\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating JSON schema: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Function to insert or update the generated schema in the database\n",
    "def insert_or_update_schema(\n",
    "    saas_edge_id: str, template_id: str, schema: Dict[str, Any]\n",
    ") -> None:\n",
    "    connection_string = create_connection_string()\n",
    "    connection = None\n",
    "    try:\n",
    "        # Establish the connection\n",
    "        connection = psycopg2.connect(connection_string)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Convert schema dictionary to JSON string\n",
    "        schema_json = json.dumps(schema)\n",
    "\n",
    "        # Insert or update the schema using ON CONFLICT clause\n",
    "        query = \"\"\"\n",
    "            INSERT INTO saas_template_schema (saas_edge_id, template_id, schema, created_at, updated_at)\n",
    "            VALUES (%s, %s, %s, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)\n",
    "            ON CONFLICT (saas_edge_id, template_id)\n",
    "            DO UPDATE SET\n",
    "                schema = EXCLUDED.schema,\n",
    "                updated_at = CURRENT_TIMESTAMP;\n",
    "        \"\"\"\n",
    "        print(\"Executing query:\", query)\n",
    "\n",
    "        # Execute the query with provided parameters\n",
    "        cursor.execute(query, (saas_edge_id, template_id, schema_json))\n",
    "        connection.commit()\n",
    "        print(\"Schema inserted or updated successfully.\")\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database error while inserting/updating schema: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error in insert_or_update_schema function: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "\n",
    "def get_existing_schema(saas_edge_id: str, template_id: str) -> Any:\n",
    "    \"\"\"\n",
    "    Fetch the existing schema for the given saas_edge_id and template_id from the saas_template_schema table.\n",
    "    \"\"\"\n",
    "    connection_string = create_connection_string()\n",
    "    connection = None\n",
    "    try:\n",
    "        # Establish database connection\n",
    "        connection = psycopg2.connect(connection_string, cursor_factory=RealDictCursor)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Query to check for existing schema\n",
    "        query = \"\"\"\n",
    "            SELECT schema FROM saas_template_schema\n",
    "            WHERE saas_edge_id = %s AND template_id = %s\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (saas_edge_id, template_id))\n",
    "        result = cursor.fetchone()\n",
    "\n",
    "        if result:\n",
    "            return result[\"schema\"]\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching schema: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "\n",
    "def genarate_template_schema(saas_edge_id: str, template_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main function to fetch or generate schema for saas_template_schema.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Check if schema already exists\n",
    "        existing_schema = get_existing_schema(saas_edge_id, template_id)\n",
    "        if existing_schema:\n",
    "            print(\"Schema found in the database.\")\n",
    "            return existing_schema\n",
    "\n",
    "        print(\"No schema found. Generating and saving schema...\")\n",
    "        # Step 2: Fetch template attributes and generate schema\n",
    "        template_attributes = fetch_template_attributes(saas_edge_id, template_id)\n",
    "        if not template_attributes:\n",
    "            raise ValueError(\n",
    "                \"No attributes found for the given saas_edge_id and template_id.\"\n",
    "            )\n",
    "\n",
    "        generated_schema = generate_json_schema(template_attributes)\n",
    "\n",
    "        # Step 3: Save the schema to the database\n",
    "        insert_or_update_schema(saas_edge_id, template_id, generated_schema)\n",
    "        return generated_schema\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genarate_template_schema(saas_edge_id, request_args.get(\"template_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "try:\n",
    "    template_id = request_args.get(\"template_id\")\n",
    "    if not template_id:\n",
    "        raise ValueError(\"template_id not found in request_args\")\n",
    "        \n",
    "    template_data = fetch_channel_attr_template(saas_edge_id, template_id)\n",
    "    template_details = fetch_saas_channel_templates(saas_edge_id, template_id)  \n",
    "    # print(template_data)\n",
    "    if not template_data:\n",
    "        print(\"WARNING: No template data retrieved or an error occurred during the fetch operation.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"ERROR: An unexpected error occurred while fetching template data.\")\n",
    "    print(f\"ERROR: {e}\")\n",
    "    print(f\"ERROR: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_template_data = {}\n",
    "\n",
    "try:\n",
    "    if template_data:\n",
    "        # filtered_template_data = [\n",
    "        #     {\n",
    "        #         'column_name': item.get('column_name', ''),  # Default to empty string if not found\n",
    "        #         'field_mapping': item.get('field_mapping', {})  # Default to empty dict if not found\n",
    "        #     }\n",
    "        #     for item in template_data\n",
    "        #     if isinstance(item, dict)  # Ensure item is a dictionary\n",
    "        # ]\n",
    "        filtered_template_data = {\n",
    "            item.get('column_name', ''): {\n",
    "                \"field_mapping\":item.get('field_mapping', \"\"),\n",
    "                \"accepts_multiple_values\":item.get('accepts_multiple_values', False),\n",
    "                \"transformation_function\":item.get(\"transformation_rules\", \"\"),\n",
    "                \"data_type\":item.get(\"data_type\", \"text\")\n",
    "            }\n",
    "            for item in template_data\n",
    "        }\n",
    "        if not filtered_template_data:\n",
    "            print(\"WARNING: No valid template data entries found after filtering\")\n",
    "    else:\n",
    "        print(\"WARNING: template_data is None or empty\")\n",
    "\n",
    "    print(\"Filtered template data:\", filtered_template_data)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to process template data: {str(e)}\")\n",
    "    print(f\"ERROR: {traceback.format_exc()}\")\n",
    "    filtered_template_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import io  # updated import\n",
    "from io import StringIO  # kept as-is for compatibility\n",
    "\n",
    "\n",
    "def get_db_connection():\n",
    "    return psycopg2.connect(\n",
    "        # host=\"localhost\",\n",
    "        host=\"/cloudsql/saastify-base-wm:us-central1:saastify-pgdb-us\",           \n",
    "        port=5432,\n",
    "        database=\"catalog-edge-db\",  \n",
    "        user=\"postgres\",             \n",
    "        password=\"saasdbforwindmill2023\"  \n",
    "    )\n",
    "\n",
    "# \u2705 Step 2: Export from PostgreSQL using COPY\n",
    "def export_products_to_dataframe(saas_edge_id, filter_params=None):\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = get_db_connection()\n",
    "\n",
    "       \n",
    "        csv_buffer = io.TextIOWrapper(io.BytesIO(), encoding='utf-8')\n",
    "\n",
    "        # Start query with saas_edge_id filter\n",
    "        query_conditions = [f\"saas_edge_id = '{saas_edge_id}'\"]\n",
    "\n",
    "        if filter_params:\n",
    "            for key, condition in filter_params.items():\n",
    "                if isinstance(condition, dict):\n",
    "                    if 'in' in condition:\n",
    "                        values = condition['in']\n",
    "                        if key.lower() == 'productid':\n",
    "                            values_list = ','.join(str(int(v)) for v in values)\n",
    "                            query_conditions.append(f\"product_id IN ({values_list})\")\n",
    "                        else:\n",
    "                            values_list = ','.join(f\"'{v}'\" for v in values)\n",
    "                            query_conditions.append(f\"{key} IN ({values_list})\")\n",
    "                    elif 'includesInsensitive' in condition:\n",
    "                        value = condition['includesInsensitive']\n",
    "                        query_conditions.append(f\"LOWER({key}) LIKE '%{value.lower()}%'\")\n",
    "                    elif 'equalTo' in condition:\n",
    "                        value = condition['equalTo']\n",
    "                        if isinstance(value, (int, float)):\n",
    "                            query_conditions.append(f\"{key} = {value}\")\n",
    "                        else:\n",
    "                            query_conditions.append(f\"{key} = '{value}'\")\n",
    "                    elif 'notEqualTo' in condition:\n",
    "                        value = condition['notEqualTo']\n",
    "                        if isinstance(value, (int, float)):\n",
    "                            query_conditions.append(f\"{key} != {value}\")\n",
    "                        else:\n",
    "                            query_conditions.append(f\"{key} != '{value}'\")\n",
    "                    elif 'greaterThanOrEqualTo' in condition or 'lessThan' in condition:\n",
    "                    # Handle date range filtering\n",
    "                        gte = condition.get('greaterThanOrEqualTo')\n",
    "                        lt = condition.get('lessThan')\n",
    "                        if gte:\n",
    "                            query_conditions.append(f\"{key} >= '{gte}'\")\n",
    "                        if lt:\n",
    "                            query_conditions.append(f\"{key} < '{lt}'\")\n",
    "                else:\n",
    "                    if isinstance(condition, (int, float)):\n",
    "                        query_conditions.append(f\"{key} = {condition}\")\n",
    "                    else:\n",
    "                        query_conditions.append(f\"{key} = '{condition}'\")\n",
    "\n",
    "        # Build final SQL query\n",
    "        final_conditions = \" AND \".join(query_conditions)\n",
    "        base_query = f\"\"\"\n",
    "        COPY (\n",
    "            SELECT * FROM products\n",
    "            WHERE {final_conditions}\n",
    "        ) TO STDOUT WITH CSV HEADER\n",
    "        \"\"\"\n",
    "\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"SET statement_timeout = 0\")  \n",
    "            cur.copy_expert(base_query, csv_buffer)\n",
    "\n",
    "        # Read from start\n",
    "        csv_buffer.seek(0)\n",
    "        df = pd.read_csv(csv_buffer, keep_default_na=False, low_memory=False)\n",
    "        return df\n",
    "\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "# df = export_products_to_dataframe(saas_edge_id, filter_params)\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = export_products_to_dataframe(saas_edge_id,filter_params)\n",
    "products = df.to_dict(orient=\"records\")\n",
    "total_count_data=len(products)\n",
    "# print(\"products\",products)\n",
    "job_response = {\n",
    "            \"total\": total_count_data,\n",
    "            \"success\": 0,\n",
    "            \"failed\":0,\n",
    "            \n",
    "        }\n",
    "update_job_details(job_id, job_response, failed_url=None, success_url=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Configuration\n",
    "GRAPHQL_ENDPOINT = \"https://edgeql.saastify.ai/graphql\"\n",
    "HEADERS = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    # \"Authorization\": \"Bearer YOUR_ACCESS_TOKEN\"  # Replace with your actual token\n",
    "}\n",
    "\n",
    "\n",
    "def fetch_template_data(saas_edge_id, template_id):\n",
    "    \"\"\"Fetches template data from the GraphQL endpoint.\"\"\"\n",
    "    graphql_query = \"\"\"\n",
    "    query ($templateId: UUID!, $saasEdgeId: UUID!) {\n",
    "      allSaasChannelTemplates(\n",
    "        condition: {\n",
    "          templateId: $templateId, \n",
    "          saasEdgeId: $saasEdgeId\n",
    "        }\n",
    "      ) {\n",
    "        edges {\n",
    "          node {\n",
    "            templateId\n",
    "            saasEdgeId\n",
    "            saasTemplateSchemasByTemplateId {\n",
    "              edges {\n",
    "                node {\n",
    "                  schema\n",
    "                  updatedAt\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "            saasChannelAttrTemplatesByTemplateId(first: 500) {\n",
    "              edges {\n",
    "                node {\n",
    "                  fieldMapping\n",
    "                  columnName\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    variables = {\"templateId\": template_id, \"saasEdgeId\": saas_edge_id}\n",
    "    response = requests.post(GRAPHQL_ENDPOINT, headers=HEADERS, json={\"query\": graphql_query, \"variables\": variables})\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def convert_to_camel_case(input_string):\n",
    "    # Define a regex pattern to match underscores, hyphens, and spaces followed by a character\n",
    "    pattern = r\"[_\\-\\s]+(\\w)\"\n",
    "\n",
    "    # The replacer function capitalizes the character after the delimiter\n",
    "    def replacer(match):\n",
    "        return match.group(1).upper()\n",
    "\n",
    "    # Lowercase the first character and the rest of the string\n",
    "    input_string = input_string.strip()\n",
    "    input_string = input_string[0].lower() + input_string[1:]\n",
    "\n",
    "    # Apply the regex substitution\n",
    "    converted_string = re.sub(pattern, replacer, input_string)\n",
    "\n",
    "    return converted_string\n",
    "\n",
    "def generate_dynamic_graphql_query(attribute_response):\n",
    "    \"\"\"Generates a dynamic GraphQL query based on attribute mappings.\"\"\"\n",
    "    field_mappings = []\n",
    "    attributes_mapping = {}\n",
    "\n",
    "    for attr in attribute_response:\n",
    "        node = attr.get(\"node\")\n",
    "        if node and node.get(\"fieldMapping\") and node.get(\"columnName\"):\n",
    "            field_mapping = node[\"fieldMapping\"]\n",
    "            column_name = node[\"columnName\"]\n",
    "            field_mappings.append(field_mapping)\n",
    "            attributes_mapping[column_name] = field_mapping\n",
    "\n",
    "    unique_fields = list(set(field_mappings))\n",
    "    parent_objects = set()\n",
    "\n",
    "    graphql_query = \"\"\"\n",
    "    query GetProducts($first: Int, $offset: Int, $filter: ProductFilter) {\n",
    "      allProducts(\n",
    "        first: $first,\n",
    "        offset: $offset,\n",
    "        filter: $filter\n",
    "      ) {\n",
    "        edges {\n",
    "          node {\n",
    "    \"\"\"\n",
    "\n",
    "    for field in unique_fields:\n",
    "        field_parts = field.split(\".\")\n",
    "        parent_field = field_parts[0]\n",
    "        parent_field_camel_case = convert_to_camel_case(parent_field)\n",
    "        if parent_field_camel_case not in parent_objects:\n",
    "            graphql_query += f\"            {parent_field_camel_case}\\n\"\n",
    "            parent_objects.add(parent_field_camel_case)\n",
    "\n",
    "    graphql_query += \"\"\"\n",
    "            updatedAt\n",
    "            productId\n",
    "          }\n",
    "        }\n",
    "        pageInfo {\n",
    "          hasNextPage\n",
    "          startCursor\n",
    "          endCursor\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    return {\"graphql_query\": graphql_query, \"attributes_mapping\": attributes_mapping}\n",
    "\n",
    "def fetch_products_recursively(graphql_query, filter=None, first=10, offset=0):\n",
    "    \"\"\"Fetches products recursively from the GraphQL endpoint with support for multiple OR filters.\"\"\"\n",
    "    products = []\n",
    "    while True:\n",
    "        # Construct the condition for the GraphQL query\n",
    "        variables = {\"first\": first, \"offset\": offset, \"filter\": filter}\n",
    "        response = requests.post(GRAPHQL_ENDPOINT, headers=HEADERS, json={\"query\": graphql_query, \"variables\": variables})\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        products_batch = data['data']['allProducts']['edges']\n",
    "        products.extend([product['node'] for product in products_batch])\n",
    "        \n",
    "        if not data['data']['allProducts']['pageInfo']['hasNextPage']:\n",
    "            break\n",
    "        offset += first\n",
    "    return products\n",
    "\n",
    "def fetch_completeness_data(saas_edge_id, template_id, product_id):\n",
    "  # Define your endpoint URL and headers\n",
    "  url = \"https://edgeql.saastify.ai/graphql\"  # Update with your actual endpoint URL\n",
    "  headers = {\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": \"Bearer YOUR_ACCESS_TOKEN\"  # Replace with your actual token if required\n",
    "  }\n",
    "\n",
    "  # Define the GraphQL query as a string\n",
    "  graphql_query = \"\"\"\n",
    "  query (\n",
    "    $saasEdgeId: UUID!,\n",
    "    $templateId:UUID!,\n",
    "    $productId: BigInt!\n",
    "  ) {\n",
    "    allProductTemplateCompletenesses(\n",
    "      condition: {\n",
    "        saasEdgeId: $saasEdgeId,\n",
    "        templateId:$templateId,\n",
    "        productId: $productId\n",
    "      }\n",
    "    ) {\n",
    "      nodes {\n",
    "        nodeId\n",
    "        internalId\n",
    "        productId\n",
    "        templateId\n",
    "        saasEdgeId\n",
    "        validationErrors\n",
    "        transformedResponse\n",
    "        lastProductUpdatedAt\n",
    "        lastTemplateUpdatedAt\n",
    "        createdAt\n",
    "        updatedAt\n",
    "        isValid\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \"\"\"\n",
    "  \n",
    "  variables = {\n",
    "      \"saasEdgeId\": saas_edge_id,  # Replace with the actual saasEdgeId\n",
    "      \"templateId\": template_id, # template id\n",
    "      \"productId\": product_id  # Replace with the actual productId\n",
    "  }\n",
    "\n",
    "  # Send the request\n",
    "  response = requests.post(\n",
    "      url,\n",
    "      headers=headers,\n",
    "      json={\"query\": graphql_query, \"variables\": variables}\n",
    "  )\n",
    "\n",
    "  # Check the response status and print the result\n",
    "  if response.status_code == 200:\n",
    "      # Pretty-print the response JSON\n",
    "      response_data = response.json()\n",
    "      print(json.dumps(response_data, indent=2))\n",
    "\n",
    "      return response_data\n",
    "  else:\n",
    "      print(f\"Query failed with status code {response.status_code}: {response.text}\")\n",
    "\n",
    "\n",
    "def check_revalidation_needed(template_obj, product_obj, completeness_obj):\n",
    "    try:\n",
    "        \"\"\"Checks if re-validation is needed based on update timestamps.\"\"\"\n",
    "        if not completeness_obj:\n",
    "            return {\"recheck_needed\": True, \"error\": \"completeness_obj is empty or invalid.\"}\n",
    "\n",
    "        completeness_data = completeness_obj[0]\n",
    "        print(\"completeness_data\",completeness_data)\n",
    "        last_template_updated_at = completeness_data.get(\"lastTemplateUpdatedAt\")\n",
    "        last_product_updated_at = completeness_data.get(\"lastProductUpdatedAt\")\n",
    "        template_updated_at = template_obj and template_obj.get(\"saasTemplateSchemasByTemplateId\", {}).get(\"edges\", [{}])[0].get(\"node\", {}).get(\"updatedAt\")\n",
    "        product_updated_at = product_obj.get(\"updatedAt\")\n",
    "\n",
    "        def parse_datetime(date_str):\n",
    "            print(\"date_str\",date_str)\n",
    "            if not date_str:\n",
    "                return datetime.strptime(datetime.now(), \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "            try:\n",
    "                return datetime.strptime(date_str, \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "            except ValueError:\n",
    "                return datetime.strptime(date_str, \"%Y-%m-%dT%H:%M:%S.%f\").replace(tzinfo=pytz.UTC)\n",
    "\n",
    "        last_template_updated_at_dt = parse_datetime(last_template_updated_at)\n",
    "        last_product_updated_at_dt = parse_datetime(last_product_updated_at)\n",
    "        template_updated_at_dt = parse_datetime(template_updated_at)\n",
    "        product_updated_at_dt = parse_datetime(product_updated_at)\n",
    "\n",
    "        if None in [last_template_updated_at_dt, last_product_updated_at_dt, template_updated_at_dt, product_updated_at_dt]:\n",
    "            return {\"error\": \"One or more dates could not be parsed.\"}\n",
    "\n",
    "        re_check_needed = (last_product_updated_at_dt < product_updated_at_dt) and (last_template_updated_at_dt < template_updated_at_dt)\n",
    "        if re_check_needed:\n",
    "            completeness_data[\"recheck_needed\"] = True\n",
    "            return completeness_data\n",
    "        else:\n",
    "            return {\n",
    "                \"validationErrors\": completeness_data.get(\"validationErrors\"),\n",
    "                \"transformedResponse\": completeness_data.get(\"transformedResponse\"),\n",
    "                \"isValid\": completeness_data.get(\"isValid\"),\n",
    "                \"recheck_needed\": False\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in check_revalidation_needed: {e}\")\n",
    "        print(f\"Error traceback: {traceback.format_exc()}\")\n",
    "        print_exc()\n",
    "        return None\n",
    "\n",
    "def transform_and_validate_product(template_attr_mappings, product_obj, schema, template_obj, saas_edge_id, recheck_resp):\n",
    "\n",
    "    def convert_to_mm(value):\n",
    "        \"\"\"Convert dimensions from various imperial units to millimeters.\"\"\"\n",
    "        # Ensure value is a string before processing\n",
    "        if isinstance(value, (int, float)):\n",
    "            value = str(value)\n",
    "        \n",
    "        if isinstance(value, str):\n",
    "            value = value.lower().strip()\n",
    "            if \"inch\" in value or \"\\\"\" in value:\n",
    "                num_value = float(''.join(filter(lambda x: x.isdigit() or x == '.', value)))\n",
    "                return num_value * 25.4  # 1 inch = 25.4 mm\n",
    "            elif \"cm\" in value:\n",
    "                num_value = float(''.join(filter(lambda x: x.isdigit() or x == '.', value)))\n",
    "                return num_value * 10  # 1 cm = 10 mm\n",
    "            elif \"mm\" in value:\n",
    "                return float(''.join(filter(lambda x: x.isdigit() or x == '.', value)))\n",
    "            else:\n",
    "                return float(value)  # Assume already in mm if no unit is specified\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported type for value conversion: {type(value)}\")\n",
    "\n",
    "\n",
    "    def convert_to_kg(value):\n",
    "        \"\"\"Convert weight from various imperial units to kilograms.\"\"\"\n",
    "        # Ensure value is a string before processing\n",
    "        if isinstance(value, (int, float)):\n",
    "            value = str(value)\n",
    "        \n",
    "        if isinstance(value, str):\n",
    "            value = value.lower().strip()\n",
    "            if \"lb\" in value or \"pound\" in value:\n",
    "                num_value = float(''.join(filter(lambda x: x.isdigit() or x == '.', value)))\n",
    "                return num_value * 0.453592  # 1 lb = 0.453592 kg\n",
    "            elif \"kg\" in value:\n",
    "                return float(''.join(filter(lambda x: x.isdigit() or x == '.', value)))\n",
    "            elif \"kgs\" in value:\n",
    "                return float(''.join(filter(lambda x: x.isdigit() or x == '.', value)))\n",
    "            else:\n",
    "                return float(value)  # Assume already in kg if no unit is specified\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported type for value conversion: {type(value)}\")\n",
    "\n",
    "    def apply_transformation(value, transformation_rules: str):\n",
    "        \"\"\"\n",
    "        Apply multiple transformation rules to a value with comprehensive error handling.\n",
    "        Rules are separated by |;| character sequence.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Handle None/empty values\n",
    "            if value is None or transformation_rules is None:\n",
    "                return value\n",
    "                \n",
    "            # Ensure transformation_rules is a string\n",
    "            if not isinstance(transformation_rules, str):\n",
    "                print(f\"WARNING: Invalid transformation rule type: {type(transformation_rules)}\")\n",
    "                return value\n",
    "            \n",
    "            # Normalize backslashes in the input value if it's a string\n",
    "            if isinstance(value, str):\n",
    "                transformed_value = value.replace('\\\\', '/')\n",
    "            else:\n",
    "                transformed_value = value\n",
    "                \n",
    "            # Split rules by |;| character sequence and process each rule in sequence\n",
    "            rules = [rule.strip() for rule in transformation_rules.split('|;|')]\n",
    "            for rule in rules:\n",
    "                # Basic string transformations\n",
    "                if rule == \"uppercase\":\n",
    "                    transformed_value = transformed_value.upper() if isinstance(transformed_value, str) else transformed_value\n",
    "                elif rule == \"lowercase\":\n",
    "                    transformed_value = transformed_value.lower() if isinstance(transformed_value, str) else transformed_value\n",
    "                elif rule == \"strip\":\n",
    "                    transformed_value = transformed_value.strip() if isinstance(transformed_value, str) else transformed_value\n",
    "                elif rule == \"split_comma\":\n",
    "                    if not isinstance(transformed_value, str):\n",
    "                        continue\n",
    "                    # Handle potential backslashes in comma-separated values\n",
    "                    transformed_value = [item.strip().replace('\\\\', '/') \n",
    "                                       for item in transformed_value.split(\",\") \n",
    "                                       if item.strip()]\n",
    "                    \n",
    "                elif rule.startswith(\"set|||\"):\n",
    "                    \n",
    "                    try:\n",
    "                        parts = rule.split(\"|||\")\n",
    "                       \n",
    "                        if len(parts) != 2:\n",
    "                            print(f\"WARNING: Invalid set rule format: {rule}. Expected 2 parts.\")\n",
    "                            continue\n",
    "                        _, value = parts\n",
    "                        \n",
    "                        transformed_value = value\n",
    "                    except Exception as e:\n",
    "                        print(f\"WARNING: Error processing set rule: {rule}. Error: {str(e)}\")\n",
    "                        continue\n",
    "                elif rule == \"date_only\":\n",
    "                    if isinstance(transformed_value, (pd.Timestamp, datetime, date)):\n",
    "                        transformed_value = transformed_value.strftime('%Y-%m-%d')  # Convert datetime to string with only date\n",
    "                    elif isinstance(transformed_value, str):\n",
    "                        try:\n",
    "                            # Handle cases where value is a string with datetime format\n",
    "                            parsed_date = pd.to_datetime(transformed_value, errors='coerce')\n",
    "                            if not pd.isnull(parsed_date):  \n",
    "                                transformed_value = parsed_date.strftime('%Y-%m-%d')\n",
    "                                print(\"new\",transformed_value)\n",
    "                        except Exception as e:\n",
    "                            print(f\"WARNING: Could not parse date for value '{transformed_value}'. Error: {str(e)}\")\n",
    "\n",
    "                elif rule == \"clean_numeric_value\":\n",
    "                    try:\n",
    "                        cleaned_value = \"\"\n",
    "                        convert_weight=True\n",
    "                        decimal_places=True\n",
    "                        if convert_weight:\n",
    "                            try:\n",
    "                                cleaned_value = round(convert_to_kg(transformed_value), decimal_places)\n",
    "                            except Exception as e:\n",
    "                                print(f\"WARNING: Failed weight conversion for '{transformed_value}'. Error: {str(e)}\")\n",
    "                \n",
    "                        elif convert_dimension:\n",
    "                            try:\n",
    "                                cleaned_value = round(convert_to_mm(transformed_value), decimal_places)\n",
    "                            except Exception as e:\n",
    "                                print(f\"WARNING: Failed dimension conversion for '{transformed_value}'. Error: {str(e)}\")\n",
    "                \n",
    "                        elif isinstance(transformed_value, (int, float)):\n",
    "                            try:\n",
    "                                cleaned_value = round(float(transformed_value), decimal_places)\n",
    "                            except Exception as e:\n",
    "                                print(f\"WARNING: Failed to convert numeric type '{transformed_value}'. Error: {str(e)}\")\n",
    "                \n",
    "                        elif isinstance(transformed_value, str):\n",
    "                            try:\n",
    "                                numeric_str = ''.join(filter(lambda x: x.isdigit() or x == '.', transformed_value))\n",
    "                                if numeric_str:\n",
    "                                    cleaned_value = round(float(numeric_str), decimal_places)\n",
    "                                else:\n",
    "                                    print(f\"WARNING: Could not extract numeric value from string '{transformed_value}'\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"WARNING: Failed string numeric conversion for '{transformed_value}'. Error: {str(e)}\")\n",
    "                \n",
    "                        else:\n",
    "                            print(f\"WARNING: Unsupported type for numeric value: {type(transformed_value)} - '{transformed_value}'\")\n",
    "                \n",
    "                        transformed_value = cleaned_value\n",
    "                \n",
    "                    except Exception as e:\n",
    "                        print(f\"[EXCEPTION] Unexpected error in clean_numeric_value for '{transformed_value}': {str(e)}\")\n",
    "\n",
    "                \n",
    "                # Complex transformations\n",
    "                elif rule.startswith(\"replace|||\"):\n",
    "                    if not isinstance(transformed_value, str):\n",
    "                        continue\n",
    "                    try:\n",
    "                        # Split using ||| delimiter\n",
    "                        parts = rule.split(\"|||\")\n",
    "                        \n",
    "                        if len(parts) != 3:\n",
    "                            print(f\"WARNING: Invalid replace rule format: {rule}. Expected 3 parts.\")\n",
    "                            continue\n",
    "                        \n",
    "                        _, _, replace_rule = parts\n",
    "                        original, new = replace_rule.split(\"||\", 1)\n",
    "                        \n",
    "                        original = original.strip().replace('\\\\', '/')\n",
    "                        \n",
    "                        new = new.strip().replace('\\\\', '/')\n",
    "                        original = original.strip().replace('\\\\', '/')\n",
    "                        \n",
    "                        new = new.strip().replace('\\\\', '/')\n",
    "                        \n",
    "                        transformed_value = transformed_value.replace(original, new)\n",
    "                        \n",
    "                    except (IndexError, ValueError) as e:\n",
    "                        print(f\"WARNING: Invalid replace rule format: {rule}. Error: {str(e)}\")\n",
    "                        continue\n",
    "                        \n",
    "                elif rule == \"vlookup:map\":\n",
    "                    if not isinstance(transformed_value, str):\n",
    "                        continue\n",
    "                    mapping = {\n",
    "                        \"yes\": True,\n",
    "                        \"no\": False,\n",
    "                        \"true\": True,\n",
    "                        \"false\": False,\n",
    "                        \"1\": True,\n",
    "                        \"0\": False,\n",
    "                        \"y\": True,\n",
    "                        \"n\": False\n",
    "                    }\n",
    "                    # Convert to lowercase and strip before mapping\n",
    "                    lookup_value = transformed_value.lower().strip()\n",
    "                    transformed_value = mapping.get(lookup_value, transformed_value)\n",
    "                elif rule.startswith(\"custom_func|||\"):\n",
    "                    try:\n",
    "                        # Extract the function definition\n",
    "                        func_def = rule.split(\"|||\", 1)[1].strip()\n",
    "                        # Execute the function definition\n",
    "                        exec(func_def, globals())\n",
    "                        # Extract function name\n",
    "                        func_name = func_def.split('(')[0].split()[1]\n",
    "                        # Retrieve the function\n",
    "                        custom_function = globals().get(func_name)\n",
    "                        if callable(custom_function):\n",
    "                            transformed_value = custom_function(transformed_value)\n",
    "                        else:\n",
    "                            print(f\"WARNING: Function {func_name} is not callable.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"WARNING: Error processing custom function: {rule}. Error: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                \n",
    "                # New transformation rule for adjusting negative numbers to zero\n",
    "                elif rule == \"adjust_negative_to_zero\":\n",
    "                    print(\"transformed_value\",transformed_value)\n",
    "                    try:\n",
    "                        if isinstance(transformed_value, (int, float)):\n",
    "                            transformed_value = max(0, transformed_value)\n",
    "                        elif isinstance(transformed_value, list):\n",
    "                            transformed_value = [max(0, float(val)) if isinstance(val, (int, float, str)) else val \n",
    "                                              for val in transformed_value]\n",
    "                        elif isinstance(transformed_value, str):\n",
    "                            try:\n",
    "                                num_value = float(transformed_value)\n",
    "                                transformed_value = str(max(0, num_value))\n",
    "                            except ValueError:\n",
    "                                print(f\"WARNING: Could not convert string '{transformed_value}' to number for negative adjustment\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"WARNING: Error applying negative adjustment: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                else:\n",
    "                    print(f\"WARNING: Unknown transformation rule: {rule}\")\n",
    "                    continue\n",
    "            \n",
    "            return transformed_value\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Transformation failed for value '{value}' with rules '{transformation_rules}': {str(e)}\")\n",
    "            return value  # Return original value on error\n",
    "\n",
    "      \n",
    "        \n",
    "    def handle_default_value(expected_type):\n",
    "        \"\"\"\n",
    "        Returns an appropriate default value based on the expected type.\n",
    "        \"\"\"\n",
    "        if expected_type == \"array\":\n",
    "            return []\n",
    "        elif expected_type == \"string\":\n",
    "            return \"\"\n",
    "        elif expected_type == \"number\":\n",
    "            return 0.0\n",
    "        elif expected_type == \"integer\":\n",
    "            return 0\n",
    "        elif expected_type == \"float\":\n",
    "            return 0.0\n",
    "        elif expected_type == \"boolean\":\n",
    "            return False\n",
    "        return None\n",
    "      \n",
    "    \n",
    "    def fix_schema_defaults(schema) -> dict:\n",
    "        \"\"\"\n",
    "        Recursively fixes the schema to replace invalid 'description' and 'default' values.\n",
    "        \"\"\"\n",
    "        if isinstance(schema, dict):\n",
    "            cleaned_schema = {}\n",
    "            for key, value in schema.items():\n",
    "                if key == \"description\" and value is None:\n",
    "                    cleaned_schema[key] = \"\"  # Replace None description with an empty string\n",
    "                elif key == \"default\" and value is None:\n",
    "                    expected_type = schema.get(\"type\")\n",
    "                    cleaned_schema[key] = handle_default_value(expected_type)\n",
    "                else:\n",
    "                    cleaned_schema[key] = fix_schema_defaults(value)\n",
    "            return cleaned_schema\n",
    "        elif isinstance(schema, list):\n",
    "            return [fix_schema_defaults(item) for item in schema]\n",
    "        else:\n",
    "            return schema\n",
    "\n",
    "\n",
    "    def handle_coercion(schema: dict, value):\n",
    "        \"\"\"\n",
    "        Attempts to coerce a value to match the schema type if possible.\n",
    "        \"\"\"\n",
    "        expected_type = schema.get(\"type\")\n",
    "        enum_values = schema.get(\"enum\")\n",
    "\n",
    "        if expected_type == \"string\":\n",
    "            if isinstance(value, bool):  # Convert boolean to lowercase string\n",
    "                return \"true\" if value else \"false\"\n",
    "            return str(value) if value is not None else \"\"\n",
    "\n",
    "        if enum_values:\n",
    "            # Match enum values (case-insensitive if strings)\n",
    "            if isinstance(value, str) and value.lower() in [v.lower() for v in enum_values]:\n",
    "                return next(v for v in enum_values if v.lower() == value.lower())\n",
    "            if isinstance(value, bool):\n",
    "                value_str = \"true\" if value else \"false\"\n",
    "                if value_str in enum_values:\n",
    "                    return value_str\n",
    "            return enum_values[0]  # Default to the first enum value if no match\n",
    "\n",
    "        if expected_type == \"array\" and not isinstance(value, list):\n",
    "            return [] if value in [None, \"\"] else [value]\n",
    "\n",
    "        if expected_type == \"number\" and not isinstance(value, (int, float)):\n",
    "            try:\n",
    "                return float(value)\n",
    "            except (ValueError, TypeError):\n",
    "                return 0.0\n",
    "\n",
    "        if expected_type == \"integer\" and not isinstance(value, int):\n",
    "            try:\n",
    "                return int(value)\n",
    "            except (ValueError, TypeError):\n",
    "                return 0\n",
    "\n",
    "        if expected_type == \"boolean\" and not isinstance(value, bool):\n",
    "            if str(value).lower() in [\"true\", \"1\"]:\n",
    "                return True\n",
    "            elif str(value).lower() in [\"false\", \"0\"]:\n",
    "                return False\n",
    "\n",
    "        return value\n",
    "\n",
    "        \n",
    "    def set_nested_value(target_dict: dict, path, value):\n",
    "        \"\"\"\n",
    "        Sets a nested value in a dictionary given a path.\n",
    "        \"\"\"\n",
    "        keys = list(path)\n",
    "        for key in keys[:-1]:\n",
    "            if key not in target_dict or not isinstance(target_dict[key], dict):\n",
    "                target_dict[key] = {}\n",
    "            target_dict = target_dict[key]\n",
    "        target_dict[keys[-1]] = value\n",
    "\n",
    "    \n",
    "    def validate_product_schema(transformed_product: dict, schema: dict):\n",
    "        \"\"\"\n",
    "        Validates the product against the JSON schema, fixing schema and product errors dynamically.\n",
    "        \"\"\"\n",
    "        errors = {}\n",
    "        resolved_product = copy.deepcopy(transformed_product)\n",
    "\n",
    "        try:\n",
    "            # Clean the schema before validation\n",
    "            cleaned_schema = fix_schema_defaults(schema)\n",
    "            validate(instance=resolved_product, schema=cleaned_schema)\n",
    "            return True, errors, resolved_product\n",
    "        except ValidationError as err:\n",
    "            path = list(err.path)\n",
    "            path_str = \".\".join(map(str, path))\n",
    "            if path_str not in errors:\n",
    "                errors[path_str] = []\n",
    "            coerced_value = handle_coercion(err.schema, err.instance)\n",
    "            errors[path_str].append({\n",
    "                \"message\": err.message,\n",
    "                \"current_value\": err.instance,\n",
    "                \"suggested_value\": coerced_value\n",
    "            })\n",
    "            set_nested_value(resolved_product, path, coerced_value)\n",
    "            return False, errors, resolved_product\n",
    "        except SchemaError as schema_err:\n",
    "            raise schema_err\n",
    "\n",
    "\n",
    "        \n",
    "    # def transform_product(template_attr_mappings: dict, product_obj: dict, schema: dict) -> dict:\n",
    "    #     \"\"\"\n",
    "    #     Transforms a product object using template mappings and schema.\n",
    "    #     Supports nested extraction from raw_product_data and saastify_product_data.\n",
    "    #     \"\"\"\n",
    "    #     transformed_product = {}\n",
    "    \n",
    "    #     # Parse JSON fields upfront\n",
    "    #     raw_data = {}\n",
    "    #     saastify_data = {}\n",
    "    \n",
    "    #     if isinstance(product_obj.get('raw_product_data'), str):\n",
    "    #         try:\n",
    "    #             raw_data = json.loads(product_obj['raw_product_data'])\n",
    "    #         except json.JSONDecodeError:\n",
    "    #             print(\"[ERROR] Could not parse raw_product_data JSON.\")\n",
    "    #     else:\n",
    "    #         raw_data = product_obj.get('raw_product_data', {})\n",
    "    \n",
    "    #     if isinstance(product_obj.get('saastify_product_data'), str):\n",
    "    #         try:\n",
    "    #             saastify_data = json.loads(product_obj['saastify_product_data'])\n",
    "    #         except json.JSONDecodeError:\n",
    "    #             print(\"[ERROR] Could not parse saastify_product_data JSON.\")\n",
    "    #     else:\n",
    "    #         saastify_data = product_obj.get('saastify_product_data', {})\n",
    "    \n",
    "    #     for key, value in template_attr_mappings.items():\n",
    "    #         keys = value.split(\".\")\n",
    "    #         temp_value = product_obj\n",
    "    #         temp_value_data = product_obj     # default base dict\n",
    "    \n",
    "    #         # Dynamically switch to nested data\n",
    "    #         if keys[0] == 'raw_product_data':\n",
    "    #             temp_value = raw_data\n",
    "    #             keys = keys[1:]\n",
    "    #         elif keys[0] == 'saastify_product_data':\n",
    "    #             temp_value = saastify_data\n",
    "    #             keys = keys[1:]\n",
    "    \n",
    "    #         for k in keys:\n",
    "    #             if isinstance(temp_value, dict) and k in temp_value:\n",
    "    #                 temp_value = temp_value[k]\n",
    "    #             else:\n",
    "    #                 # print(f\"[DEBUG] Missing key '{k}' in path '{value}' for product {product_obj.get('sku')}\")\n",
    "    #                 temp_value = None\n",
    "    #                 break\n",
    "    \n",
    "    #         expected_type = schema.get(\"properties\", {}).get(key, {}).get(\"type\")\n",
    "    #         if temp_value is None:\n",
    "    #             temp_value = handle_default_value(expected_type)\n",
    "    #         elif expected_type == \"string\" and isinstance(temp_value, bool):\n",
    "    #             temp_value = \"true\" if temp_value else \"false\"\n",
    "    #         if key in filtered_template_data:\n",
    "    #             field_config = filtered_template_data[key]\n",
    "    #             transformation_function = field_config.get(\"transformation_function\", \"\")\n",
    "    #             if transformation_function and transformation_function.replace(\" \", \"\") != \"\":\n",
    "    #                 temp_value = apply_transformation(temp_value_data, transformation_function)\n",
    "    \n",
    "    #         transformed_product[key] = temp_value\n",
    "    \n",
    "    #     return transformed_product\n",
    "\n",
    "    # def transform_product(template_attr_mappings: dict, product_obj: dict, schema: dict) -> dict:\n",
    "    #     \"\"\"\n",
    "    #     Transforms a product object using template mappings and schema.\n",
    "    #     Supports nested extraction from raw_product_data, saastify_product_data, channel_product_data, etc.\n",
    "    #     \"\"\"\n",
    "    #     transformed_product = {}\n",
    "    #     nested_sources = {}\n",
    "    \n",
    "    #     # Step 1: Parse all *_data fields into a unified nested_sources dict\n",
    "    #     error_logged_fields = set()  # \u2705 Prevent duplicate error prints per field\n",
    "\n",
    "    #     for key, val in product_obj.items():\n",
    "    #         if key.endswith(\"_data\"):\n",
    "    #             if isinstance(val, str):\n",
    "    #                 try:\n",
    "    #                     nested_sources[key] = json.loads(val)\n",
    "    #                 except json.JSONDecodeError:\n",
    "    #                     if key not in error_logged_fields:\n",
    "    #                         print(f\"[ERROR] Could not parse {key} JSON.\")\n",
    "    #                         error_logged_fields.add(key)\n",
    "    #                     nested_sources[key] = {}\n",
    "    #             else:\n",
    "    #                 nested_sources[key] = val\n",
    "    \n",
    "    #     # Step 2: Process all mappings\n",
    "    #     for key, value in template_attr_mappings.items():\n",
    "    #         keys = value.split(\".\")\n",
    "    #         temp_value = product_obj     # default base dict\n",
    "    #         temp_value_data = product_obj  # for transformation use\n",
    "    \n",
    "    #         # Check if first key points to a known nested source\n",
    "    #         if keys[0] in nested_sources:\n",
    "    #             temp_value = nested_sources[keys[0]]\n",
    "    #             keys = keys[1:]\n",
    "    \n",
    "    #         for k in keys:\n",
    "    #             if isinstance(temp_value, dict) and k in temp_value:\n",
    "    #                 temp_value = temp_value[k]\n",
    "    #             else:\n",
    "    #                 # print(f\"[DEBUG] Missing key '{k}' in path '{value}' for product {product_obj.get('sku')}\")\n",
    "    #                 temp_value = None\n",
    "    #                 break\n",
    "    \n",
    "    #         expected_type = schema.get(\"properties\", {}).get(key, {}).get(\"type\")\n",
    "    #         if temp_value is None:\n",
    "    #             temp_value = handle_default_value(expected_type)\n",
    "    #         elif expected_type == \"string\" and isinstance(temp_value, bool):\n",
    "    #             temp_value = \"true\" if temp_value else \"false\"\n",
    "    \n",
    "    #         if key in filtered_template_data:\n",
    "    #             field_config = filtered_template_data[key]\n",
    "    #             transformation_function = field_config.get(\"transformation_function\", \"\")\n",
    "    #             if transformation_function and transformation_function.replace(\" \", \"\") != \"\":\n",
    "    #                 temp_value = apply_transformation(temp_value_data, transformation_function)\n",
    "    \n",
    "    #         transformed_product[key] = temp_value\n",
    "    \n",
    "    #     return transformed_product\n",
    "    def transform_product(template_attr_mappings: dict, product_obj: dict, schema: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Transforms a product object using template mappings and schema.\n",
    "        Supports nested extraction from raw_product_data, saastify_product_data, channel_product_data, etc.\n",
    "        \"\"\"\n",
    "        transformed_product = {}\n",
    "    \n",
    "        # Step 1: Parse all *_data fields into a unified nested_sources dict\n",
    "        nested_sources = {}\n",
    "        error_logged_fields = set()  # \u2705 Prevent duplicate error prints per field\n",
    "    \n",
    "        for key, val in product_obj.items():\n",
    "            if key.endswith(\"_data\"):\n",
    "                if isinstance(val, str):\n",
    "                    try:\n",
    "                        nested_sources[key] = json.loads(val)\n",
    "                    except json.JSONDecodeError:\n",
    "                        if key not in error_logged_fields:\n",
    "                            error_logged_fields.add(key)\n",
    "                        nested_sources[key] = {}\n",
    "                else:\n",
    "                    nested_sources[key] = val\n",
    "    \n",
    "        # Step 2: Process all mappings\n",
    "        for key, value in template_attr_mappings.items():\n",
    "            keys = value.split(\".\")\n",
    "            temp_value = product_obj     # default base dict\n",
    "            temp_value_data = product_obj  # for transformation use\n",
    "    \n",
    "            # Check if first key points to a known nested source\n",
    "            if keys[0] in nested_sources:\n",
    "                temp_value = nested_sources[keys[0]]\n",
    "                keys = keys[1:]\n",
    "    \n",
    "            for k in keys:\n",
    "                if isinstance(temp_value, dict) and k in temp_value:\n",
    "                    temp_value = temp_value[k]\n",
    "                else:\n",
    "                    # print(f\"[DEBUG] Missing key '{k}' in path '{value}' for product {product_obj.get('sku')}\")\n",
    "                    temp_value = None\n",
    "                    break\n",
    "    \n",
    "            expected_type = schema.get(\"properties\", {}).get(key, {}).get(\"type\")\n",
    "            if temp_value is None:\n",
    "                temp_value = handle_default_value(expected_type)\n",
    "            elif expected_type == \"string\" and isinstance(temp_value, bool):\n",
    "                temp_value = \"true\" if temp_value else \"false\"\n",
    "    \n",
    "            if key in filtered_template_data:\n",
    "               \n",
    "                field_config = filtered_template_data[key]\n",
    "                \n",
    "                transformation_function = field_config.get(\"transformation_function\", \"\")\n",
    "                if transformation_function and transformation_function.replace(\" \", \"\") != \"\":\n",
    "                    temp_value = apply_transformation(temp_value,transformation_function)\n",
    "                    # while isinstance(temp_value, list) and temp_value:\n",
    "                    #     temp_value = temp_value[0]\n",
    "    \n",
    "            transformed_product[key] = temp_value\n",
    "    \n",
    "        return transformed_product\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"Transforms and validates a product object.\"\"\"\n",
    "    if recheck_resp or not recheck_resp :\n",
    "        db_response = product_obj\n",
    "        schema_dict = schema\n",
    "        transformed_product = transform_product(template_attr_mappings, db_response, schema_dict)\n",
    "        is_valid, validation_errors, resolved_product = validate_product_schema(transformed_product, schema_dict)\n",
    "        return {\n",
    "            \"transformed_response\": resolved_product if not is_valid else transformed_product,\n",
    "            \"is_valid\": is_valid,\n",
    "            \"validation_errors\": validation_errors,\n",
    "            \"last_product_updated_at\": product_obj.get(\"updatedAt\"),\n",
    "            \"last_template_updated_at\": template_obj.get(\"updatedAt\"),\n",
    "            \"product_id\": product_obj.get(\"productId\"),\n",
    "            \"template_id\": template_obj.get(\"templateId\"),\n",
    "            \"saas_edge_id\": saas_edge_id\n",
    "        }\n",
    "    else:\n",
    "        return recheck_resp\n",
    "\n",
    "########################################################################################################\n",
    "def transform_and_validate_product_data(template_attr_mappings, product_obj, schema, template_obj, saas_edge_id, recheck_resp):\n",
    "    def handle_default_value(expected_type):\n",
    "        if expected_type == \"array\": return []\n",
    "        if expected_type == \"string\": return \"\"\n",
    "        if expected_type == \"number\": return 0.0\n",
    "        if expected_type == \"integer\": return 0\n",
    "        if expected_type == \"boolean\": return False\n",
    "        return None\n",
    "\n",
    "    def fix_schema_defaults(schema) -> dict:\n",
    "        if isinstance(schema, dict):\n",
    "            cleaned = {}\n",
    "            for key, value in schema.items():\n",
    "                if key == \"description\" and value is None:\n",
    "                    cleaned[key] = \"\"\n",
    "                elif key == \"default\" and value is None:\n",
    "                    cleaned[key] = handle_default_value(schema.get(\"type\"))\n",
    "                else:\n",
    "                    cleaned[key] = fix_schema_defaults(value)\n",
    "            return cleaned\n",
    "        elif isinstance(schema, list):\n",
    "            return [fix_schema_defaults(item) for item in schema]\n",
    "        return schema\n",
    "\n",
    "    def validate_product_schema(product, schema):\n",
    "        try:\n",
    "            cleaned_schema = fix_schema_defaults(schema)\n",
    "            validate(instance=product, schema=cleaned_schema)\n",
    "            return True, {}, product\n",
    "        except ValidationError as err:\n",
    "            path = list(err.path)\n",
    "            path_str = \".\".join(map(str, path))\n",
    "            return False, {path_str: err.message}, product\n",
    "\n",
    "    def transform_product(attr_map, raw_product, schema):\n",
    "        transformed = {}\n",
    "        # print(\"testing\",schema)\n",
    "        for output_key, db_key_path in attr_map.items():\n",
    "            keys = db_key_path.split(\".\")\n",
    "            value = raw_product\n",
    "            for k in keys:\n",
    "                if isinstance(value, dict) and k in value:\n",
    "                    value = value[k]\n",
    "                else:\n",
    "                    value = None\n",
    "                    break\n",
    "            expected_type = schema.get(\"properties\", {}).get(output_key, {}).get(\"type\")\n",
    "            if value is None:\n",
    "                value = handle_default_value(expected_type)\n",
    "            elif expected_type == \"string\" and isinstance(value, bool):\n",
    "                value = \"true\" if value else \"false\"\n",
    "            transformed[output_key] = value\n",
    "        return transformed\n",
    "\n",
    "    # ---- Main Logic ----\n",
    "    schema_dict = schema\n",
    "    # print(\"new--------------\",schema_dict)\n",
    "    db_response = product_obj  # directly from DB now\n",
    "    transformed_product = transform_product(template_attr_mappings, db_response, schema_dict)\n",
    "    is_valid, validation_errors, resolved_product = validate_product_schema(transformed_product, schema_dict)\n",
    "\n",
    "    return {\n",
    "        \"transformed_response\": resolved_product if not is_valid else transformed_product,\n",
    "        \"is_valid\": is_valid,\n",
    "        \"validation_errors\": validation_errors,\n",
    "        \"last_product_updated_at\": product_obj.get(\"updated_at\"),\n",
    "        \"last_template_updated_at\": template_obj.get(\"updatedAt\"),\n",
    "        \"product_id\": product_obj.get(\"product_id\"),\n",
    "        \"template_id\": template_obj.get(\"templateId\"),\n",
    "        \"saas_edge_id\": saas_edge_id\n",
    "    }\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "\n",
    "def insert_or_update_template_completeness(template):\n",
    "    try:\n",
    "        \"\"\"Inserts or updates template completeness data in the database.\"\"\"\n",
    "        print(\"template in insert_or_update_template_completeness\",template)\n",
    "        conn = get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get required values with safe dictionary access\n",
    "        saas_edge_id = template.get('saas_edge_id')\n",
    "        product_id = template.get('product_id')\n",
    "        template_id = template.get('template_id')\n",
    "        \n",
    "        # Ensure last_template_updated_at is not null\n",
    "        if 'last_template_updated_at' not in template or template['last_template_updated_at'] is None:\n",
    "            template['last_template_updated_at'] = datetime.now(pytz.UTC).isoformat()\n",
    "\n",
    "        check_query = \"\"\"\n",
    "        SELECT internal_id FROM product_template_completeness\n",
    "        WHERE product_id = %s AND saas_edge_id = %s AND template_id = %s;\n",
    "        \"\"\"\n",
    "        cursor.execute(check_query, (product_id, saas_edge_id, template_id))\n",
    "        existing_row = cursor.fetchone()\n",
    "\n",
    "        if existing_row:\n",
    "            internal_id = existing_row[0]\n",
    "            update_fields = []\n",
    "            update_values = []\n",
    "            for key, value in template.items():\n",
    "                if key not in ['product_id', 'saas_edge_id', 'template_id']:\n",
    "                    update_fields.append(f\"{key} = %s\")\n",
    "                    if isinstance(value, dict):\n",
    "                        value = json.dumps(value)\n",
    "                    update_values.append(value)\n",
    "            update_values.append(internal_id)\n",
    "            print(\"update_values in insert_or_update_template_completeness\",update_values)\n",
    "            update_query = f\"\"\"\n",
    "            UPDATE product_template_completeness\n",
    "            SET {\", \".join(update_fields)}, updated_at = CURRENT_TIMESTAMP\n",
    "            WHERE internal_id = %s\n",
    "            RETURNING *;\n",
    "            \"\"\"\n",
    "            cursor.execute(update_query, update_values)\n",
    "            updated_record = cursor.fetchone()\n",
    "        else:\n",
    "            insert_fields = ['product_id', 'saas_edge_id', 'template_id']\n",
    "            insert_placeholders = ['%s', '%s', '%s']\n",
    "            insert_values = [product_id, saas_edge_id, template_id]\n",
    "            for key, value in template.items():\n",
    "                if key not in ['product_id', 'saas_edge_id', 'template_id']:\n",
    "                    insert_fields.append(key)\n",
    "                    insert_placeholders.append(\"%s\")\n",
    "                    if isinstance(value, dict):\n",
    "                        value = json.dumps(value)\n",
    "                    insert_values.append(value)\n",
    "            print(\"insert_values in insert_or_update_template_completeness\",insert_values)\n",
    "            insert_query = f\"\"\"\n",
    "            INSERT INTO product_template_completeness ({\", \".join(insert_fields)}, created_at, updated_at)\n",
    "            VALUES ({\", \".join(insert_placeholders)}, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)\n",
    "            RETURNING *;\n",
    "            \"\"\"\n",
    "            cursor.execute(insert_query, insert_values)\n",
    "            updated_record = cursor.fetchone()\n",
    "\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return updated_record\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in insert_or_update_template_completeness: {e}\")\n",
    "        print(f\"Error traceback: {traceback.format_exc()}\")\n",
    "        print_exc()\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "            conn.close()\n",
    "        return None\n",
    "\n",
    "def fetch_products_from_db(saas_edge_id, db_conn):\n",
    "    \"\"\"\n",
    "    Fetches all products from the database matching the given saas_edge_id.\n",
    "    \"\"\"\n",
    "    cursor = db_conn.cursor()\n",
    "\n",
    "    query = \"SELECT * FROM products WHERE saas_edge_id = %s\"\n",
    "    cursor.execute(query, (saas_edge_id,))\n",
    "\n",
    "    rows = cursor.fetchall()\n",
    "    columns = [desc[0] for desc in cursor.description]\n",
    "    cursor.close()\n",
    "\n",
    "    # Convert rows to list of dictionaries\n",
    "    products = [dict(zip(columns, row)) for row in rows]\n",
    "    return products\n",
    "\n",
    "def orchestrate_workflow(request_args):\n",
    "    \"\"\"Orchestrates the entire workflow.\"\"\"\n",
    "    try:\n",
    "        # saas_edge_id = request_args.get(\"saas_edge_id\")\n",
    "        template_id = request_args.get(\"template_id\")\n",
    "        filter_params = request_args.get(\"filter_params\", {})\n",
    "        export_with_readiness=request_args.get(\"export_with_readiness\", {})\n",
    "        saas_edge_id\n",
    "        try:\n",
    "            template_data = fetch_template_data(saas_edge_id, template_id)\n",
    "            # print(\"template_data\",template_data)\n",
    "            template_node = template_data['data']['allSaasChannelTemplates']['edges'][0]['node']\n",
    "        except Exception as e:\n",
    "            print(f\"Error in orchestrate_workflow: {e}\")\n",
    "            print_exc()\n",
    "      \n",
    "        attribute_response = template_node['saasChannelAttrTemplatesByTemplateId']['edges']\n",
    "        # Step 2: Generate dynamic GraphQL query\n",
    "        query_result = generate_dynamic_graphql_query(attribute_response)\n",
    "        db_conn = get_db_connection()\n",
    "        \n",
    "        graphql_query = query_result['graphql_query']\n",
    "        attributes_mapping = query_result['attributes_mapping']\n",
    "\n",
    "        # Step 3: Fetch products\n",
    "        # products = fetch_products_recursively(graphql_query, filter=filter_params)\n",
    "        df = export_products_to_dataframe(saas_edge_id,filter_params)\n",
    "        products = df.to_dict(orient=\"records\")\n",
    "        print(f\"Fetched {len(products)}\")\n",
    "        \n",
    "        db_conn.close()\n",
    "        # update_job_details(job_id, job_response, failed_url=failed_url,success_url=success_url)\n",
    "        # Step 4: Process each product\n",
    "        result_list = []\n",
    "        failed_list = []\n",
    "        recheck_resp=export_with_readiness\n",
    "        \n",
    "        \n",
    "        for product in products:\n",
    "            \n",
    "            \n",
    "            product_id = product.get('product_id')\n",
    "            if not product_id:\n",
    "                continue\n",
    "            completeness_response=[]\n",
    "            # Get completeness data\n",
    "            if export_with_readiness:\n",
    "                try:\n",
    "                    completeness_response = fetch_completeness_data(saas_edge_id, template_id, product_id)\n",
    "                    completeness_data = completeness_response.get('data', {}).get('allProductTemplateCompletenesses', {}).get('nodes', [])\n",
    "                    print(\"completeness_data\",completeness_response)\n",
    "                    # Check if re-validation is needed\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in orchestrate_workflow: {e}\")\n",
    "                    failed_list.append({\n",
    "                        \"product_id\": product_id,\n",
    "                        \"error\": str(e),\n",
    "                    })\n",
    "                    continue\n",
    "                try:\n",
    "                    recheck_resp = check_revalidation_needed(template_node, product,completeness_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in orchestrate_workflow: {e}\")\n",
    "                    print_exc()\n",
    "            try:\n",
    "                schema_edges = template_node.get('saasTemplateSchemasByTemplateId', {}).get('edges', [])\n",
    "                schema = schema_edges[0].get('node', {}).get('schema', '{}') if schema_edges else '{}'\n",
    "                validation_result = transform_and_validate_product(\n",
    "                    attributes_mapping, product, schema, template_node, saas_edge_id, recheck_resp\n",
    "                )\n",
    "                \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in orchestrate_workflow: {e}\")\n",
    "                failed_list.append({\n",
    "                        \"product_id\": product_id,\n",
    "                        \"error\": str(e) +' '+ 'data is empty or schema is not generated',\n",
    "                    })\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            if recheck_resp:\n",
    "                try:\n",
    "                    if recheck_resp and recheck_resp.get(\"recheck_needed\"):\n",
    "                        result_list.append(insert_or_update_template_completeness(validation_result))\n",
    "                    else:\n",
    "                        result_list.append(validation_result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in orchestrate_workflow: {e}\")\n",
    "                    failed_list.append({\n",
    "                        \"product_id\": product.get(\"product_id\"),\n",
    "                        \"error\": str(e) ,\n",
    "                        \n",
    "                    })\n",
    "                    continue\n",
    "            else:\n",
    "                try:\n",
    "                    if recheck_resp:\n",
    "                        result_list.append(insert_or_update_template_completeness(validation_result))\n",
    "                    else:\n",
    "                        \n",
    "                        result_list.append(validation_result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in orchestrate_workflow: {e}\")\n",
    "                    failed_list.append({\n",
    "                        \"product_id\": product_id,\n",
    "                        \"error\": str(e) ,\n",
    "                        \n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "            \n",
    "        return result_list,failed_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in orchestrate_workflow: {e}\")\n",
    "        print(f\"Error traceback: {traceback.format_exc()}\")\n",
    "        failed_list.append({\n",
    "            \"product_id\": product.get(\"product_id\"),\n",
    "            \"error\": str(e) ,\n",
    "        })\n",
    "         \n",
    "        return None\n",
    "        \n",
    "        \n",
    "        \n",
    "        # insert_or_update_template_completeness(validation_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "\n",
    "def save_to_xlsx(data_list, file_name=\"output.xlsx\"):\n",
    "    \"\"\"\n",
    "    Converts the transformedResponse from the data list to an XLSX file.\n",
    "    Returns the file name after saving.\n",
    "    \"\"\"\n",
    "    success_count = 0\n",
    "    failure_count = 0\n",
    "    batch_size=5\n",
    "\n",
    "    # Create a new workbook and select the active worksheet\n",
    "    workbook = Workbook()\n",
    "    sheet = workbook.active\n",
    "    try:\n",
    "        data_list = [item for item in data_list if isinstance(item, dict)]\n",
    "        # Write headers\n",
    "        if data_list:  # Ensure first item is a dictionary\n",
    "            first_item = data_list[0]\n",
    "            headers = list(first_item.keys())\n",
    "            sheet.append(headers)\n",
    "    \n",
    "            # Write data rows\n",
    "            for data in data_list:\n",
    "                try:\n",
    "                    transformed_response = data\n",
    "                    row = [transformed_response.get(header, '') for header in headers]\n",
    "                    sheet.append(row)\n",
    "                    success_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: Failed to write row to XLSX: {str(e)}\")\n",
    "                    failure_count += 1\n",
    "                # if (success_count + failure_count) % batch_size == 0:\n",
    "                #     job_response = {\n",
    "                #         \"total\": success_count + failure_count,\n",
    "                #         \"success\": success_count,\n",
    "                #         \"failed\": failure_count\n",
    "                #     }\n",
    "                #     update_job_details(job_id, job_response)\n",
    "    except Exception as e:\n",
    "        print_exc()\n",
    "        print(f\"ERROR: Failed to write row to XLSX: {str(e)}\")\n",
    "        failure_count += 1\n",
    "\n",
    "    # Save the workbook\n",
    "    workbook.save(file_name)\n",
    "    print(f\"XLSX file saved as {file_name}\")\n",
    "    return file_name, success_count, failure_count  # Return the file name and counts\n",
    "\n",
    "def save_to_json(data_list, file_name=\"output.json\"):\n",
    "    \"\"\"\n",
    "    Converts the transformedResponse from the data list to a JSON file.\n",
    "    Returns the file name after saving.\n",
    "    \"\"\"\n",
    "    success_count = 0\n",
    "    failure_count = 0\n",
    "\n",
    "    # Extract transformedResponse and convert to JSON\n",
    "    json_data = []\n",
    "    for data in data_list:\n",
    "        try:\n",
    "            json_data.append(data)\n",
    "            success_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to process JSON data: {str(e)}\")\n",
    "            failure_count += 1\n",
    "\n",
    "    # Save to JSON file\n",
    "    with open(file_name, 'w') as json_file:\n",
    "        json.dump(json_data, json_file, indent=2)\n",
    "    print(f\"JSON file saved as {file_name}\")\n",
    "    return file_name, success_count, failure_count  # Return the file name and counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "import requests\n",
    "import os\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.image import MIMEImage\n",
    "\n",
    "def send_export_email(\n",
    "    receiver_emails: str,\n",
    "    receiver_name: str,\n",
    "    exported_by: str,\n",
    "    total_records: int,\n",
    "    xlsx_url: str = None,\n",
    "    json_url: str = None,\n",
    "    sender_email: str = 'support@saastify.ai',\n",
    "    password: str = 'wvkm covc lkkt vjez',  # Replace with Gmail App Password\n",
    "    logo_url: str = 'https://static.wixstatic.com/media/4a54ac_ecabd8ab59c94c92a606cde2d504a155~mv2.png/v1/fit/w_2500,h_1330,al_c/4a54ac_ecabd8ab59c94c92a606cde2d504a155~mv2.png'  # \u2705 hosted logo\n",
    "):\n",
    "    smtp_server = 'smtp.gmail.com'\n",
    "    smtp_port = 587\n",
    "    subject = \"Export Complete \u2013 Your Files Are Ready for Download\"\n",
    "\n",
    "    # Status summary\n",
    "    if xlsx_url and json_url:\n",
    "        status = \"Success \u2013 Excel & JSON exported\"\n",
    "    elif xlsx_url:\n",
    "        status = \"Success \u2013 Excel exported\"\n",
    "    elif json_url:\n",
    "        status = \"Success \u2013 JSON exported\"\n",
    "    else:\n",
    "        status = \"Export completed but no file was generated\"\n",
    "    if isinstance(receiver_emails, str):\n",
    "        receiver_emails = [email.strip().lower() for email in receiver_emails.split(\",\") if email.strip()]\n",
    "    # Build download buttons\n",
    "    download_buttons = \"\"\n",
    "    if xlsx_url:\n",
    "        download_buttons += f\"\"\"<a href=\"{xlsx_url}\" class=\"button\" style=\"color:#0f1129; margin-right:10px;\">\u2b07\ufe0f Download Excel</a>\"\"\"\n",
    "    if json_url:\n",
    "        download_buttons += f\"\"\"<a href=\"{json_url}\" class=\"button\" style=\"color:#0f1129;\">\u2b07\ufe0f Download JSON</a>\"\"\"\n",
    "    if not xlsx_url and not json_url:\n",
    "        download_buttons = \"<p style='color:red; font-weight:bold;'>No downloadable file available.</p>\"\n",
    "\n",
    "    # Email HTML\n",
    "    html = f\"\"\"\n",
    "    <html>\n",
    "        <head>\n",
    "            <style>\n",
    "                body {{\n",
    "                    font-family: 'Segoe UI', sans-serif;\n",
    "                    background-color: #f4f4f4;\n",
    "                    padding: 30px;\n",
    "                }}\n",
    "                .container {{\n",
    "                    max-width: 620px;\n",
    "                    background: white;\n",
    "                    border-radius: 10px;\n",
    "                    box-shadow: 0 4px 12px rgba(0,0,0,0.05);\n",
    "                    padding: 30px;\n",
    "                    margin: auto;\n",
    "                }}\n",
    "                .logo {{\n",
    "                    text-align: center;\n",
    "                    margin-top: 30px;\n",
    "                    margin-bottom: 35px;\n",
    "                }}\n",
    "                .logo img {{\n",
    "                    height: 180px;\n",
    "                }}\n",
    "                .greeting {{\n",
    "                    font-size: 18px;\n",
    "                    color: #333;\n",
    "                    margin-bottom: 10px;\n",
    "                }}\n",
    "                .message {{\n",
    "                    font-size: 16px;\n",
    "                    color: #444;\n",
    "                    margin: 15px 0 25px;\n",
    "                    line-height: 1.6;\n",
    "                }}\n",
    "                .summary {{\n",
    "                    background-color: #fef5f0;\n",
    "                    padding: 15px;\n",
    "                    border-left: 4px solid #FF7A1A;\n",
    "                    border-radius: 5px;\n",
    "                    margin-bottom: 30px;\n",
    "                    font-size: 15px;\n",
    "                    color: #333;\n",
    "                }}\n",
    "                .button {{\n",
    "                    display: inline-block;\n",
    "                    background-color: #FF7A1A;\n",
    "                    color: black;\n",
    "                    padding: 12px 25px;\n",
    "                    text-decoration: none;\n",
    "                    border-radius: 5px;\n",
    "                    font-size: 16px;\n",
    "                    font-weight: bold;\n",
    "                }}\n",
    "                .footer {{\n",
    "                    margin-top: 40px;\n",
    "                    text-align: center;\n",
    "                    font-size: 13px;\n",
    "                    color: #888;\n",
    "                }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"container\">\n",
    "                <div class=\"logo\">\n",
    "                    <img src=\"cid:saastifylogo\" alt=\"Saastify Logo\">\n",
    "                </div>\n",
    "                <div class=\"greeting\">Hi {receiver_name},</div>\n",
    "                <div class=\"message\">\n",
    "                    We're excited to inform you that your product export has been completed successfully.<br>\n",
    "                    Below are the export details:\n",
    "                </div>\n",
    "                <div class=\"summary\">\n",
    "                    <strong>Exported By:</strong> {exported_by}<br>\n",
    "                    <strong>Total Records:</strong> {total_records}<br>\n",
    "                    <strong>Status:</strong> {status}\n",
    "                </div>\n",
    "                <div style=\"text-align:center;\">\n",
    "                    {download_buttons}\n",
    "                </div>\n",
    "                <div class=\"footer\">\n",
    "                    Sent by Saastify \u2022 support@saastify.ai<br>\n",
    "                    \u00a9 2025 Saastify. All rights reserved.\n",
    "                </div>\n",
    "            </div>\n",
    "        </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # === Build and send email ===\n",
    "    msg = MIMEMultipart(\"related\")\n",
    "    msg[\"Subject\"] = subject\n",
    "    msg[\"From\"] = sender_email\n",
    "    msg[\"To\"] = receiver_emails[0]  # First as primary recipient\n",
    "    if len(receiver_emails) > 1:\n",
    "        msg[\"Cc\"] = \", \".join(receiver_emails[1:])  # Remaining as CC\n",
    "\n",
    "    # Add the HTML part\n",
    "    msg_alt = MIMEMultipart(\"alternative\")\n",
    "    msg.attach(msg_alt)\n",
    "    msg_alt.attach(MIMEText(html, \"html\"))\n",
    "\n",
    "    # Attach logo image (optional)\n",
    "    try:\n",
    "        response = requests.get(logo_url)\n",
    "        response.raise_for_status()\n",
    "        img = MIMEImage(response.content)\n",
    "        img.add_header(\"Content-ID\", \"<saastifylogo>\")\n",
    "        img.add_header(\"Content-Disposition\", \"inline\", filename=\"saastify_logo.png\")\n",
    "        msg.attach(img)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch logo: {e}\")\n",
    "\n",
    "    # Send the email\n",
    "    try:\n",
    "        server = smtplib.SMTP(smtp_server, smtp_port)\n",
    "        server.starttls()\n",
    "        server.login(sender_email, password)\n",
    "\n",
    "        # Combine To and Cc recipients for actual sending\n",
    "        all_recipients = receiver_emails\n",
    "\n",
    "        server.sendmail(sender_email, all_recipients, msg.as_string())\n",
    "        print(f\"Email sent to: {', '.join(all_recipients)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending email: {e}\")\n",
    "    finally:\n",
    "        server.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "\n",
    "def send_email(smtp_res: dict, to_email: str, subject: str, content: str):\n",
    "    smtp_host = smtp_res.get(\"host\")\n",
    "    smtp_port = smtp_res.get(\"port\")\n",
    "    smtp_user = smtp_res.get(\"user\")\n",
    "    smtp_password = smtp_res.get(\"password\")\n",
    "\n",
    "    # Create message\n",
    "    message = MIMEMultipart()\n",
    "    message[\"From\"] = smtp_user\n",
    "    message[\"To\"] = to_email\n",
    "    message[\"Subject\"] = subject\n",
    "    message.attach(MIMEText(content, \"plain\"))\n",
    "\n",
    "    try:\n",
    "        # If using SSL (port 465)\n",
    "        if smtp_port == 465:\n",
    "            server = smtplib.SMTP_SSL(smtp_host, smtp_port)\n",
    "        else:\n",
    "            # Default to TLS (port 587)\n",
    "            server = smtplib.SMTP(smtp_host, smtp_port)\n",
    "            server.starttls()  # Secure the connection with TLS\n",
    "\n",
    "        server.login(smtp_user, smtp_password)\n",
    "\n",
    "        # Send the email\n",
    "        server.sendmail(smtp_user, to_email, message.as_string())\n",
    "        server.quit()\n",
    "\n",
    "        return f\"Email sent from {smtp_user} to {to_email} with subject {subject} and content as {content}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Failed to send email. Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_list, job_id, is_xlsx=False, is_json=False, email=\"\",failed_url=None):\n",
    "    \"\"\"\n",
    "    Processes the data list and saves to XLSX and/or JSON based on flags.\n",
    "    \"\"\"\n",
    "    # print(\"new_records\",data_list,failed_url)\n",
    "    failed_url_data=failed_url\n",
    "    # Extract only the transformedResponse from the data list\n",
    "    transformed_responses = [\n",
    "            item.get('transformed_response', {}) if isinstance(item, dict) and 'transformed_response' in item else {}\n",
    "            for item in data_list\n",
    "        ]\n",
    "    print(\"data_len\",len(transformed_responses))\n",
    "    \n",
    "    success_count = 0\n",
    "    failure_count = 0\n",
    "    failed_url = None\n",
    "    success_url = None\n",
    "    json_url = None\n",
    "    xlsx_url = None\n",
    "    batch_size = 5\n",
    "\n",
    "    if not transformed_responses and failed_url_data:\n",
    "        print(\"No transformed data, but failed records exist.\")\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                df = pd.DataFrame(failed_url_data)\n",
    "            except ValueError:\n",
    "                df = pd.json_normalize(failed_url_data)\n",
    "\n",
    "            csv_buffer = BytesIO()\n",
    "            df.to_csv(csv_buffer, index=False)\n",
    "            csv_buffer.seek(0)\n",
    "\n",
    "            csv_filename = get_output_filename_failed(saas_edge_id, job_path, 'csv')\n",
    "            success_csv, url_csv = upload_to_gcp_bucket(\n",
    "                file_data=csv_buffer,\n",
    "                file_name=csv_filename,\n",
    "                bucket_name=GCP_BUCKET_NAME,\n",
    "                credentials=credentials\n",
    "            )\n",
    "            failed_url = url_csv if success_csv else None\n",
    "            print(\"Uploaded failed report CSV:\", failed_url)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error uploading failed CSV:\", str(e))\n",
    "            print_exc()\n",
    "\n",
    "        job_response = {\n",
    "            \"total\": total_count_data,\n",
    "            \"success\": 0,\n",
    "            \"failed\": len(failed_url_data),\n",
    "            \n",
    "        }\n",
    "\n",
    "        update_job_details(job_id, job_response, failed_url=failed_url, success_url=None)\n",
    "        return\n",
    "\n",
    "    if transformed_responses:\n",
    "        total_records = len(transformed_responses)\n",
    "        if is_xlsx:\n",
    "            try:\n",
    "                xlsx_file, xlsx_success, xlsx_failure = save_to_xlsx(transformed_responses)\n",
    "               \n",
    "                if xlsx_file:  # Ensure xlsx_file is not None\n",
    "                    output_filename = get_output_filename(\n",
    "                        saas_edge_id=saas_edge_id,\n",
    "                        job_path=job_path,\n",
    "                        file_type=\"xlsx\"\n",
    "                    )\n",
    "                   \n",
    "                    success, xlsx_url = upload_to_gcp_bucket(xlsx_file, output_filename, GCP_BUCKET_NAME, credentials=credentials)\n",
    "                    if success:\n",
    "                        success_url = xlsx_url\n",
    "                    print(\"success\", success)\n",
    "                    print(\"url\", xlsx_url)\n",
    "                \n",
    "\n",
    "            except Exception as e:\n",
    "                failure_count += 1\n",
    "                print_exc()\n",
    "                print(f\"ERROR: Failed to fetch products: {str(e)}\")\n",
    "\n",
    "        if is_json:\n",
    "            try:\n",
    "                json_file, json_success, json_failure = save_to_json(transformed_responses)\n",
    "                \n",
    "                if json_file:  # Ensure json_file is not None\n",
    "                    output_filename = get_output_filename(\n",
    "                        saas_edge_id=saas_edge_id,\n",
    "                        job_path=job_path,\n",
    "                        file_type=\"json\"\n",
    "                    )\n",
    "                    print(\"output_filename\", output_filename)\n",
    "                    success, json_url = upload_to_gcp_bucket(json_file, output_filename, GCP_BUCKET_NAME, credentials=credentials)\n",
    "                    if success:\n",
    "                        success_url = json_url\n",
    "                    print(\"success\", success)\n",
    "                    print(\"url\", json_url)\n",
    "                else:\n",
    "                    print(\"ERROR: Failed to save JSON file.\")\n",
    "            except Exception as e:\n",
    "                failure_count += 1\n",
    "                print_exc()\n",
    "                print(f\"ERROR: Failed to fetch products: {str(e)}\")\n",
    "        \n",
    "        success_count += total_records\n",
    "\n",
    "        job_response = {\n",
    "            \"total\": total_count_data,\n",
    "            \"success\": success_count,\n",
    "            \"failed\": failure_count,\n",
    "        }\n",
    "        \n",
    "        if json_url:\n",
    "            job_response[\"json_url\"] = json_url\n",
    "        if xlsx_url:\n",
    "            job_response[\"xlsx_url\"] = xlsx_url\n",
    "                \n",
    "            \n",
    "            \n",
    "        \n",
    "        update_job_details(job_id, job_response, failed_url=failed_url,success_url=success_url)\n",
    "\n",
    "        job_name = request_args.get('job_name', '')\n",
    "        exported_by_email = ''\n",
    "        if 'by ' in job_name:\n",
    "            exported_by_email = job_name.split('by ')[-1].strip()\n",
    "\n",
    "        # Extract just the name for greeting from email (e.g. 'karthik.s' \u2192 'Karthik')\n",
    "        def extract_name_from_email(email):\n",
    "            local_part = email.split('@')[0]\n",
    "            name_part = local_part.split('.')[0]  # Handle \"karthik.s\"\n",
    "            return name_part.capitalize()\n",
    "\n",
    "        # Final greeting name\n",
    "        greeting_name = extract_name_from_email(email)\n",
    "        exported_by_name = extract_name_from_email(exported_by_email)\n",
    "        print(\"Receiver Email:\", email)\n",
    "        print(\"Greeting Name:\", greeting_name)  # e.g. \"Hi Karthik\"\n",
    "        print(\"Exported By Email:\", exported_by_email)\n",
    "        print(\"Exported By Name:\", exported_by_name)\n",
    "        print(\"Total Count:\",success_count)\n",
    "        # email=False\n",
    "        if email:\n",
    "            json_export = json_url if json_url else None\n",
    "            xlsx_export = xlsx_url if xlsx_url else None\n",
    "            if str(saas_edge_id)=='8dddba20-2928-45b2-a26f-381515f9ea68':\n",
    "                print(f\"Skipping send_export_email for saas_edge_id={saas_edge_id}\")\n",
    "            else:\n",
    "                \n",
    "                send_export_email(\n",
    "                receiver_emails=email,\n",
    "                receiver_name=greeting_name,\n",
    "                exported_by=exported_by_name,\n",
    "                total_records=success_count,\n",
    "                xlsx_url=xlsx_export,\n",
    "                json_url=json_export )\n",
    "            # send_export_email(email=email,greeting_name,exported_by_name, success_count,json_export,xlsx_export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "output_list,falied_url= orchestrate_workflow(request_args)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_only = [item['transformed_response'] for item in output_list if 'transformed_response' in item]\n",
    "# cleaned_data = clean_nan(transformed_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_data(output_list,job_id, is_xlsx=request_args.get(\"is_xlsx\"), is_json=request_args.get(\"is_json\"), email=request_args.get(\"email\"),failed_url=falied_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# import paramiko\n",
    "# import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 2000\n",
    "# CSV_FILE = \"new_data.csv\"\n",
    "# CSV_HEADERS = [\n",
    "#     'Sku', 'Title', 'Specific Color', 'Color', 'Description', 'Print or Solid', 'Fabric Design', 'Suitable For',\n",
    "#     'Specific Fiber Content', 'Fiber Content', 'Care Instructions', 'Put Up', 'Compare To Price', 'Price',\n",
    "#     'Status', 'QuantityMultiples', 'Categories', 'Reorderable', 'Width', 'Repeat Vertical', 'Repeat Horizontal',\n",
    "#     'Panel Width', 'New Arrival Month', 'Track Inventory', '684 Inventory - FFC', '5528 Inventory - Stone Harbor',\n",
    "#     '6647 Inventory - Premier Bolts', '683 Inventory - Richlin', '1570 Inventory - Richlin Bolts',\n",
    "#     '3693 Inventory - Preview Textiles', '1673 Inventory - Textile Creations', '1617 Inventory - Spandexhouse',\n",
    "#     'images_0', 'images_1', 'images_2', 'images_3', 'images_4', 'images_5', 'images_6',\n",
    "#     'Assigned Catalogs', 'Tax Classification', 'Weightclass for top nav org', 'Main Fiber Content', 'OK for Promo',\n",
    "#     'List of Warehouses', 'isCreated', 'CreateDate Sort', 'List of Dropshippers', 'List of Suppliers',\n",
    "#     'Variant Compare At Price', 'internal_notes_comments', 'Priority Level'\n",
    "# ]\n",
    "\n",
    "# def fetch_data_in_batches(batch_size=BATCH_SIZE):\n",
    "#     try:\n",
    "#         conn = get_db_connection()\n",
    "#         cursor = conn.cursor()\n",
    "#         offset = 0\n",
    "#         while True:\n",
    "#             query = f\"\"\"\n",
    "#             SELECT product_id, name, sku, price, raw_product_data, description, image_url,\n",
    "#             image_url_1,image_url_2,image_url_3,image_url_4,image_url_5,\n",
    "#             image_url_6,brand, category,warehouse_01_quantity, warehouse_02_quantity, warehouse_03_quantity,\n",
    "#             priority_level_notes,internal_notes_comments,retail_price,list_of_dropshippers,list_of_suppliers,\n",
    "#             list_of_warehouses,quantity_multiples,width,channel_status,color\n",
    "#             FROM products\n",
    "#             WHERE saas_edge_id = '0ce8b38e-9121-4a07-a12f-cbb00bb7e945' AND LOWER(text_attr_1) LIKE '%gc%'\n",
    "#               AND channel_status ='active'\n",
    "#             LIMIT {batch_size} OFFSET {offset};\n",
    "    \n",
    "#                         \"\"\"\n",
    "#             cursor.execute(query)\n",
    "#             rows = cursor.fetchall()\n",
    "#             if not rows:\n",
    "#                 break\n",
    "#             print(f\"Fetched {len(rows)} records (Offset: {offset})\")\n",
    "#             yield rows\n",
    "#             offset += batch_size\n",
    "#         cursor.close()\n",
    "#         conn.close()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching data from DB: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def export_to_csv(job_id):\n",
    "#     success_count = 0\n",
    "#     failure_count = 0\n",
    "#     total_count = 0\n",
    "\n",
    "#     try:\n",
    "#         with open(CSV_FILE, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "#             writer = csv.DictWriter(file, fieldnames=CSV_HEADERS)\n",
    "#             writer.writeheader()\n",
    "\n",
    "#             for batch in fetch_data_in_batches():\n",
    "#                 for product in batch:\n",
    "#                     total_count += 1\n",
    "#                     try:\n",
    "#                         sku = product[2]\n",
    "#                         title = product[1]\n",
    "#                         price = product[3]\n",
    "#                         description = product[5]\n",
    "#                         image_url = product[6]\n",
    "#                         image_url_1=product[7]\n",
    "#                         image_url_2=product[8]\n",
    "#                         image_url_3=product[9]\n",
    "#                         image_url_4=product[10]\n",
    "#                         image_url_5=product[11]\n",
    "#                         image_url_6=product[12]\n",
    "#                         category = product[14]\n",
    "\n",
    "#                         try:\n",
    "#                             raw_data = product[4] if product[4] else {}\n",
    "#                         except Exception as e:\n",
    "#                             print(f\"Invalid JSON in raw_product_data for SKU {sku}: {e}\")\n",
    "#                             raw_data = {}\n",
    "\n",
    "#                         product_dict = {\n",
    "#                             'Sku': sku,\n",
    "#                             'Title': title,\n",
    "#                             'Description': description,\n",
    "#                             'Price': price,\n",
    "#                             'Status': 'active',\n",
    "#                             'Categories': category,\n",
    "#                             '684 Inventory - FFC': product[15],\n",
    "#                             '5528 Inventory - Stone Harbor': product[16],\n",
    "#                             '3693 Inventory - Preview Textiles': product[17],\n",
    "#                             'images_0': image_url,\n",
    "#                             'images_1': image_url_1,\n",
    "#                             'images_2': image_url_2,\n",
    "#                             'images_3': image_url_3,\n",
    "#                             'images_4': image_url_4,\n",
    "#                             'images_5': image_url_5,\n",
    "#                             'images_6': image_url_6,\n",
    "#                             'Variant Compare At Price':product[20],\n",
    "#                             'internal_notes_comments':product[19],\n",
    "#                             'Priority Level':product[18],\n",
    "#                             'List of Dropshippers':product[21],\n",
    "#                             'List of Suppliers':product[22],\n",
    "#                             'List of Warehouses':product[23],\n",
    "#                             'QuantityMultiples':product[24],\n",
    "#                             'Width':product[25],\n",
    "#                             'Status':product[26],\n",
    "#                             'Color':product[27],\n",
    "#                             'Compare To Price':raw_data.get('MSRP', '')\n",
    "                            \n",
    "#                         }\n",
    "\n",
    "#                         # Fill values from raw_product_data\n",
    "#                         for key in CSV_HEADERS:\n",
    "#                             if key not in product_dict and key in raw_data:\n",
    "#                                 product_dict[key] = raw_data[key]\n",
    "\n",
    "#                         # Fill remaining headers with \"\"\n",
    "#                         for key in CSV_HEADERS:\n",
    "#                             if key not in product_dict:\n",
    "#                                 product_dict[key] = \"\"\n",
    "\n",
    "#                         writer.writerow(product_dict)\n",
    "#                         success_count += 1\n",
    "\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Failed to process product {product[2]}: {e}\")\n",
    "#                         failure_count += 1\n",
    "\n",
    "#                 print(f\"Processed {len(batch)} records in this batch.\")\n",
    "\n",
    "#         print(f\"\\nCSV file '{CSV_FILE}' created successfully.\")\n",
    "#         print(f\" Success: {success_count}, Failure: {failure_count}, Total: {total_count}\")\n",
    "\n",
    "#         # Return local file path and counts (no GCP upload)\n",
    "#         return CSV_FILE, success_count, failure_count, total_count\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during CSV export: {e}\")\n",
    "#         return None, 0, 0, 0\n",
    "\n",
    "# export_to_csv(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}