{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Products Export Job (SDK-Based)\n",
        "\n",
        "This notebook runs a products export job using the SaaStify Edge SDK.\n",
        "\n",
        "## Parameters (Papermill)\n",
        "\n",
        "This notebook accepts the following parameters (matching productsExport.ipynb pattern):\n",
        "- `job_id`: Request/job identifier (request_id in saas_edge_jobs table)\n",
        "- `saas_edge_id`: Tenant identifier\n",
        "\n",
        "The notebook will fetch job details from `saas_edge_jobs` table and extract:\n",
        "- `template_id` from `request_args.template_id`\n",
        "- `filter_params` from `request_args.filter_params`\n",
        "- `job_path` from `request_args.job_path`\n",
        "- `is_xlsx`, `is_json` from `request_args`\n",
        "- `email` from `request_args.email`\n",
        "- `export_with_readiness` from `request_args.export_with_readiness`\n",
        "\n",
        "## Environment Variables Required\n",
        "\n",
        "Set these before running:\n",
        "- `DB_MODE`: Database connection mode (direct/proxy/local)\n",
        "- `DB_INSTANCE`: Cloud SQL instance (for direct mode)\n",
        "- `DB_NAME`: Database name\n",
        "- `DB_USER`: Database user\n",
        "- `DB_PASSWORD`: Database password\n",
        "- `GOOGLE_APPLICATION_CREDENTIALS`: Path to GCS service account JSON (optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Papermill parameters - these will be injected when running with papermill\n",
        "# Parameters match productsExport.ipynb pattern\n",
        "job_id = \"your-job-id-here\"  # request_id in saas_edge_jobs table\n",
        "saas_edge_id = \"your-saas-edge-id-here\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies and Import SDK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (matching productsExport.ipynb)\n",
        "%pip install saastify-edge-sdk pg8000 psycopg2-binary pandas requests openpyxl google-cloud-storage --quiet\n",
        "\n",
        "# Import required modules\n",
        "import asyncio\n",
        "import json\n",
        "import os\n",
        "import traceback\n",
        "import tempfile\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any, List, Optional\n",
        "from uuid import UUID\n",
        "from io import BytesIO\n",
        "\n",
        "# Import SDK components\n",
        "from saastify_edge import ExportPipelineConfig, run_product_export\n",
        "from saastify_edge.db import PostgreSQLClient, get_db_config, JobStatusUpdater\n",
        "from saastify_edge.utils import setup_logging, set_job_context, get_or_generate_template_schema\n",
        "\n",
        "# Import GCS upload helper\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "\n",
        "print(\"âœ… Dependencies installed and SDK imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fetch Job Details from Database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize database client to fetch job details\n",
        "db_config = get_db_config()\n",
        "db_client = PostgreSQLClient(db_config)\n",
        "await db_client.connect()\n",
        "\n",
        "# Fetch job details from saas_edge_jobs table (matching productsExport.ipynb pattern)\n",
        "job_manager = JobStatusUpdater(db_client)\n",
        "\n",
        "try:\n",
        "    # Get job by request_id (matching productsExport.ipynb)\n",
        "    job_record = await job_manager.get_job_by_request_id(job_id, saas_edge_id)\n",
        "    \n",
        "    if not job_record:\n",
        "        raise ValueError(f\"No job found with request_id={job_id} and saas_edge_id={saas_edge_id}\")\n",
        "    \n",
        "    # Extract request_args (matching productsExport.ipynb)\n",
        "    request_args = job_record.get(\"request_args\", {})\n",
        "    \n",
        "    print(\"âœ… Job details fetched successfully\")\n",
        "    print(f\"Job Name: {job_record.get('job_name', 'N/A')}\")\n",
        "    print(f\"Job Status: {job_record.get('job_status', 'N/A')}\")\n",
        "    print(f\"\\nRequest Args:\")\n",
        "    for key, value in request_args.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    \n",
        "    # Extract parameters from request_args (matching productsExport.ipynb structure)\n",
        "    template_id = request_args.get(\"template_id\")\n",
        "    filter_params = request_args.get(\"filter_params\", {})\n",
        "    job_path = request_args.get(\"job_path\", \"\")\n",
        "    job_name = request_args.get(\"job_name\", job_record.get(\"job_name\", \"Products Export\"))\n",
        "    is_xlsx = request_args.get(\"is_xlsx\", False)\n",
        "    is_json = request_args.get(\"is_json\", False)\n",
        "    email = request_args.get(\"email\", \"\")\n",
        "    export_with_readiness = request_args.get(\"export_with_readiness\", False)\n",
        "    \n",
        "    # Determine file format\n",
        "    if is_xlsx:\n",
        "        file_format = \"xlsx\"\n",
        "    elif is_json:\n",
        "        file_format = \"json\"\n",
        "    else:\n",
        "        file_format = \"csv\"  # default\n",
        "    \n",
        "    print(f\"\\nâœ… Extracted parameters:\")\n",
        "    print(f\"  Template ID: {template_id}\")\n",
        "    print(f\"  File Format: {file_format}\")\n",
        "    print(f\"  Export with Readiness: {export_with_readiness}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Failed to fetch job details: {e}\")\n",
        "    traceback.print_exc()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Generate Template Schema (if needed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate or fetch template schema (matching productsExport.ipynb pattern)\n",
        "if not template_id:\n",
        "    raise ValueError(\"template_id is required for export\")\n",
        "\n",
        "# Generate or fetch template schema\n",
        "try:\n",
        "    template_schema = await get_or_generate_template_schema(db_client, saas_edge_id, template_id)\n",
        "    print(f\"âœ… Template schema ready\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  Schema generation failed: {e}\")\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Add Utility Functions (Matching Products Notebooks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility functions matching productsExport.ipynb pattern\n",
        "GCP_BUCKET_NAME = os.getenv(\"GCP_BUCKET_NAME\", \"edge-assets\")\n",
        "\n",
        "def sanitize_path(path):\n",
        "    \"\"\"Sanitize path by replacing forward slashes with underscores\"\"\"\n",
        "    return path.replace('/', '_').replace(\".ipynb\", \"\").strip()\n",
        "\n",
        "def get_output_filename(saas_edge_id, job_path, file_type):\n",
        "    \"\"\"Generate standardized output filename with proper date formatting\"\"\"\n",
        "    try:\n",
        "        current_date = datetime.now()\n",
        "        date_str = current_date.strftime('%Y-%m-%d')\n",
        "        timestamp_str = current_date.strftime('%H%M%S')\n",
        "        sanitized_job_path = sanitize_path(job_path)\n",
        "        return f\"{saas_edge_id}/catalog-edge/export-job-reports/{sanitized_job_path}/{date_str}/export-file-{timestamp_str}.{file_type}\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating output filename: {str(e)}\")\n",
        "        return f\"{saas_edge_id}/catalog-edge/job-reports/error_report.{file_type}\"\n",
        "\n",
        "def get_output_filename_failed(saas_edge_id, job_path, file_type):\n",
        "    \"\"\"Generate filename for failed export report\"\"\"\n",
        "    try:\n",
        "        current_date = datetime.now()\n",
        "        date_str = current_date.strftime('%Y-%m-%d')\n",
        "        timestamp_str = current_date.strftime('%H%M%S')\n",
        "        sanitized_job_path = sanitize_path(job_path)\n",
        "        return f\"{saas_edge_id}/catalog-edge/export-job-reports/{sanitized_job_path}/{date_str}/export-file-failed{timestamp_str}.{file_type}\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating output filename: {str(e)}\")\n",
        "        return f\"{saas_edge_id}/catalog-edge/job-reports/failed_report.{file_type}\"\n",
        "\n",
        "async def upload_to_gcp_bucket(file_data, file_name, bucket_name, base_path=\"\"):\n",
        "    \"\"\"Upload data to GCP bucket using Application Default Credentials\"\"\"\n",
        "    try:\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        full_path = f\"{base_path.rstrip('/')}/{file_name}\" if base_path else file_name\n",
        "        blob = bucket.blob(full_path)\n",
        "        \n",
        "        if isinstance(file_data, (list, dict)):\n",
        "            upload_data = json.dumps(file_data, indent=2)\n",
        "            blob.upload_from_string(upload_data, content_type='application/json')\n",
        "        elif isinstance(file_data, str):\n",
        "            blob.upload_from_string(file_data)\n",
        "        elif isinstance(file_data, bytes):\n",
        "            blob.upload_from_string(file_data, content_type='application/octet-stream')\n",
        "        else:\n",
        "            blob.upload_from_file(file_data)\n",
        "        \n",
        "        url = f\"https://storage.googleapis.com/{bucket_name}/{full_path}\"\n",
        "        print(f\"Successfully uploaded {file_name} to {url}\")\n",
        "        return True, url\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading to GCP bucket: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        return False, \"\"\n",
        "\n",
        "def save_to_xlsx(data_list, file_name=\"output.xlsx\"):\n",
        "    \"\"\"Convert data list to XLSX file (matching productsExport.ipynb)\"\"\"\n",
        "    success_count = 0\n",
        "    failure_count = 0\n",
        "    \n",
        "    workbook = Workbook()\n",
        "    sheet = workbook.active\n",
        "    try:\n",
        "        data_list = [item for item in data_list if isinstance(item, dict)]\n",
        "        if data_list:\n",
        "            first_item = data_list[0]\n",
        "            headers = list(first_item.keys())\n",
        "            sheet.append(headers)\n",
        "            \n",
        "            for data in data_list:\n",
        "                try:\n",
        "                    row = [data.get(header, '') for header in headers]\n",
        "                    sheet.append(row)\n",
        "                    success_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"ERROR: Failed to write row to XLSX: {str(e)}\")\n",
        "                    failure_count += 1\n",
        "    except Exception as e:\n",
        "        print_exc()\n",
        "        print(f\"ERROR: Failed to write row to XLSX: {str(e)}\")\n",
        "        failure_count += 1\n",
        "    \n",
        "    workbook.save(file_name)\n",
        "    print(f\"XLSX file saved as {file_name}\")\n",
        "    return file_name, success_count, failure_count\n",
        "\n",
        "def save_to_json(data_list, file_name=\"output.json\"):\n",
        "    \"\"\"Convert data list to JSON file (matching productsExport.ipynb)\"\"\"\n",
        "    success_count = 0\n",
        "    failure_count = 0\n",
        "    \n",
        "    json_data = []\n",
        "    for data in data_list:\n",
        "        try:\n",
        "            json_data.append(data)\n",
        "            success_count += 1\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Failed to process JSON data: {str(e)}\")\n",
        "            failure_count += 1\n",
        "    \n",
        "    with open(file_name, 'w') as json_file:\n",
        "        json.dump(json_data, json_file, indent=2)\n",
        "    print(f\"JSON file saved as {file_name}\")\n",
        "    return file_name, success_count, failure_count\n",
        "\n",
        "async def update_job_details(job_id, job_response, failed_url=None, success_url=None):\n",
        "    \"\"\"Update job details in database (matching productsExport.ipynb pattern)\"\"\"\n",
        "    try:\n",
        "        update_data = {\n",
        "            \"job_response\": job_response,\n",
        "            \"updated_at\": datetime.utcnow()\n",
        "        }\n",
        "        \n",
        "        if failed_url:\n",
        "            update_data[\"failed_job_summary_link\"] = failed_url\n",
        "        if success_url:\n",
        "            update_data[\"success_job_summary_link\"] = success_url\n",
        "        \n",
        "        await db_client.update(\n",
        "            \"saas_edge_jobs\",\n",
        "            {\"request_id\": job_id},\n",
        "            update_data\n",
        "        )\n",
        "        \n",
        "        print(f\"âœ… Job details updated\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed to update job details: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "print(\"âœ… Utility functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Fetch Products from Database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch products from database based on filters (matching productsExport.ipynb pattern)\n",
        "# Build query with filter_params (matching productsExport.ipynb export_products_to_dataframe)\n",
        "import io\n",
        "\n",
        "# Helper function to get sync DB connection (matching productsExport.ipynb pattern)\n",
        "def get_db_connection_sync():\n",
        "    \"\"\"Get synchronous database connection for COPY operations\"\"\"\n",
        "    import psycopg2\n",
        "    import os\n",
        "    \n",
        "    # Get connection details from environment or config\n",
        "    db_host = os.getenv(\"DB_HOST\", \"/cloudsql/saastify-base-wm:us-central1:saastify-pgdb-us\")\n",
        "    db_port = int(os.getenv(\"DB_PORT\", \"5432\"))\n",
        "    db_name = os.getenv(\"DB_NAME\", \"catalog-edge-db\")\n",
        "    db_user = os.getenv(\"DB_USER\", \"postgres\")\n",
        "    db_password = os.getenv(\"DB_PASSWORD\", \"\")\n",
        "    \n",
        "    is_local = os.getenv(\"ENV\", \"product\") == \"local\"\n",
        "    \n",
        "    if is_local:\n",
        "        return psycopg2.connect(\n",
        "            host=db_host,\n",
        "            port=db_port,\n",
        "            database=db_name,\n",
        "            user=db_user,\n",
        "            password=db_password\n",
        "        )\n",
        "    else:\n",
        "        # Cloud SQL Unix socket\n",
        "        return psycopg2.connect(\n",
        "            host=db_host,\n",
        "            port=db_port,\n",
        "            database=db_name,\n",
        "            user=db_user,\n",
        "            password=db_password\n",
        "        )\n",
        "\n",
        "def export_products_to_dataframe(saas_edge_id, filter_params=None):\n",
        "    \"\"\"Export products to dataframe using COPY command (matching productsExport.ipynb)\"\"\"\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = get_db_connection_sync()\n",
        "        csv_buffer = io.TextIOWrapper(io.BytesIO(), encoding='utf-8')\n",
        "        \n",
        "        # Start query with saas_edge_id filter (matching productsExport.ipynb)\n",
        "        query_conditions = [f\"saas_edge_id = '{saas_edge_id}'\"]\n",
        "        \n",
        "        if filter_params:\n",
        "            for key, condition in filter_params.items():\n",
        "                if isinstance(condition, dict):\n",
        "                    if 'in' in condition:\n",
        "                        values = condition['in']\n",
        "                        if key.lower() == 'productid' or key.lower() == 'product_id':\n",
        "                            values_list = ','.join(str(int(v)) for v in values)\n",
        "                            query_conditions.append(f\"{key} IN ({values_list})\")\n",
        "                        else:\n",
        "                            values_list = ','.join(f\"'{v}'\" for v in values)\n",
        "                            query_conditions.append(f\"{key} IN ({values_list})\")\n",
        "                    elif 'includesInsensitive' in condition:\n",
        "                        value = condition['includesInsensitive']\n",
        "                        query_conditions.append(f\"LOWER({key}) LIKE '%{value.lower()}%'\")\n",
        "                    elif 'equalTo' in condition:\n",
        "                        value = condition['equalTo']\n",
        "                        if isinstance(value, (int, float)):\n",
        "                            query_conditions.append(f\"{key} = {value}\")\n",
        "                        else:\n",
        "                            query_conditions.append(f\"{key} = '{value}'\")\n",
        "                    elif 'notEqualTo' in condition:\n",
        "                        value = condition['notEqualTo']\n",
        "                        if isinstance(value, (int, float)):\n",
        "                            query_conditions.append(f\"{key} != {value}\")\n",
        "                        else:\n",
        "                            query_conditions.append(f\"{key} != '{value}'\")\n",
        "                    elif 'greaterThanOrEqualTo' in condition or 'lessThan' in condition:\n",
        "                        gte = condition.get('greaterThanOrEqualTo')\n",
        "                        lt = condition.get('lessThan')\n",
        "                        if gte:\n",
        "                            query_conditions.append(f\"{key} >= '{gte}'\")\n",
        "                        if lt:\n",
        "                            query_conditions.append(f\"{key} < '{lt}'\")\n",
        "                else:\n",
        "                    if isinstance(condition, (int, float)):\n",
        "                        query_conditions.append(f\"{key} = {condition}\")\n",
        "                    else:\n",
        "                        query_conditions.append(f\"{key} = '{condition}'\")\n",
        "        \n",
        "        # Build final SQL query (using products table)\n",
        "        final_conditions = \" AND \".join(query_conditions)\n",
        "        base_query = f\"\"\"\n",
        "        COPY (\n",
        "            SELECT * FROM products\n",
        "            WHERE {final_conditions}\n",
        "        ) TO STDOUT WITH CSV HEADER\n",
        "        \"\"\"\n",
        "        \n",
        "        with conn.cursor() as cur:\n",
        "            cur.execute(\"SET statement_timeout = 0\")\n",
        "            cur.copy_expert(base_query, csv_buffer)\n",
        "        \n",
        "        # Read from start\n",
        "        csv_buffer.seek(0)\n",
        "        df = pd.read_csv(csv_buffer, keep_default_na=False, low_memory=False)\n",
        "        return df\n",
        "        \n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "\n",
        "# Export products to dataframe\n",
        "df = export_products_to_dataframe(saas_edge_id, filter_params)\n",
        "products = df.to_dict(orient=\"records\")\n",
        "\n",
        "print(f\"âœ… Fetched {len(products)} products from database\")\n",
        "if products:\n",
        "    print(f\"\\nSample product:\")\n",
        "    print(f\"  ID: {products[0].get('product_id')}\")\n",
        "    print(f\"  Name: {products[0].get('name')}\")\n",
        "\n",
        "# Update job details with total count (matching productsExport.ipynb)\n",
        "total_count_data = len(products)\n",
        "job_response = {\n",
        "    \"total\": total_count_data,\n",
        "    \"success\": 0,\n",
        "    \"failed\": 0,\n",
        "}\n",
        "await update_job_details(job_id, job_response, failed_url=None, success_url=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Setup Logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup structured logging (matching productsExport.ipynb)\n",
        "setup_logging(level=\"INFO\", structured=True)\n",
        "\n",
        "# Set job context for logging\n",
        "set_job_context(\n",
        "    job_name=job_name,\n",
        "    saas_edge_id=saas_edge_id,\n",
        "    template_id=template_id\n",
        ")\n",
        "\n",
        "print(\"âœ… Logging configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Run Export Pipeline (Using SDK)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run export pipeline using SDK (matching productsExport.ipynb pattern)\n",
        "# This follows the same orchestration pattern as productsExport.ipynb\n",
        "# The SDK pipeline handles transformation, validation, and cache writing\n",
        "# We then extract transformed data from completeness cache for file processing\n",
        "\n",
        "print(f\"ðŸš€ Starting export job: {job_name}\")\n",
        "print(f\"   Template: {template_id}\")\n",
        "print(f\"   Products: {len(products)}\")\n",
        "print(f\"   Format: {file_format}\")\n",
        "print(f\"   Export with Readiness: {export_with_readiness}\")\n",
        "print()\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Import export components\n",
        "from saastify_edge.export.orchestrator import ExportPipeline, ExportPipelineConfig\n",
        "from saastify_edge.db import CompletenessReader, CompletenessWriter\n",
        "from saastify_edge.utils import get_or_generate_template_schema\n",
        "from saastify_edge.import_pipeline.template_mapper import TemplateMapper\n",
        "from saastify_edge.transformations.engine import apply_transformations\n",
        "from saastify_edge.validation.engine import validate_row\n",
        "\n",
        "# Create temporary local file path (not used since we process manually)\n",
        "temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_format}\")\n",
        "local_output_path = temp_file.name\n",
        "temp_file.close()\n",
        "\n",
        "# Configure file builder\n",
        "file_config = {\n",
        "    \"delimiter\": \",\" if file_format == \"csv\" else \"\\t\" if file_format == \"tsv\" else None,\n",
        "    \"include_headers\": True\n",
        "}\n",
        "\n",
        "# Initialize SDK components for transformation/validation\n",
        "template_mapper = TemplateMapper(db_client=db_client)\n",
        "completeness_reader = CompletenessReader(db_client=db_client)\n",
        "completeness_writer = CompletenessWriter(db_client=db_client)\n",
        "\n",
        "# Load template (matching productsExport.ipynb pattern)\n",
        "template = await template_mapper.load_template(saas_edge_id, template_id)\n",
        "if not template:\n",
        "    raise ValueError(f\"Template {template_id} not found\")\n",
        "\n",
        "print(f\"âœ… Template loaded: {template.get('template_name', template_id)}\")\n",
        "\n",
        "# Process each product (matching productsExport.ipynb orchestrate_workflow pattern)\n",
        "result_list = []\n",
        "failed_list = []\n",
        "\n",
        "for product in products:\n",
        "    product_id = product.get(\"product_id\")\n",
        "    if not product_id:\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        recheck_resp = None\n",
        "        \n",
        "        # Check completeness cache if export_with_readiness is enabled (matching productsExport.ipynb)\n",
        "        if export_with_readiness:\n",
        "            completeness_record = await completeness_reader.get_record(\n",
        "                saas_edge_id=saas_edge_id,\n",
        "                template_id=template_id,\n",
        "                product_id=str(product_id),\n",
        "                check_freshness=True\n",
        "            )\n",
        "            \n",
        "            if completeness_record and completeness_record.get(\"cache_freshness\"):\n",
        "                # Use cached transformed response\n",
        "                result_list.append({\n",
        "                    \"transformed_response\": completeness_record.get(\"transformed_response\", {}),\n",
        "                    \"is_valid\": completeness_record.get(\"is_valid\", False),\n",
        "                    \"validation_errors\": completeness_record.get(\"validation_errors\", {}),\n",
        "                    \"product_id\": product_id,\n",
        "                    \"template_id\": template_id,\n",
        "                    \"saas_edge_id\": saas_edge_id\n",
        "                })\n",
        "                continue\n",
        "        \n",
        "        # Transform and validate using SDK (matching SDK's import pipeline pattern)\n",
        "        mapped_data = template_mapper.map_row_to_fields(\n",
        "            raw_row=product,\n",
        "            template=template\n",
        "        )\n",
        "        \n",
        "        # Apply transformations (matching SDK's import pipeline pattern)\n",
        "        transformed_data = {}\n",
        "        for field_name, raw_value in mapped_data.items():\n",
        "            transformations = template_mapper.get_transformation_pipeline(\n",
        "                template=template,\n",
        "                field_name=field_name\n",
        "            )\n",
        "            \n",
        "            # Convert transformation steps to rule string (matching SDK pattern)\n",
        "            if transformations:\n",
        "                rule_parts = []\n",
        "                for trans in transformations:\n",
        "                    if trans.args:\n",
        "                        # Format with args: \"operation|arg1|arg2\"\n",
        "                        args_str = \"|\".join(str(v) for v in trans.args.values())\n",
        "                        rule_parts.append(f\"{trans.operation}|{args_str}\")\n",
        "                    else:\n",
        "                        rule_parts.append(trans.operation)\n",
        "                rule_string = \" + \".join(rule_parts)\n",
        "                transformed_value = apply_transformations(raw_value, rule_string)\n",
        "            else:\n",
        "                transformed_value = raw_value\n",
        "            \n",
        "            transformed_data[field_name] = transformed_value\n",
        "        \n",
        "        # Validate fields (matching SDK's import pipeline pattern)\n",
        "        validation_errors = {}\n",
        "        from saastify_edge.validation.engine import validate_field\n",
        "        \n",
        "        # Build field_validations dict for row-level validation\n",
        "        field_validations = {}\n",
        "        \n",
        "        for field_name, value in transformed_data.items():\n",
        "            validation_rules = template_mapper.get_validation_rules(\n",
        "                template=template,\n",
        "                field_name=field_name\n",
        "            )\n",
        "            \n",
        "            if validation_rules:\n",
        "                field_validations[field_name] = validation_rules\n",
        "                field_errors = validate_field(\n",
        "                    field_name=field_name,\n",
        "                    value=value,\n",
        "                    rules=validation_rules,\n",
        "                    context=transformed_data\n",
        "                )\n",
        "                if field_errors:\n",
        "                    validation_errors[field_name] = field_errors\n",
        "        \n",
        "        # Row-level validation (matching SDK pattern)\n",
        "        from saastify_edge.validation.engine import validate_row\n",
        "        is_valid_row, row_errors_dict, error_count = validate_row(transformed_data, field_validations)\n",
        "        if row_errors_dict:\n",
        "            validation_errors[\"_row\"] = row_errors_dict\n",
        "        \n",
        "        is_valid = len(validation_errors) == 0\n",
        "        \n",
        "        validation_result = {\n",
        "            \"transformed_response\": transformed_data,\n",
        "            \"is_valid\": is_valid,\n",
        "            \"validation_errors\": validation_errors,\n",
        "            \"product_id\": str(product_id),\n",
        "            \"template_id\": template_id,\n",
        "            \"saas_edge_id\": saas_edge_id,\n",
        "            \"last_product_updated_at\": product.get(\"updated_at\"),\n",
        "            \"last_template_updated_at\": template.get(\"updated_at\")\n",
        "        }\n",
        "        \n",
        "        # Write to completeness cache\n",
        "        await completeness_writer.write_record(\n",
        "            saas_edge_id=saas_edge_id,\n",
        "            template_id=template_id,\n",
        "            product_id=str(product_id),\n",
        "            transformed_response=transformed_data,\n",
        "            validation_errors=validation_errors,\n",
        "            is_valid=is_valid,\n",
        "            run_type=\"EXPORT\"\n",
        "        )\n",
        "        \n",
        "        result_list.append(validation_result)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing product {product_id}: {e}\")\n",
        "        traceback.print_exc()\n",
        "        failed_list.append({\n",
        "            \"product_id\": product_id,\n",
        "            \"error\": str(e)\n",
        "        })\n",
        "\n",
        "end_time = datetime.now()\n",
        "duration = (end_time - start_time).total_seconds()\n",
        "\n",
        "print(f\"âœ… Processing completed!\")\n",
        "print(f\"  Processed: {len(result_list)}\")\n",
        "print(f\"  Failed: {len(failed_list)}\")\n",
        "print(f\"  Duration: {duration:.2f} seconds\")\n",
        "\n",
        "# Store results for next cell (matching productsExport.ipynb pattern)\n",
        "output_list = result_list\n",
        "failed_url_data = failed_list\n",
        "total_count_data = len(products)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Process and Upload Files to GCS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process data and upload to GCS (matching productsExport.ipynb process_data pattern)\n",
        "# Extract transformed_response from result_list (matching productsExport.ipynb)\n",
        "\n",
        "# Extract only the transformedResponse from the data list (matching productsExport.ipynb)\n",
        "transformed_responses = [\n",
        "    item.get('transformed_response', {}) if isinstance(item, dict) and 'transformed_response' in item else {}\n",
        "    for item in output_list\n",
        "]\n",
        "\n",
        "print(f\"Extracted {len(transformed_responses)} transformed responses\")\n",
        "\n",
        "success_count = 0\n",
        "failure_count = 0\n",
        "failed_url = None\n",
        "success_url = None\n",
        "json_url = None\n",
        "xlsx_url = None\n",
        "\n",
        "# Handle case where no transformed data but failed records exist (matching productsExport.ipynb)\n",
        "if not transformed_responses and failed_url_data:\n",
        "    print(\"No transformed data, but failed records exist.\")\n",
        "    try:\n",
        "        df = pd.DataFrame(failed_url_data)\n",
        "        csv_buffer = BytesIO()\n",
        "        df.to_csv(csv_buffer, index=False)\n",
        "        csv_buffer.seek(0)\n",
        "        \n",
        "        csv_filename = get_output_filename_failed(saas_edge_id, job_path, 'csv')\n",
        "        success_csv, url_csv = await upload_to_gcp_bucket(csv_buffer, csv_filename, GCP_BUCKET_NAME)\n",
        "        failed_url = url_csv if success_csv else None\n",
        "        print(f\"âœ… Failed report uploaded: {failed_url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to upload failed report: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    job_response = {\n",
        "        \"total\": total_count_data,\n",
        "        \"success\": 0,\n",
        "        \"failed\": len(failed_url_data),\n",
        "    }\n",
        "    await update_job_details(job_id, job_response, failed_url=failed_url, success_url=None)\n",
        "else:\n",
        "    # Process transformed responses (matching productsExport.ipynb)\n",
        "    if transformed_responses:\n",
        "        total_records = len(transformed_responses)\n",
        "        \n",
        "        if is_xlsx:\n",
        "            try:\n",
        "                xlsx_file, xlsx_success, xlsx_failure = save_to_xlsx(transformed_responses)\n",
        "                if xlsx_file:\n",
        "                    output_filename = get_output_filename(saas_edge_id, job_path, \"xlsx\")\n",
        "                    success, xlsx_url = await upload_to_gcp_bucket(xlsx_file, output_filename, GCP_BUCKET_NAME)\n",
        "                    if success:\n",
        "                        success_url = xlsx_url\n",
        "                    print(f\"âœ… XLSX uploaded: {xlsx_url}\")\n",
        "            except Exception as e:\n",
        "                failure_count += 1\n",
        "                traceback.print_exc()\n",
        "                print(f\"ERROR: Failed to process XLSX: {str(e)}\")\n",
        "        \n",
        "        if is_json:\n",
        "            try:\n",
        "                json_file, json_success, json_failure = save_to_json(transformed_responses)\n",
        "                if json_file:\n",
        "                    output_filename = get_output_filename(saas_edge_id, job_path, \"json\")\n",
        "                    success, json_url = await upload_to_gcp_bucket(json_file, output_filename, GCP_BUCKET_NAME)\n",
        "                    if success:\n",
        "                        success_url = json_url\n",
        "                    print(f\"âœ… JSON uploaded: {json_url}\")\n",
        "            except Exception as e:\n",
        "                failure_count += 1\n",
        "                traceback.print_exc()\n",
        "                print(f\"ERROR: Failed to process JSON: {str(e)}\")\n",
        "        \n",
        "        success_count += total_records\n",
        "        \n",
        "        # Handle failed items\n",
        "        if failed_url_data:\n",
        "            try:\n",
        "                df = pd.DataFrame(failed_url_data)\n",
        "                csv_buffer = BytesIO()\n",
        "                df.to_csv(csv_buffer, index=False)\n",
        "                csv_buffer.seek(0)\n",
        "                \n",
        "                csv_filename = get_output_filename_failed(saas_edge_id, job_path, 'csv')\n",
        "                success_csv, url_csv = await upload_to_gcp_bucket(csv_buffer, csv_filename, GCP_BUCKET_NAME)\n",
        "                failed_url = url_csv if success_csv else None\n",
        "                print(f\"âœ… Failed report uploaded: {failed_url}\")\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR: Failed to upload failed report: {str(e)}\")\n",
        "                traceback.print_exc()\n",
        "        \n",
        "        # Update job details (matching productsExport.ipynb)\n",
        "        job_response = {\n",
        "            \"total\": total_count_data,\n",
        "            \"success\": success_count,\n",
        "            \"failed\": failure_count,\n",
        "        }\n",
        "        \n",
        "        if json_url:\n",
        "            job_response[\"json_url\"] = json_url\n",
        "        if xlsx_url:\n",
        "            job_response[\"xlsx_url\"] = xlsx_url\n",
        "        \n",
        "        await update_job_details(job_id, job_response, failed_url=failed_url, success_url=success_url)\n",
        "        \n",
        "        print(f\"\\nâœ… Export processing completed:\")\n",
        "        print(f\"  Success: {success_count}\")\n",
        "        print(f\"  Failed: {failure_count}\")\n",
        "        if success_url:\n",
        "            print(f\"  Success URL: {success_url}\")\n",
        "        if failed_url:\n",
        "            print(f\"  Failed URL: {failed_url}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Final Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate final summary (matching productsExport.ipynb pattern)\n",
        "summary = {\n",
        "    \"job_id\": job_id,\n",
        "    \"job_name\": job_name,\n",
        "    \"status\": \"completed\",\n",
        "    \"template_id\": template_id,\n",
        "    \"total_rows\": total_count_data,\n",
        "    \"success_rows\": success_count,\n",
        "    \"failed_rows\": failure_count,\n",
        "    \"file_format\": file_format,\n",
        "    \"json_url\": json_url,\n",
        "    \"xlsx_url\": xlsx_url,\n",
        "    \"failed_url\": failed_url,\n",
        "    \"success_url\": success_url,\n",
        "    \"duration_seconds\": duration,\n",
        "    \"completed_at\": datetime.utcnow().isoformat()\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PRODUCTS EXPORT JOB SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "for key, value in summary.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# Store summary as JSON for papermill output\n",
        "summary_json = json.dumps(summary, indent=2)\n",
        "print(\"\\nSummary JSON:\")\n",
        "print(summary_json)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Cleanup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Disconnect from database\n",
        "await db_client.disconnect()\n",
        "print(\"âœ… Database connection closed\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
